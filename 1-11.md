---
title: sparksql-2
date: 1-11 8.40
categories: 日志
comments: "true"
tag: spark
---
# 构建df

* rdd
* hive
* 外部数据源
  * json,csv,jdbc/odbc

# 加载外部数据源

## api简介

### TEXT

| **Property Name** | **Default**                                            | **Meaning**                                                                                                                                                  | **Scope** |
| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------- |
| wholetext               | false                                                        | If true, read each file from input path(s) as a single row.                                                                                                        | read            |
| lineSep                 | `\r`, `\r\n`, `\n` (for reading), `\n` (for writing) | Defines the line separator that should be used for reading or writing.                                                                                             | read/write      |
| compression             | (none)                                                       | Compression codec to use when saving to file.<br />This can be one of the known case-insensitive shorten names<br /> (none, bzip2, gzip, lz4, snappy and deflate). | write           |

### json

| **Property Name**                | **Default**                                                                                   | **Meaning**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | **Scope**                                                                                                                                                                            |
| -------------------------------------- | --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `timeZone`                           | (value of `spark.sql.session.timeZone` configuration)                                             | Sets the string that indicates a time zone ID to be used to format timestamps in the JSON datasources or partition values. The following formats of `timeZone` are supported:``* Region-based zone ID: <br />It should have the form 'area/city',<br /> such as 'America/Los_Angeles'.* Zone offset: It should be in the format '(+                                                                                                                                                                                                                                                                                                                                                                                                                  | -)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.Other short names like 'CST' are not recommended to use because they can be ambiguous. |
| `primitivesAsString`                 | `false`                                                                                           | Infers all primitive values as a string type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | read                                                                                                                                                                                       |
| `prefersDecimal`                     | `false`                                                                                           | Infers all floating-point values as a decimal type.<br />If the values do not fit in decimal, then it infers them as doubles.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | read                                                                                                                                                                                       |
| `allowComments`                      | `false`                                                                                           | Ignores Java/C++ style comment in JSON records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | read                                                                                                                                                                                       |
| `allowUnquotedFieldNames`            | `false`                                                                                           | Allows unquoted JSON field names.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | read                                                                                                                                                                                       |
| `allowSingleQuotes`                  | `true`                                                                                            | Allows single quotes in addition to double quotes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | read                                                                                                                                                                                       |
| `allowNumericLeadingZero`            | `false`                                                                                           | Allows leading zeros in numbers (e.g. 00012).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | read                                                                                                                                                                                       |
| `allowBackslashEscapingAnyCharacter` | `false`                                                                                           | Allows accepting quoting of all character using backslash quoting mechanism<br />.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | read                                                                                                                                                                                       |
| `mode`                               | `PERMISSIVE`                                                                                      | Allows a mode for dealing with corrupt records during parsing.``*`PERMISSIVE`: when it meets a corrupted record, puts the malformed string into a field configured by `columnNameOfCorruptRecord`, and sets malformed fields to `null`. To keep corrupt records, an user can set a string type field named `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a `columnNameOfCorruptRecord` field in an output schema.* `DROPMALFORMED`: ignores the whole corrupted records. This mode is unsupported in the JSON built-in functions.* `FAILFAST`: throws an exception when it meets corrupted records. | read                                                                                                                                                                                       |
| `columnNameOfCorruptRecord`          | (value of `spark.sql.columnNameOfCorruptRecord` configuration)                                    | Allows renaming the new field having malformed string created by `PERMISSIVE` mode. This overrides spark.sql.columnNameOfCorruptRecord.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read                                                                                                                                                                                       |
| `dateFormat`                         | `yyyy-MM-dd`                                                                                      | Sets the string that indicates a date format. Custom date formats follow the formats at[datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to date type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | read/write                                                                                                                                                                                 |
| `timestampFormat`                    | `yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]`                                                                | Sets the string that indicates a timestamp format. Custom date formats follow the formats at[datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to timestamp type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read/write                                                                                                                                                                                 |
| `timestampNTZFormat`                 | yyyy-MM-dd'T'HH:mm:ss[.SSS]                                                                         | Sets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at[Datetime Patterns](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.                                                                                                                                                                                                                                                                                                                                                                                           | read/write                                                                                                                                                                                 |
| `multiLine`                          | `false`                                                                                           | Parse one record, which may span multiple lines, per file. JSON built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | read                                                                                                                                                                                       |
| `allowUnquotedControlChars`          | `false`                                                                                           | Allows JSON Strings to contain unquoted control characters (ASCII characters with value less than 32, including tab and line feed characters) or not.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | read                                                                                                                                                                                       |
| `encoding`                           | Detected automatically when `multiLine` is set to `true` (for reading), `UTF-8` (for writing) | For reading, allows to forcibly set one of standard basic or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. For writing, Specifies encoding (charset) of saved json files. JSON built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | read/write                                                                                                                                                                                 |
| `lineSep`                            | `\r`, `\r\n`, `\n` (for reading), `\n` (for writing)                                        | Defines the line separator that should be used for parsing. JSON built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | read/write                                                                                                                                                                                 |
| `samplingRatio`                      | `1.0`                                                                                             | Defines fraction of input JSON objects used for schema inferring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | read                                                                                                                                                                                       |
| `dropFieldIfAllNull`                 | `false`                                                                                           | Whether to ignore column of all null values or empty array/struct during schema inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | read                                                                                                                                                                                       |
| `locale`                             | `en-US`                                                                                           | Sets a locale as language tag in IETF BCP 47 format. For instance,`locale` is used while parsing dates and timestamps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | read                                                                                                                                                                                       |
| `allowNonNumericNumbers`             | `true`                                                                                            | Allows JSON parser to recognize set of “Not-a-Number” (NaN) tokens as legal floating number values.``*`+INF`:<br /> for positive infinity, as well as alias of `+Infinity` and `Infinity`.* `-INF`: for negative infinity, alias `-Infinity`.* `NaN`: for other not-a-numbers, like result of division by zero.                                                                                                                                                                                                                                                                                                                                                                                                                        | read                                                                                                                                                                                       |
| `compression`                        | (none)                                                                                              | Compression codec to use when saving to file. This can be one of the known case-insensitive shorten names<br /> (none, bzip2, gzip, lz4, snappy and deflate). JSON built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | write                                                                                                                                                                                      |
| `ignoreNullFields`                   | (value of `spark.sql.jsonGenerator.ignoreNullFields` configuration)                               | Whether to ignore null fields when generating JSON objects.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | write                                                                                                                                                                                      |

### csv

| **Property Name**       | **Default**                                                | **Meaning**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | **Scope** |
| ----------------------------- | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- |
| `sep`                       | ,                                                                | Sets a separator for each field and value. This separator can be one or more characters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | read/write      |
| `encoding`                  | UTF-8                                                            | For reading, decodes the CSV files by the given encoding type. For writing, specifies encoding (charset) of saved CSV files. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | read/write      |
| `quote`                     | "                                                                | Sets a single character used for escaping quoted values where the separator can be part of the value. For reading, if you would like to turn off quotations, you need to set not `null` but an empty string. For writing, if an empty string is set, it uses `u0000` (null character).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | read/write      |
| `quoteAll`                  | false                                                            | A flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | write           |
| `escape`                    | \                                                                | Sets a single character used for escaping quotes inside an already quoted value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | read/write      |
| `escapeQuotes`              | true                                                             | A flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | write           |
| `comment`                   |                                                                  | Sets a single character used for skipping lines beginning with this character. By default, it is disabled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | read            |
| `header`                    | false                                                            | For reading, uses the first line as names of columns. For writing, writes the names of columns as the first line. Note that if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | read/write      |
| `inferSchema`               | false                                                            | Infers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read            |
| `enforceSchema`             | true                                                             | If it is set to `true`, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to `false`, the schema will be validated against all headers in CSV files in the case when the `header` option is set to `true`. Field names in the schema and column names in CSV headers are checked by their positions taking into account `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable the `enforceSchema` option to avoid incorrect results. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | read            |
| `ignoreLeadingWhiteSpace`   | `false` (for reading), `true` (for writing)                  | A flag indicating whether or not leading whitespaces from values being read/written should be skipped.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | read/write      |
| `ignoreTrailingWhiteSpace`  | `false` (for reading), `true` (for writing)                  | A flag indicating whether or not trailing whitespaces from values being read/written should be skipped.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | read/write      |
| `nullValue`                 |                                                                  | Sets the string representation of a null value. Since 2.0.1, this `nullValue` param applies to all supported types including the string type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | read/write      |
| `nanValue`                  | NaN                                                              | Sets the string representation of a non-number value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read            |
| `positiveInf`               | Inf                                                              | Sets the string representation of a positive infinity value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read            |
| `negativeInf`               | -Inf                                                             | Sets the string representation of a negative infinity value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read            |
| `dateFormat`                | yyyy-MM-dd                                                       | Sets the string that indicates a date format. Custom date formats follow the formats at[Datetime Patterns](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to date type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | read/write      |
| `timestampFormat`           | yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]                                 | Sets the string that indicates a timestamp format. Custom date formats follow the formats at[Datetime Patterns](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to timestamp type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | read/write      |
| `timestampNTZFormat`        | yyyy-MM-dd'T'HH:mm:ss[.SSS]                                      | Sets the string that indicates a timestamp without timezone format. Custom date formats follow the formats at[Datetime Patterns](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html). This applies to timestamp without timezone type, note that zone-offset and time-zone components are not supported when writing or reading this data type.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read/write      |
| `maxColumns`                | 20480                                                            | Defines a hard limit of how many columns a record can have.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | read            |
| `maxCharsPerColumn`         | -1                                                               | Defines the maximum number of characters allowed for any given value being read. By default, it is -1 meaning unlimited length                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | read            |
| `mode`                      | PERMISSIVE                                                       | Allows a mode for dealing with corrupt records during parsing. It supports the following case-insensitive modes. Note that Spark tries to parse only required columns in CSV under column pruning. Therefore, corrupt records can be different based on required set of fields. This behavior can be controlled by `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).``* `PERMISSIVE`: when it meets a corrupted record, puts the malformed string into a field configured by `columnNameOfCorruptRecord`, and sets malformed fields to `null`. To keep corrupt records, an user can set a string type field named `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets `null` to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.* `DROPMALFORMED`: ignores the whole corrupted records. This mode is unsupported in the CSV built-in functions.* `FAILFAST`: throws an exception when it meets corrupted records. | read            |
| `columnNameOfCorruptRecord` | (value of `spark.sql.columnNameOfCorruptRecord` configuration) | Allows renaming the new field having malformed string created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | read            |
| `multiLine`                 | false                                                            | Parse one record, which may span multiple lines, per file. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read            |
| `charToEscapeQuoteEscaping` | `escape` or `\0`                                             | Sets a single character used for escaping the escape for the quote character. The default value is escape character when escape and quote characters are different,`\0` otherwise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | read/write      |
| `samplingRatio`             | 1.0                                                              | Defines fraction of rows used for schema inferring. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | read            |
| `emptyValue`                | (for reading),`""` (for writing)                               | Sets the string representation of an empty value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | read/write      |
| `locale`                    | en-US                                                            | Sets a locale as language tag in IETF BCP 47 format. For instance, this is used while parsing dates and timestamps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | read            |
| `lineSep`                   | `\r`, `\r\n` and `\n` (for reading), `\n` (for writing)  | Defines the line separator that should be used for parsing/writing. Maximum length is 1 character. CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | read/write      |
| `unescapedQuoteHandling`    | STOP_AT_DELIMITER                                                | Defines how the CsvParser will handle values with unescaped quotes.``*`STOP_AT_CLOSING_QUOTE`: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.* `BACK_TO_DELIMITER`: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.* `STOP_AT_DELIMITER`: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter or a line ending is found in the input.* `SKIP_VALUE`: If unescaped quotes are found in the input, the content parsed for the given value will be skipped and the value set in nullValue will be produced instead.* `RAISE_ERROR`: If unescaped quotes are found in the input, a TextParsingException will be thrown.                                                                                         | read            |
| `compression`               | (none)                                                           | Compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (`none`, `bzip2`, `gzip`, `lz4`, `snappy` and `deflate`). CSV built-in functions ignore this option.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | write           |

### jdbc

| **Property Name**                     | **Default**                                                                                                                     | **Meaning**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | **Scope** |
| ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- |
| `url`                                     | (none)                                                                                                                                | The JDBC URL of the form `jdbc:subprotocol:subname` to connect to. The source-specific connection properties may be specified in the URL. e.g., `jdbc:postgresql://localhost/test?user=fred&password=secret`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | read/write      |
| `dbtable`                                 | (none)                                                                                                                                | The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a `FROM` clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify `dbtable` and `query` options at the same time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read/write      |
| `query`                                   | (none)                                                                                                                                | A query that will be used to read data into Spark. The specified query will be parenthesized and used as a subquery in the `FROM` clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source.``SELECT <columns> FROM (<user_specified_query>) spark_gen_alias``Below are a couple of restrictions while using this option.``1. It is not allowed to specify `dbtable` and `query` options at the same time.1. It is not allowed to specify `query` and `partitionColumn` options at the same time. When specifying `partitionColumn` option is required, the subquery can be specified using `dbtable` option instead and partition columns can be qualified using the subquery alias provided as part of `dbtable`.``Example:`spark.read.format("jdbc").option("url", jdbcUrl).option("query", "select c1, c2 from t1").load()` | read/write      |
| `driver`                                  | (none)                                                                                                                                | The class name of the JDBC driver to use to connect to this URL.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | read/write      |
| `partitionColumn, lowerBound, upperBound` | (none)                                                                                                                                | These options must all be specified if any of them is specified. In addition,`numPartitions` must be specified. They describe how to partition the table when reading in parallel from multiple workers. `partitionColumn` must be a numeric, date, or timestamp column from the table in question. Notice that `lowerBound` and `upperBound` are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.                                                                                                                                                                                                                                                                                                                                                                                   | read            |
| `numPartitions`                           | (none)                                                                                                                                | The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling `coalesce(numPartitions)` before writing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read/write      |
| `queryTimeout`                            | `0`                                                                                                                                 | The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API `setQueryTimeout`, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | read/write      |
| `fetchsize`                               | `0`                                                                                                                                 | The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | read            |
| `batchsize`                               | `1000`                                                                                                                              | The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | write           |
| `isolationLevel`                          | `READ_UNCOMMITTED`                                                                                                                  | The transaction isolation level, which applies to current connection. It can be one of `NONE`, `READ_COMMITTED`, `READ_UNCOMMITTED`, `REPEATABLE_READ`, or `SERIALIZABLE`, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of `READ_UNCOMMITTED`. Please refer the documentation in `java.sql.Connection`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | write           |
| `sessionInitStatement`                    | (none)                                                                                                                                | After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example:`option("sessionInitStatement", """BEGIN execute immediate 'alter session set "_serial_direct_read"=true'; END;""")`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | read            |
| `truncate`                                | `false`                                                                                                                             | This is a JDBC writer related option. When `SaveMode.Overwrite` is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. In case of failures, users should turn off `truncate` option to use `DROP TABLE` again. Also, due to the different behavior of `TRUNCATE TABLE` among DBMS, it's not always safe to use this. MySQLDialect, DB2Dialect, MsSqlServerDialect, DerbyDialect, and OracleDialect supports this while PostgresDialect and default JDBCDirect doesn't. For unknown and unsupported JDBCDirect, the user option `truncate` is ignored.                                                                                                              | write           |
| `cascadeTruncate`                         | the default cascading truncate behaviour of the JDBC database in question, specified in the `isCascadeTruncate` in each JDBCDialect | This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a `TRUNCATE TABLE t CASCADE` (in the case of PostgreSQL a `TRUNCATE TABLE ONLY t CASCADE` is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | write           |
| `createTableOptions`                      |                                                                                                                                       | This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g.,`CREATE TABLE t (name string) ENGINE=InnoDB.`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | write           |
| `createTableColumnTypes`                  | (none)                                                                                                                                | The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g:`"name CHAR(64), comments VARCHAR(1024)")`. The specified types should be valid spark sql data types.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | write           |
| `customSchema`                            | (none)                                                                                                                                | The custom schema to use for reading data from JDBC connectors. For example,`"id DECIMAL(38, 0), name STRING"`. You can also specify partial fields, and the others use the default type mapping. For example, `"id DECIMAL(38, 0)"`. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | read            |
| `pushDownPredicate`                       | `true`                                                                                                                              | The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.                                                                                                                                                                                                                                                                                                                                                                                                                                                       | read            |
| `pushDownAggregate`                       | `false`                                                                                                                             | The option to enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Otherwise, if sets to true, aggregates will be pushed down to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down. If `numPartitions` equals to 1 or the group by key is the same as `partitionColumn`, Spark will push down aggregate to data source completely and not apply a final aggregate over the data source output. Otherwise, Spark will apply a final aggregate over the data source output.                                                                                             | read            |
| `pushDownLimit`                           | `false`                                                                                                                             | The option to enable or disable LIMIT push-down into V2 JDBC data source. The LIMIT push-down also includes LIMIT + SORT , a.k.a. the Top N operator. The default value is false, in which case Spark does not push down LIMIT or LIMIT with SORT to the JDBC data source. Otherwise, if sets to true, LIMIT or LIMIT with SORT is pushed down to the JDBC data source. If `numPartitions` is greater than 1, SPARK still applies LIMIT or LIMIT with SORT on the result from data source even if LIMIT or LIMIT with SORT is pushed down. Otherwise, if LIMIT or LIMIT with SORT is pushed down and `numPartitions` equals to 1, SPARK will not apply LIMIT or LIMIT with SORT on the result from data source.                                                                                                                                                                                                        | read            |
| `pushDownTableSample`                     | `false`                                                                                                                             | The option to enable or disable TABLESAMPLE push-down into V2 JDBC data source. The default value is false, in which case Spark does not push down TABLESAMPLE to the JDBC data source. Otherwise, if value sets to true, TABLESAMPLE is pushed down to the JDBC data source.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | read            |
| `keytab`                                  | (none)                                                                                                                                | Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by `--files` option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise `--files` assumed. If both `keytab` and `principal` are defined then Spark tries to do kerberos authentication.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | read/write      |
| `principal`                               | (none)                                                                                                                                | Specifies kerberos principal name for the JDBC client. If both `keytab` and `principal` are defined then Spark tries to do kerberos authentication.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | read/write      |
| `refreshKrb5Config`                       | `false`                                                                                                                             | This option controls whether the kerberos configuration is to be refreshed or not for the JDBC client before establishing a new connection. Set to true if you want to refresh the configuration, otherwise set to false. The default value is false. Note that if you set this option to true and try to establish multiple connections, a race condition can occur. One possble situation would be like as follows.1. refreshKrb5Config flag is set with security context 11. A JDBC connection provider is used for the corresponding DBMS1. The krb5.conf is modified but the JVM not yet realized that it must be reloaded1. Spark authenticates successfully for security context 11. The JVM loads security context 2 from the modified krb5.conf1. Spark restores the previously saved security context 11. The modified krb5.conf content just gone                                                               | read/write      |
| `connectionProvider`                      | (none)                                                                                                                                | The name of the JDBC connection provider to use to connect to this URL, e.g.`db2`, `mssql`. Must be one of the providers loaded with the JDBC data source. Used to disambiguate when more than one provider can handle the specified driver and options. The selected provider must not be disabled by `spark.sql.sources.disabledJdbcConnProviderList`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | read/write      |

### excel

暂时没找到，找到再补

### hive

对于hive我们要定义输入格式输出格式甚至是元素内部，以及每个元素之间的分隔符如下

| Property Name                                                    | Meaning                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| fileFormat                                                       | A fileFormat is kind of a package of storage format specifications, including "serde", "input format" and "output format". Currently we support 6 fileFormats: 'sequencefile', 'rcfile', 'orc', 'parquet', 'textfile' and 'avro'.                                                                                                          |
| inputFormat, outputFormat                                        | These 2 options specify the name of a corresponding `InputFormat` and `OutputFormat` class as a string literal, e.g. `org.apache.hadoop.hive.ql.io.orc.OrcInputFormat`. These 2 options must be appeared in a pair, and you can not specify them if you already specified the `fileFormat` option.                                 |
| serde                                                            | This option specifies the name of a serde class. When the `fileFormat` option is specified, do not specify this option if the given `fileFormat` already include the information of serde. Currently "sequencefile", "textfile" and "rcfile" don't include the serde information and you can use this option with these 3 fileFormats. |
| fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim | These options can only be used with "textfile" fileFormat. They define how to read delimited files into rows.                                                                                                                                                                                                                              |

关于不同版本的spark关联到hive的源数据库如下配置

```
One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against built-in Hive and use those classes for internal execution (serdes, UDFs, UDAFs, etc).
```

| Property Name                            | Default                                                                             | Meaning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Since Version |
| ---------------------------------------- | ----------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| spark.sql.hive.metastore.version         | 2.3.9                                                                               | Version of the Hive metastore. Available options are `0.12.0` through `2.3.9` and `3.0.0` through `3.1.2`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1.40          |
| spark.sql.hive.metastore.jars            | builtin                                                                             | Location of the jars that should be used to instantiate the HiveMetastoreClient.<br />This property can be one of four options:<br />1.builtin<br />Use Hive 2.3.9, which is bundled with the Spark assembly when `-Phive` is enabled. When this option is chosen, `spark.sql.hive.metastore.version` must be either `2.3.9` or not defined.<br />maven<br />Use Hive jars of specified version downloaded from Maven repositories. This configuration is not generally recommended for production deployments.<br />path<br />Use Hive jars configured by `spark.sql.hive.metastore.jars.path` in comma separated format. Support both local or remote paths. The provided jars should be the same version as `spark.sql.hive.metastore.version`.<br />A classpath in the standard format for the JVM. This classpath must include all of Hive and its dependencies, including the correct version of Hadoop. The provided jars should be the same version as `spark.sql.hive.metastore.version`. These jars only need to be present on the driver, but if you are running in yarn cluster mode then you must ensure they are packaged with your application. | 1.40          |
| spark.sql.hive.metastore.jars.path       | (empty)                                                                             | Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient. This configuration is useful only when `spark.sql.hive.metastore.jars` is set as `path`.``The paths can be any of the following format:1. `file://path/to/jar/foo.jar`1. `hdfs://nameservice/path/to/jar/foo.jar`1. `/path/to/jar/`(path without URI scheme follow conf `fs.defaultFS`'s URI schema)1. `[http/https/ftp]://path/to/jar/foo.jar`Note that 1, 2, and 3 support wildcard. For example:1. `file://path/to/jar/*,file://path2/to/jar/*/*.jar`1. `hdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 3.10          |
| spark.sql.hive.metastore.sharedPrefixes  | com.mysql.jdbc,<br />org.postgresql,<br />com.microsoft.sqlserver,<br />oracle.jdbc | A comma-separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 1.40          |
| spark.sql.hive.metastore.barrierPrefixes | (empty)                                                                             | A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e.`org.apache.spark.*`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 1.40          |

## 读数据

### TEXT

官方简介

```
Spark SQL provides spark.read().text("file_name") to read a file or directory of text files into a Spark DataFrame, and dataframe.write().text("path") to write to a text file. When reading a text file, each line becomes each row that has string “value” column by default. The line separator can be changed as shown in the example below. The option() function can be used to customize the behavior of reading or writing, such as controlling behavior of the line separator, compression, and so on.
```

读的时候我们不用设置压缩格式，它和mr一样会自动解压

```
package sparkfirst

import org.apache.spark.sql.SparkSession

object sparksql2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()
    val df = spark.read.text("file:///D:\\test.txt") // 返回值是DF
    df.show()
    df.printSchema()
  
  
    var result =
      """
+--------------------+
|               value|
+--------------------+
|as,s,ed,f,,,qq,eq...|
|,w,wq,e,w,ewq,we,...|
+--------------------+
      """
  
    //text所带有的schame信息是他自己给我们添加上的，所以有时候我们用这个会不方便
    val df1 = spark.read.textFile("file:///D:\\test.txt") // 返回值是dataset
    df1.printSchema()
    //--------------------------------------------------------------------
    //使用lineSep改变分隔符如下
    val df2 = spark.read.option("lineSep",",").text("file:///D:\\test.txt")
    df2.show()
    result =
      """
+---------+
|    value|
+---------+
|       as|
|        s|
|       ed|
|        f|
|         |
|         |
|       qq|
|eqedqwe\n|
|        w|
|       wq|
|        e|
|        w|
|      ewq|
|       we|
|        q|
|        e|
|wewqeqwel|
|       qe|
| lqeweqwl|
|     qw\n|
+---------+
      """
    //---------------------------------------------------------------------
    //使用wholetext把一整个文件当作一行来接受
    val df3 = spark.read.option("wholetext",true).text("file:///D:\\test.txt")
    df3.show()
    var result =
      """
+--------------------+
|               value|
+--------------------+
|as,s,ed,f,,,qq,eq...|
+--------------------+
      """
    //这里的text和textFile是可以相互用的
    val df4 = spark.read.option("wholetext",true).textFile("file:///D:\\test.txt")
    df4.show()
    var result =
      """
+--------------------+
|               value|
+--------------------+
|as,s,ed,f,,,qq,eq...|
+--------------------+
      """
    val df5 = spark.read.option("wholetext",true).format("text").load("file:///D:\\test.txt")
    df5.show()
    var result =
      """
+--------------------+
|               value|
+--------------------+
|as,s,ed,f,,,qq,eq...|
+--------------------+
      """

  }
}

```

我们点进去源码发现text的底层是

`def text(paths: String*): DataFrame = format("text").load(paths : _*)`

所以我们使用的时候可以

```
val df5=spark.read.option("wholetext",true).format("text").load("file:///D:\\test.txt")
```

### json

简介

```
Spark SQL can automatically infer the schema of a JSON dataset and load it as a Dataset[Row]. This conversion can be done using SparkSession.read.json() on either a Dataset[String], or a JSON file.

Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. For more information, please see JSON Lines text format, also called newline-delimited JSON.

For a regular multi-line JSON file, set the multiLine option to true.
```

json分为简单json和嵌套json

```
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()

    //-------------------------普通json
    val df = spark.read.json("file:///C:\\Users\\dell\\Desktop\\dept.json")
    df.show()
    df.printSchema()
    //--------------------------嵌套json如果嵌套的是STruct => 打点 / ARRAY 类型 先炸开 再打点
    var df1 = spark.read.format("json").load("file:///C:\\Users\\dell\\Desktop\\Skills.json")
    df1.printSchema()
    //--------------------------api
    //-withColumn可以增加一个字段，或者把一个字段重命名 =》 提出字段
    df1=df1.withColumn("critical",col("damage.critical"))
    df1=df1.withColumn("elementId",explode(col("damage.elementId")))
    df1.printSchema()
    //------------------------删除字段
    df1=df1.drop("damage.critical","damage.elementId")
    //-------------------------sql
    //------------------------对比hivesql
    df1.createOrReplaceTempView("test")
    //spark.sql("SELECT get_json_object('{\"a\":\"b\"}', '$.a');").show()
    // struct可以用下面打点的方法
    spark.sql(
      """
        |select
        |effects.ddd,
        |damage.ddddds
        |from
        |test
        |""".stripMargin).show()
    //或者用爆炸加侧写进行 ：array元素,嵌套json
    spark.sql(
      """
        |select
        |effects.ddd,
        |damage.ddddds
        |from
        |test
        |lateral view explode(store.fruit) as fruit
        |""".stripMargin)




```

### csv

简介

```
Spark SQL provides spark.read().csv("file_name") to read a file or directory of files in CSV format into Spark DataFrame, and dataframe.write().csv("path") to write to a CSV file. Function option() can be used to customize the behavior of reading or writing, such as controlling behavior of the header, delimiter character, character set, and so on.
```

csv文件默认的分隔符是，但是可以更改

```
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()

    val df = spark.read.format("csv").load("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df.show()
    var result=
      """
        |+---+---------+------+----+----------+---+--------------------+------------+----------+
        ||_c0|      _c1|   _c2| _c3|       _c4|_c5|                 _c6|         _c7|       _c8|
        |+---+---------+------+----+----------+---+--------------------+------------+----------+
        || id|device_id|gender| age|university|gpa|active_days_withi...|question_cnt|answer_cnt|
        ||  1|     2138|  male|  21|  北京大学|3.4|                   7|           2|        12|
        ||  2|     3214|  male|null|  复旦大学|  4|                  15|           5|        25|
        ||  3|     6543|female|  20|  北京大学|3.2|                  12|           3|        30|
        ||  4|     2315|female|  23|  浙江大学|3.6|                   5|           1|         2|
        ||  5|     5432|  male|  25|  山东大学|3.8|                  20|          15|        70|
        ||  6|     2131|  male|  28|  山东大学|3.3|                  15|           7|        13|
        ||  7|     4321|  male|  28|  复旦大学|3.6|                   9|           6|        52|
        |+---+---------+------+----+----------+---+--------------------+------------+----------+
        |""".stripMargin
    // 这样默认是通过，进行分割的
    //可以通过delimiter来设置分割参数，sep和他一样
    val df1 = spark.read.option("delimiter",";").csv("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df1.show()
    result =
      """
        |+------------------------+
        ||                     _c0|
        |+------------------------+
        ||    "id","device_id",...|
        ||  1,2138,male,21,北京...|
        ||2,3214,male,,复旦大学...|
        ||    3,6543,female,20,...|
        ||    4,2315,female,23,...|
        ||  5,5432,male,25,山东...|
        ||  6,2131,male,28,山东...|
        ||  7,4321,male,28,复旦...|
        |+------------------------+
        |""".stripMargin
    val df4 = spark.read.option("sep",";").csv("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df4.show()
    result =
      """
        |+------------------------+
        ||                     _c0|
        |+------------------------+
        ||    "id","device_id",...|
        ||  1,2138,male,21,北京...|
        ||2,3214,male,,复旦大学...|
        ||    3,6543,female,20,...|
        ||    4,2315,female,23,...|
        ||  5,5432,male,25,山东...|
        ||  6,2131,male,28,山东...|
        ||  7,4321,male,28,复旦...|
        |+------------------------+
        |""".stripMargin
    //还可以从csv里加载表头
    val df2 = spark.read.option("delimiter",",").option("header","true").format("csv").load("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df2.show()
    result=
      """
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        || id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        ||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|
        ||  2|     3214|  male|null|  复旦大学|  4|                   15|           5|        25|
        ||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|
        ||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|
        ||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|
        ||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|
        ||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        |""".stripMargin
    //还可以把上面两个option合并
    val df3 = spark.read.options(Map("delimiter" -> "," ,"header" -> "true")).csv("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df3.show()
    result=
      """
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        || id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        ||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|
        ||  2|     3214|  male|null|  复旦大学|  4|                   15|           5|        25|
        ||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|
        ||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|
        ||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|
        ||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|
        ||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        |""".stripMargin
    //还可以加上自动推断类型，如果不加，它就会默认是字符串类型inferSchema
    val df5 = spark.read.options(Map("sep"->",","header"->"true","inferSchema"->"true","encoding"->"UTF8")).csv("file:///C:\\Users\\dell\\Desktop\\user_profile.csv")
    df5.show()
    result=
      """
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        || id|device_id|gender| age|university|gpa|active_days_within_30|question_cnt|answer_cnt|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        ||  1|     2138|  male|  21|  北京大学|3.4|                    7|           2|        12|
        ||  2|     3214|  male|null|  复旦大学|4.0|                   15|           5|        25|
        ||  3|     6543|female|  20|  北京大学|3.2|                   12|           3|        30|
        ||  4|     2315|female|  23|  浙江大学|3.6|                    5|           1|         2|
        ||  5|     5432|  male|  25|  山东大学|3.8|                   20|          15|        70|
        ||  6|     2131|  male|  28|  山东大学|3.3|                   15|           7|        13|
        ||  7|     4321|  male|  28|  复旦大学|3.6|                    9|           6|        52|
        |+---+---------+------+----+----------+---+---------------------+------------+----------+
        |""".stripMargin
    df5.printSchema()
    result=
      """
        |root
        | |-- id: integer (nullable = true)
        | |-- device_id: integer (nullable = true)
        | |-- gender: string (nullable = true)
        | |-- age: integer (nullable = true)
        | |-- university: string (nullable = true)
        | |-- gpa: double (nullable = true)
        | |-- active_days_within_30: integer (nullable = true)
        | |-- question_cnt: integer (nullable = true)
        | |-- answer_cnt: integer (nullable = true)
        |""".stripMargin
    //等 剩下的请看api简介

    df5.createOrReplaceTempView("csv")
    spark.sql(
      """
        |select
        |gender,
        |device_id,
        |active_days_within_30,
        |university
        |from
        |csv
        |where university = '北京大学'
        |""".stripMargin).show()

    result=
      """
        |+------+---------+---------------------+----------+
        ||gender|device_id|active_days_within_30|university|
        |+------+---------+---------------------+----------+
        ||  male|     2138|                    7|  北京大学|
        ||female|     6543|                   12|  北京大学|
        |+------+---------+---------------------+----------+
        |""".stripMargin
```

### jdbc

```
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()

    //用代码创建
    val df = spark.read.format("JDBC")
      .option("url","jdbc:mysql://bigdata2:3306/try")
      .option("dbtable", "emp")
      .option("user", "root")
      .option("password", "liuzihan010616")
      .load()
    df.printSchema()
    df.show()
    // 但是这样传入是把整个表直接传进来，但是有时候我们只要其中一一部分可以这样相当于谓词下压
        val sal =
      """
        |select
        |*
        |from
        |emp where sal > 1500
        |""".stripMargin
    val df1 = spark.read.format("JDBC")
      .option("url","jdbc:mysql://bigdata2:3306/try")
      .option("dbtable", s"($sal) as tmp")
      .option("user", "root")
      .option("password", "liuzihan010616")
      .load()
    df1.show()
     result =
      """
        |+-----+------+---------+----+-------------------+-------+-------+------+
        ||empno| ename|      job| mgr|           hiredate|    sal|   comm|deptno|
        |+-----+------+---------+----+-------------------+-------+-------+------+
        || 7499| ALLEN| SALESMAN|7698|1981-02-20 00:00:00|1600.00| 300.00|    30|
        || 7566| JONES|  MANAGER|7839|1981-04-02 00:00:00|2975.00|   null|    20|
        || 7698| BLAKE|  MANAGER|7839|1981-05-01 00:00:00|2850.00|   null|    30|
        || 7782| CLARK|  MANAGER|7839|1981-06-09 00:00:00|2450.00|   null|    10|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000.00|   null|    10|
        || 7902|  FORD|  ANALYST|7566|1981-12-03 00:00:00|3000.00|   null|    20|
        || 7839|  KING|PRESIDENT|null|1981-11-17 00:00:00|5000.00|   null|    10|
        || 7654|MARTIN| SALESMAN|7698|1981-09-28 00:00:00|3200.00|1400.00|    30|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        || 7788| SCOTT|  ANALYST|7566|1982-12-09 00:00:00|3000.00|   null|    20|
        |+-----+------+---------+----+-------------------+-------+-------+------+
        |only showing top 20 rows
        |
        |
        |Process finished with exit code 0
        |
        |""".stripMargin
    //用Properties传入
    val connectionProperties = new Properties()
    connectionProperties.put("user", "root")
    connectionProperties.put("password", "liuzihan010616")
    val jdbcDF2 = spark.read
      .jdbc("jdbc:mysql://bigdata2:3306/try", "try.emp", connectionProperties)
```

### excel

在idea里要先导入spark-excel的pom：这里的版本要和scala对应上

```
   <dependency>
      <groupId>com.crealytics</groupId>
      <artifactId>spark-excel_2.12</artifactId>
      <version>0.14.0</version>
    </dependency>
```

然后代码

```
package sparkfirst

import java.util.Properties

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import com.crealytics.spark.excel._
object sparksql2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()
    val df = spark.read.excel(header = true,inferSchema = true).load("file:////C:\\Users\\dell\\Desktop\\2023届毕业设计题目-计算机-选题志愿表.xlsx")
    df.show()
    val result =
      """
        |+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+
        ||2023届计算机科学与技术专业毕业设计选题|     _c1|                                  _c2|   _c3|     _c4|     _c5|     _c6|
        |+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+
        ||                                  序号|指导老师|                                 题目|学生数|第一志愿|第二志愿|第三志愿|
        ||                                     1|  王海涛|          基于android的房产中介app...|     1|    null|    null|    null|
        ||                                     2|  王海涛|        基于android的酒店预约入住a...|     1|    null|    null|    null|
        ||                                     3|  王海涛|          基于android的有声书app的...|     1|    null|    null|    null|
        ||                                     4|  王海涛|          基于android的掌上医院app...|     1|    null|    null|    null|
        ||                                     5|  王海涛|    基于web的考试管理系统的设计与实现|     1|    null|    null|    null|
        ||                                     6|    王琢|           电商平台产品评论爬虫的设计|     1|    null|    null|    null|
        ||                                     7|    王琢|     基于Django的智能水务系统前端开发|     1|    null|    null|    null|
        ||                                     8|    王琢|           个人账本管理微信小程序开发|     1|    null|    null|    null|
        ||                                     9|    王琢|       智能水务系统远程监控模块的开发|     1|    null|    null|    null|
        ||                                    10|  张文波|基于安卓系统的硕士研究生招生预报名...|     1|    null|    null|    null|
        ||                                    11|  张文波|面向工业互联网的联网设备故障检测技...|     1|    null|    null|    null|
        ||                                    12|  张文波|面向工业互联网的联网设备运行维护系...|     1|    null|    null|    null|
        ||                                    13|    曹烨|     疫情防控管理信息系统的设计与开发|     1|    null|    null|    null|
        ||                                    14|    曹烨|             多线程下载器的设计与开发|     1|    null|    null|    null|
        ||                                    15|    曹烨|             坦克对战游戏的设计与开发|     1|    null|    null|    null|
        ||                                    16|    曹烨|           五子棋游戏大厅的设计与开发|     1|    null|    null|    null|
        ||                                    17|    杜焱|       疫情封闭人员及物资管理系统开发|     1|    null|    null|    null|
        ||                                    18|    杜焱|                   志愿者服务系统开发|     1|    null|    null|    null|
        ||                                    19|    杜焱|         高校教师工作绩效管理系统开发|     1|    null|    null|    null|
        |+--------------------------------------+--------+-------------------------------------+------+--------+--------+--------+
        |only showing top 20 rows
        |
        |
        |Process finished with exit code 0
        |
        |""".stripMargin


```

### hive

在生产上我们要对配置文件进行修改

```
Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.
```

直接cp hivehome 下的配置文件 hive-site.xml到spark的配置下或者做软连接到spark的配置文件下

即使hive和spark不在同一台机器上也是可以的，只不过不能做软连接了

如果缺少mysql驱动的化把mysql驱动添加到spark的jar文件夹里就好

或者用 --jars 路径 来配置启动方式

接下来我们直接执行如下语句

```
scala> spark.sql("show databases").show
+-------------+
|    namespace|
+-------------+
|      bigdata|
| bigdata_hive|
|bigdata_hive2|
|bigdata_hive3|
|bigdata_hive4|
|      default|
|         test|
+-------------+

```

我们可以用spark-sql脚本来执行hive的语句

如下：

```
[hadoop@bigdata5 conf]$ spark-sql --master local[4]
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/01/12 10:10:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/01/12 10:10:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
23/01/12 10:10:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
Spark master: local[4], Application Id: local-1673489414801
spark-sql (default)> show databases;
namespace
bigdata
bigdata_hive
bigdata_hive2
bigdata_hive3
bigdata_hive4
default
test
Time taken: 2.213 seconds, Fetched 7 row(s)

```

我们一般都在spark-sql里进行测试sql语句，然后再通过代码部署=》不要再sparksql里创建表会有点问题=》尽可能再hive里键

维护数仓 =》 可以用spark-sql -e/-f  sql文件 =》 推荐的方式维护离线数仓 好维护 简单

hive引擎如果改成spark =》 不稳定 =》 bug =》 有的时候spark的function无法使用

hive里有的function =》 spark始终有

但是spark里有的hive里没有

idea里也是要把hive-site放到resource文件夹里

东西很多想要什么上官网查看 `https://spark.apache.org/docs/latest/sql-ref-syntax.html#ddl-statements`

idea里要提前加上依赖

如下

```
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.12</artifactId>
      <version>3.2.1</version>
    </dependency>
```

然后执行以下代码就ok了

```
package sparkfirst

import org.apache.spark.sql.SparkSession

object sparksql3 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
    val frame = spark.sql(
      """
        |select
        |*
        |from
        |bigdata_hive3.emp
        |""".stripMargin)
    frame.show()
    frame.printSchema()
    val result =
      """
        |+-----+--------+---------+----+--------+----+----+------+
        ||empno|   ename|      job| mgr|hiredate| sal|comm|deptno|
        |+-----+--------+---------+----+--------+----+----+------+
        || 7369|   SMITH|    CLERK|7902|    null| 800|null|    20|
        || 7499|   ALLEN| SALESMAN|7698|    null|1600| 300|    30|
        || 7521|    WARD| SALESMAN|7698|    null|1250| 500|    30|
        || 7566|   JONES|  MANAGER|7839|    null|2975|null|    20|
        || 7654|  MARTIN| SALESMAN|7698|    null|1250|1400|    30|
        || 7698|   BLAKE|  MANAGER|7839|    null|2850|null|    30|
        || 7782|   CLARK|  MANAGER|7839|    null|2450|null|    10|
        || 7788|   SCOTT|  ANALYST|7566|    null|3000|null|    20|
        || 7839|    KING|PRESIDENT|null|    null|5000|null|    10|
        || 7844|  TURNER| SALESMAN|7698|    null|1500|   0|    30|
        || 7876|   ADAMS|    CLERK|7788|    null|1100|null|    20|
        || 7900|lebulang|    CLERK|7698|    null| 950|null|    30|
        || 7902|    FORD|  ANALYST|7566|    null|3000|null|    20|
        || 7934|  MILLER|    CLERK|7782|    null|1300|null|    10|
        || 7839|    KING|PRESIDENT|null|    null|5000|null|    10|
        || 7654|  MARTIN| SALESMAN|7698|    null|3200|1400|    30|
        || 7788|   SCOTT|  ANALYST|7566|    null|3000|null|    20|
        || 7788|   SCOTT|  ANALYST|7566|    null|3000|null|    20|
        || 7788|   SCOTT|  ANALYST|7566|    null|3000|null|    20|
        || 7788|   SCOTT|  ANALYST|7566|    null|3000|null|    20|
        |+-----+--------+---------+----+--------+----+----+------+
        |only showing top 20 rows
        |
        |root
        | |-- empno: string (nullable = true)
        | |-- ename: string (nullable = true)
        | |-- job: string (nullable = true)
        | |-- mgr: long (nullable = true)
        | |-- hiredate: date (nullable = true)
        | |-- sal: decimal(10,0) (nullable = true)
        | |-- comm: decimal(10,0) (nullable = true)
        | |-- deptno: long (nullable = true)
        |
        |
        |Process finished with exit code 0
        |
        |""".stripMargin
  }
}

```

## 写数据

写数据的时候一般会伴随crc文件

### TEXT

注意text仅仅支持一列的数据进行输出，不支持多列，因为我们的resource文件夹里有配置文件所以它走我们的配置文件压缩格式为bz2，不过可以自己指定格式，一般不指定且无配置文件是不压缩的

```
package sparkfirst

import org.apache.spark.sql.SparkSession

object sparksql2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").getOrCreate()
    val df2 = spark.read.option("lineSep",",").text("file:///D:\\test.txt")
    df2.show()
    //-----------------------------------------写数据
    df2.write.text("file:///D:\\test1.txt")
    //-------------------------------------------加压缩
    df2.write.option("compression", "gzip").text("file:///D:\\test2.txt")
  }
}

```

如果想解决，要自己定义外部数据源，相当于自己修改源码

或者把dataframe变成rdd进行输出 因为saveasTextFile是可以多列输出的

`df2.rdd.saveAsTextFile("file:///D:\\test3.txt")`

![](https://pic.imgdb.cn/item/63be1a9cbe43e0d30e184271.jpg)

查看文件格式

![](https://pic.imgdb.cn/item/63be1ad7be43e0d30e18aa99.jpg)

### json

```
    //常用的输出方式追加append，或者覆盖(overwrite),或者忽略（ignore），错误等（error）
    //df.write.mode(saveMode = "overwrite").json("hdfs://bigdata3:9000/spark")
```

### csv

```
  //写出可以用sep进行设置导出的分隔符，mode设置是不是覆盖，compression设置压缩
    df5.write.options(Map("sep"->";","compression"->"gzip")).mode("overwrite").format("csv").save("file:///C:\\Users\\dell\\Desktop\\user_profile1.csv")

```

结果如下

![](https://pic.imgdb.cn/item/63be621fbe43e0d30e93423c.jpg)

### JDBC

```
//写出 --------------------------代码
    //如果用overwrite，会把之前的表删掉，然后重新建一个，表的数据结构会发生改变
    df.write.mode("append")
      .format("jdbc")
      .option("url", "jdbc:mysql://bigdata2:3306/try")
      .option("dbtable", "emp1")
      .option("user", "root")
      .option("password", "liuzihan010616")
      .save()
      // --------------------------Properties
    df.write.mode("append")
      .jdbc("jdbc:mysql://bigdata2:3306/try", "emp1", connectionProperties)

    // 可以在写的时候多创建列
    df.write.mode("append")
      .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
      .jdbc("jdbc:mysql://bigdata2:3306/try", "try.emp", connectionProperties)
```

### excel

如下

```
df.write.mode("overwrite").excel(header = true,"A1").save("file:///C:\\Users\\dell\\Desktop\\2023届毕业设计题目-计算机-选题志愿表1.xlsx")

```


### hive

一共有几种方法

* ctas

```
  val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
  def main(args: Array[String]): Unit = {

    spark.sql(
      """
        |create table bigdata.sparkfinish as
        |select
        |*
        |from(
        |select
        |area,
        |product_name,
        |rank() over(partition by area order by cnt) as rk
        |from (
        |select
        |area,
        |product_name,
        |count(1) as cnt
        |from bigdata.tmp
        |group by area,product_name
        |)
        |)where rk < 3;
        |""".stripMargin)

    // --------------------------------------insert into  数据追加
    spark.sql(
      """
        |insert into table bigdata.sparkfinish
        |select * from bigdata.sparkfinish
        |""".stripMargin)

    // ----------------------------------------数据覆盖
    spark.sql(
      """
        |insert overwrite table bigdata.sparkfinish
        |select * from bigdata.sparkfinish
        |""".stripMargin)
    // ----------------------------------------分区表 emp_partition是元数据分区表 emp_partition1是后来键的分区表
    // ---------------------------------------执行动态分区的时候要配置参数 静态分区不用
    spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
    spark.conf.set("hive.exec.dynamic.partition","true")
    spark.sql(
      """
        |insert overwrite table bigdata_hive3.emp_partition1 partition(deptno)
        |select * from bigdata_hive3.emp_partition
        |""".stripMargin)
//-----------------------------------------------------------------api
	  val frame = checksql(hivesqlchoose("empno , ename , job , mgr , deptno ", "bigdata_hive3.emp", "where sal > 3000"))

    // -----------------------------这里如果用覆盖模式是会把整个表都弄美哦然后重新建表到数据的，所以一般不用，放置我们只对一个分区数据进行操作的时候别的数据不见
    // -------------------------------------普通表
    frame.write.mode(saveMode = "append").format("hive").saveAsTable("bigdata_hive3.emp89")
        spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
        spark.conf.set("hive.exec.dynamic.partition","true")
    // -------------------------------------分区表
    frame.write.partitionBy("deptno").mode(saveMode = "append").format("hive").saveAsTable("bigdata_hive3.emp891")
    // ---------------------------------------insertInto 插入数据 如果是用于分区表它会自动使用动态分区 对普通表可以
    // Exception in thread "main" org.apache.spark.sql.AnalysisException:  insertInto() can't be used together with partitionBy(). Partition columns have already been defined for the table. It is not necessary to use partitionBy().
    //frame.write.partitionBy("deptno").mode(saveMode = "overwrite").format("hive").insertInto("bigdata_hive3.emp891")
    // 可以把数据作为路径写入到hdfs上 ，写入他对应的table path下

    frame.select("empno","ename","job","mgr").write.mode("overwrite").parquet("hdfs://bigdata3:9000/user/hive/warehouse/bigdata_hive3.db/emp_partition1/deptno=20")
    // 对于普通表这样写入是ok的，但是对于分区表 因为元数据的不同 所以可能会导致元数据关联不上
    // 修复元数据库就ok了
    // 注意这里要是parquet的存储格式的或者orc的，如果是text的会成乱码
    // 修复元数据
    // msck repair table table_name [ADD/DROP/SYNC partition]
    // 或者通过rdd的方法进行存储
}


  def hivesqlchoose(string: String*)={

    val str = "select" + " " + string(0) + " " + "from" + " " +  string(1)
    if (string.length > 2){
      str +" " + string(2)
    }else{
      str
    }
  }

  def descfunctionsql(string: String)={
    s"""
      |desc function extended $string
      |""".stripMargin
  }

  def checksql(string: String)={
    spark.sql(string).show(false)
    spark.sql(string)
  }
}
```
