---
title: sparksql-3
date: 1-13 8.40
categories: 日志
comments: "true"
tag: spark
---
# 案例：业务数据 + 日志数据

业务数据 在mysql里 =》

* city_info
* user_info

日志数据 在hive里 =》

* user_click

思路 =》 先用代码把数据统计到df中对df进行操作

### 代码如下：

```
package sparkfirst

import org.apache.spark.sql.SparkSession

object xiangmu1 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
    import spark.implicits._
    val city_info = spark.read.format("jdbc")
      .options(Map("url"->args(0),"dbtable"->args(3),"user"->args(1),"password"->args(2),"driver"->"com.mysql.jdbc.Driver")).load()

    val user_info = spark.sql(
      """
        |select * from bigdata.product_info
        |""".stripMargin)

//    city_info.show()
//    user_info.show()

    val product_info = spark.read.textFile("hdfs://bigdata3:9000/data/user_click.txt")
//    product_info.show(false)

    val userlog = product_info.map(line => {
      val strings = line.split(",")
      val userid = strings(0)
      val sessionid = strings(1)
      val dt = strings(2)
      val cityid = strings(3)
      val shopid = strings(4)
      (userid, sessionid, dt, cityid, shopid)
    }).toDF("userid", "sessionid", "dt", "cityid", "shopid")


//    userlog.show(false)



    //----------------------------------------------------------------
    city_info.createOrReplaceTempView("city_info")
    userlog.createOrReplaceTempView("user_log")
    user_info.createOrReplaceTempView("product_info")
    //--------------------------------------------------------------
    spark.sql(
      """
        |drop table if exists bigdata.tmp
        |""".stripMargin)
    spark.sql(
      """
        |
        |
        |create table bigdata.tmp as
        |select
        |*
        |from (
        |    select * from city_info left join user_log on city_info.city_id = user_log.cityid left join product_info  on user_log.shopid = product_info.product_id
        |)
        |""".stripMargin)
    spark.sql(
      """
        |drop table if exists bigdata.sparkfinish
        |""".stripMargin)
    spark.sql(
      """
        |create table bigdata.sparkfinish as
        |select
        |*
        |from(
        |select
        |area,
        |product_name,
        |rank() over(partition by area order by cnt) as rk
        |from (
        |select
        |area,
        |product_name,
        |count(1) as cnt
        |from bigdata.tmp
        |group by area,product_name
        |)
        |)where rk < 3;
        |""".stripMargin)


  }
}

```

### 然后我们进行打包上传

进行打包之前，我们要把我们的

```
 val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
```

给注释掉，因为我们用spark-submit的时候会通过命令的方式指定他

打包上传之后我们可以进行调用脚本spark-submit脚本

但是这里还是有分歧的，因为spark-submit部署的时候一般是部署在yarn的要分几种模式的简单介绍一下两种模式

Cluster：

* 提交作业  client作业提交  client就可以关闭了 对spark作业是没有影响的 而且运行的时候
* driver  是在集群机器里的
* 上yarn上看日志

client：

* 提交作业  client作业提交 如果client关闭了 driver process 挂了  对spark作业有影响的
* 是在 client机器里的
* 可以直接看见日志的

以下代码分别写一下

```
spark-submit \
--master yarn \
--deploy-mode client \
--name userlog \
--executor-memory 1g \
--num-executors 1 \
--executor-cores 1 \
--class sparkfirst.xiangmu1 \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
"jdbc:mysql://bigdata2:3306/bigdata" root liuzihan010616 city_info
--------------------------------------------------------cluster
spark-submit \
--master yarn \
--deploy-mode cluster \
--name userlog \
--executor-memory 1g \
--num-executors 2 \
--executor-cores 1 \
--class sparkfirst.xiangmu1 \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
"jdbc:mysql://bigdata2:3306/bigdata" root liuzihan010616 city_info
-------------------------------------------------------------- 以下是正常的因为我把mysql的driver加入到spark的jars文件夹里了，所以上面不用指定jar
spark-submit \
--master yarn \
--deploy-mode client  \
--name userlog \
--executor-memory 1g \
--num-executors 1 \
--executor-cores 1 \
--jars /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--driver-class-path /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--driver-library-path /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--class sparkfirst.xiangmu1 \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
"jdbc:mysql://bigdata2:3306/bigdata" root liuzihan010616 city_info user_info
--------------------------------------------------------------------------------------------------
spark-submit \
--master yarn \
--deploy-mode cluster \
--name userlog \
--executor-memory 1g \
--num-executors 1 \
--executor-cores 1 \
--jars /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--driver-class-path /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--driver-library-path /home/hadoop/software/mysql-connector-java-5.1.28.jar \
--class sparkfirst.xiangmu1 \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
"jdbc:mysql://bigdata2:3306/bigdata" root liuzihan010616 city_info user_info

```

### 几种加入driver的方法是

在执行命令的时候加入命令：如上

直接加到jars里

这里比较不推荐的就是第二种，因为怕和spark本身的包产生冲突

### 执行流程

spark到yarn的执行流程和hadoop基本一样，除了spark的持久化操作要用到catche，其余都一样

driver =》manager

excuter =》 container

### catlog

hive元数据 在mysql里

spark 访问hive元数据 通过jdbc

spark提供了个catlog

直接调用catlog可以直接拿到hive的元数据的功能

比如制作大数据分析平台

获取catlog

`sparksesson.catlog`

然后里面有很多的方法可以在idea里通过ctrl + f12 查看方法

冷数据可能放在cos 或者 oss上

### udf

* 代码的方式定义udf
* hive的udf可以在spark可以直接用

```
idea里定义udf
---------------------------------------
先导包
import org.apache.spark.sql.functions.udf
然后


val spark = SparkSession
  .builder()
  .appName("Spark SQL UDF scalar example")
  .getOrCreate()

// Define and register a zero-argument non-deterministic UDF
// UDF is deterministic by default, i.e. produces the same result for the same input.
val random = udf(() => Math.random())
spark.udf.register("random", random.asNondeterministic())
spark.sql("SELECT random()").show()
// +-------+
// |UDF()  |
// +-------+
// |xxxxxxx|
// +-------+

// Define and register a one-argument UDF
val plusOne = udf((x: Int) => x + 1)
spark.udf.register("plusOne", plusOne)
spark.sql("SELECT plusOne(5)").show()
// +------+
// |UDF(5)|
// +------+
// |     6|
// +------+

// Define a two-argument UDF and register it with Spark in one step
spark.udf.register("strLenScala", (_: String).length + (_: Int))
spark.sql("SELECT strLenScala('test', 1)").show()
// +--------------------+
// |strLenScala(test, 1)|
// +--------------------+
// |                   5|
// +--------------------+

// UDF in a WHERE clause
spark.udf.register("oneArgFilter", (n: Int) => { n > 5 })
spark.range(1, 10).createOrReplaceTempView("test")
spark.sql("SELECT * FROM test WHERE oneArgFilter(id)").show()
// +---+
// | id|
// +---+
// |  6|
// |  7|
// |  8|
// |  9|
// +---+
```
