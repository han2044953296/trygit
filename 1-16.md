---
title: sparksql-4
date: 1-16 8.40
categories: 日志
comments: "true"
tag: spark
---
# sparkstreaming

用于实时计算的模块 =》 sparkstreaming，structuredstreaming

## 流处理 ： 实时

* 实时 来一套数据处理一条 storm，flink 数据叫event
* 近实时 来一批数据处理 mini-batch sparkstreaming
* 数据会源源不断地来

## 批处理 ： 离线

* 代码或者程序处理一个批次的数据

  * 例子：数据放在hdfs上，我们对他进行处理 =》 ok

## 技术选型

生产上：

* sparkstreaming，structuredstreaming 10%
* flink 90%
* storm 2%

开发角度：

* code =》 flink > sparkstreaming
* sql => flink > spark streaming

业务：

* 实施指标 ：都差不多
* 实时数仓：
  * 代码 ： 差不多
  * sql文件 ： flinksql维护实时数仓 =》 ok

## 特性

容易使用 =》 客观看

批流一体的处理方法 =》 sparksql <=> 流处理

低延迟高消费

## 简介

* sparkstreaming开发是spark-core的一个扩展
* 接收数据的渠道多
* 还可以对数据进行流处理的可以机器学习等

一般来说流式处理会比批处理负载小，但不绝对

### 数据源 ：

* kafka ****** 流式引擎重要的数据源 -》 通过topic进行数据缓冲，它会根据sp的吞吐量来进行处理，两个引擎之间会有联系
* flume **** 可以使用但是一般不用 flume 没有数据缓冲 致命 -》 直接把数据弄到sp里，如果数据量特别多，会让sp程序挂掉，因为如果sa程序的吞吐量比较小，则会崩掉，和sp无联系
* hdfs

### 数据积压：kafka数据太大，导致sp程序一直处理不过来，一个出不来报表 =>解决方法

* 吞吐量提高
* 数据量减少

### sparkstreaming运行机制

* 接收数据
* 拆分成batches

### sparkstreaming -> kafka ：

* 5s处理数据
* 每5s会切分成一次batch
* 交给sparkngine处理
* 处理完的也是一个batch

### sparkstreaming编程模型：Dstream

* 外部数据源
* 高级算子
* 类似RDD

### idea开发先配置pom文件

```
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.12</artifactId>
      <version>3.2.1</version>
    </dependency>
```

### idea代码

```
package sparkstreaming
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.SparkConf

object sparkstreaming1 {
  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setMaster("local[4]").setAppName("NetworkWordCount")
    val ssc = new StreamingContext(conf, Seconds(5))
    // 或者通过sparkcontext进行创建
    //val ssc = new StreamingContext(sc, Seconds(1))
    // 数据源
    val lines = ssc.socketTextStream("bigdata5", 9999)
    // 处理数据
    val words = lines.flatMap(_.split(" "))
    val pairs = words.map(word => (word, 1))
    val wordCounts = pairs.reduceByKey(_ + _)

    // 打印数据当前批次
    wordCounts.print()
    ssc.start()             // Start
    ssc.awaitTermination()  // Wait

    // 配置数据源在目标机器上执行nc -lk 9999 然后输入数据就ok了
  }

}

```

还可以在webui上查看

如下：

![](https://pic.imgdb.cn/item/63c4b657be43e0d30e191256.jpg)

他的打印数据是处理当前批次的数据，不是累积批次的数据

### 双流join

api :

* flink -》调用api
* sparkstreaming code 很多 -》 api join stste

 延迟数据

* processtime + udf
* eventime + watermaker
  * 数据和离线对不上（容易）

如何构建DStream

* 从inputstream的方式 生产上
* receiver 测试用 为面试准备

## 构建Dstream

### inputstrteam

比如卡夫卡

### receiver

用receiver接受的时候如果是本地则要大于1 -> local[2+]

因为sparkstreaming最少是有两部分切分以及处理，如果只给1则会没有资源进行处理

所以针对于receiver一个要大于等于

上面仅仅是针对receiver

例子 ：

```
 val lines = ssc.socketTextStream("bigdata5", 9999)
```

因为他底层源码是

```
  def socketTextStream(
      hostname: String,
      port: Int,
      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
    ): ReceiverInputDStream[String] = withNamedScope("socket text stream") {
    socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)
  }

```

## Dstream算子

转换操作：

```
Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the common ones are as follows.
```

| Transformation                                          | Meaning                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **map** ( *func* )                              | Return a new DStream by passing each element of the source DStream through a function*func* .                                                                                                                                                                                                                                                                                                                                                                         |
| **flatMap** ( *func* )                          | Similar to map, but each input item can be mapped to 0 or more output items.                                                                                                                                                                                                                                                                                                                                                                                            |
| **filter** ( *func* )                           | Return a new DStream by selecting only the records of the source DStream on which*func* returns true.                                                                                                                                                                                                                                                                                                                                                                 |
| **repartition** ( *numPartitions* )             | Changes the level of parallelism in this DStream by creating more or fewer partitions.                                                                                                                                                                                                                                                                                                                                                                                  |
| **union** ( *otherStream* )                     | Return a new DStream that contains the union of the elements in the source DStream and*otherDStream* .                                                                                                                                                                                                                                                                                                                                                                |
| **count** ()                                      | Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.                                                                                                                                                                                                                                                                                                                                                       |
| **reduce** ( *func* )                           | Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function*func* (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.                                                                                                                                                                                              |
| **countByValue** ()                               | When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.                                                                                                                                                                                                                                                                                                 |
| **reduceByKey** ( *func* , [ *numTasks* ])    | When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function.**Note:** By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property `spark.default.parallelism`) to do the grouping. You can pass an optional `numTasks` argument to set a different number of tasks. |
| **join** ( *otherStream* , [ *numTasks* ])    | When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.                                                                                                                                                                                                                                                                                                                              |
| **cogroup** ( *otherStream* , [ *numTasks* ]) | When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.                                                                                                                                                                                                                                                                                                                                                                |
| **transform** ( *func* )                        | Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.                                                                                                                                                                                                                                                                                                              |
| **updateStateByKey** ( *func* )                 | Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.                                                                                                                                                                                                                                    |

输出操作：

| Output Operation                                           | Meaning                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **print** ()                                         | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.``Python API This is called**pprint()** in the Python API.                                                                                                                                                                                                                               |
| **saveAsTextFiles** ( *prefix* , [ *suffix* ])   | Save this DStream's contents as text files. The file name at each batch interval is generated based on*prefix* and  *suffix* :  *"prefix-TIME_IN_MS[.suffix]"* .                                                                                                                                                                                                                                                                                              |
| **saveAsObjectFiles** ( *prefix* , [ *suffix* ]) | Save this DStream's contents as `SequenceFiles` of serialized Java objects. The file name at each batch interval is generated based on *prefix* and  *suffix* :  *"prefix-TIME_IN_MS[.suffix]"* .``Python API This is not available in the Python API.                                                                                                                                                                                                      |
| **saveAsHadoopFiles** ( *prefix* , [ *suffix* ]) | Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on*prefix* and  *suffix* :  *"prefix-TIME_IN_MS[.suffix]"* .``Python API This is not available in the Python API.                                                                                                                                                                                                                                       |
| **foreachRDD** ( *func* )                          | The most generic output operator that applies a function,*func* , to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function *func* is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |

我们之前的计算代码只是计算当前批次的数据，也是sparkstreaming默认的

基于上面官方提出了状态

## 状态

* 有状态 前后批次有联系
* 无状态 前后批次无联系

用于解决统计类问题

**updateStateByKey** ( *func* )：这个算子

```
def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
    val newCount = ...  // add the new values with the previous running count to get the new count
    Some(newCount)
}
val runningCounts = pairs.updateStateByKey[Int](updateFunction _)
```

代码如下：

```
package sparkstreaming
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.SparkConf
import tool.streamingcontext
object sparkstreaming1 {
  private val streamingcontext = new streamingcontext
  def main(args: Array[String]): Unit = {

    val ssc = streamingcontext.getstreamcotext()
    // 或者通过sparkcontext进行创建
    //val ssc = new StreamingContext(sc, Seconds(1))
    // 数据源
    val lines = ssc.socketTextStream("bigdata5", 9999)
    // 处理数据
    val words = lines.flatMap(_.split(" "))
    val pairs = words.map(word => (word, 1))
    val wordCounts = pairs.reduceByKey(_ + _)
    // 要指定checkpoint目录
    ssc.checkpoint("file:///D:\\checkpoint")
    val totalwc = pairs.updateStateByKey(updateFunction _)
    //wordCounts.updateStateByKey()
    // 打印数据当前批次
    wordCounts.print()
    totalwc.print()
    ssc.start()             // Start
    ssc.awaitTermination()  // Wait

    // 配置数据源在目标机器上执行nc -lk 9999 然后输入数据就ok了
  }

  def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
      // add the new values with the previous running count to get the new count
    val sum = newValues.sum
    val i = runningCount.getOrElse(0)
    Some(sum+i)
  }

}

```

但是这样也产生了个新问题

我们观察checkpoint文件夹

![](https://pic.imgdb.cn/item/63c4fa26be43e0d30e9044af.jpg)

生成很多个小文件

我们该如何解决

生产上我们不用

但是必备的知识还是要的

为了容错，恢复作业，和kafka里的一样

## checkpoint的存储东西

matestore 元数据

* conf 作业里的配置信息
* 算子操作
* 未完成的批次

Data

* 就是批次的数据

使用场景

* 作业失败的时候回复的时候用
* 转换算子的时候

但是注意生产上用不了

如何使用

```
Checkpointing can be enabled by setting a directory in a fault-tolerant, reliable file system (e.g., HDFS, S3, etc.) to which the checkpoint information will be saved. This is done by using streamingContext.checkpoint(checkpointDirectory). This will allow you to use the aforementioned stateful transformations. Additionally, if you want to make the application recover from driver failures, you should rewrite your streaming application to have the following behavior.

When the program is being started for the first time, it will create a new StreamingContext, set up all the streams and then call start().
When the program is being restarted after failure, it will re-create a StreamingContext from the checkpoint data in the checkpoint directory.
```

idea代码

```
// Function to create and setup a new StreamingContext
def functionToCreateContext(): StreamingContext = {
  val ssc = new StreamingContext(...)   // new context
  val lines = ssc.socketTextStream(...) // create DStreams
  ...
  ssc.checkpoint(checkpointDirectory)   // set checkpoint directory
  ssc
}

// Get StreamingContext from checkpoint data or create a new one
val context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)

// Do additional setup on context that needs to be done,
// irrespective of whether it is being started or restarted
context. ...

// Start the context
context.start()
context.awaitTermination()
```

### 缺点

小文件

修改代码就费了，就要重整

checkpoint用不了-》累计批次指标问题 -》 出现问题

## 如何实现相同功能？

实现存储到外部，如何根据而外部文件进行累计

## 使用checkpoint

解决checkpoint修改代码报错和小文件问题

所以简历上不可以出现我在生产上用过updateStateByKey，坚决不会用

## 如何把处理好的数据存储到外部

如下：

```
dstream.foreachRDD { rdd =>
  val connection = createNewConnection()  // executed at the driver
  rdd.foreach { record =>
    connection.send(record) // executed at the worker
  }
}
```

idea

```
package sparkstreaming
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import tool.{mysqlutils, streamingcontext,savefile}
object sparkstreaming1 {
  private val mysqlutils = new mysqlutils
  private val streamingcontext = new streamingcontext
  private val savefile = new savefile
  def main(args: Array[String]): Unit = {


    val ssc = streamingcontext.getstreamcotext()
    // 或者通过sparkcontext进行创建
    //val ssc = new StreamingContext(sc, Seconds(1))
    // 数据源
    val lines = ssc.socketTextStream("bigdata5", 9999)
    // 处理数据
    val words = lines.flatMap(_.split(" "))
    val pairs = words.map(word => (word, 1))
    val wordCounts = pairs.reduceByKey(_ + _)
    //val totalwc = pairs.updateStateByKey(updateFunction _)
    //wordCounts.updateStateByKey()
    // 打印数据当前批次
    wordCounts.print()
    //totalwc.print()
    // 把结果输入到mysql里 先在mysql里创建完表了
    // 下面会报错-> mysql链接没有进行序列化 ，我们不能加除非更改底层源码
    // closure 闭包 -> 方法内使用了方法外的变量 比如下述的connect
    wordCounts.foreachRDD(rdd=>{
      val connection = mysqlutils.getconnect("jdbc:mysql://bigdata2:3306/bigdata", "root", "liuzihan010616")
      rdd.foreach { record =>
        val sql = s"insert into wc values('${record._1}','${record._2}')"
        connection.createStatement.execute(sql)
      }
    })
// --------------------------------------------------------
    //对上述进行修改之后
    //这样是可以的但是性能不高
    //因为会一直拿链接，会造成性能下降
        wordCounts.foreachRDD(rdd=>{
          rdd.foreach { record =>
            val sql = s"insert into wc values('${record._1}','${record._2}')"
            val connection = mysqlutils.getconnect("jdbc:mysql://bigdata2:3306/bigdata", "root", "liuzihan010616")
            connection.createStatement.execute(sql)
          }
        })
    //优化性能
    wordCounts.foreachRDD(rdd=>{
      rdd.foreachPartition(record=>{
        val connection = mysqlutils.getconnect("jdbc:mysql://bigdata2:3306/bigdata", "root", "liuzihan010616")
        record.foreach(pari => {
          val sql = s"insert into wc values('${pari._1}','${pari._2}')"
          connection.createStatement.execute(sql)
        })
        mysqlutils.closeconnect(connection)
      })
    })
    // 再次进行优化 原因 -》 partition的数量过高
    // 通过连接池来进行
    // 或者通过coalse来控制这个分区数量
    dstream.foreachRDD { rdd =>
      rdd.foreachPartition { partitionOfRecords =>
        // ConnectionPool is a static, lazily initialized pool of connections
        val connection = ConnectionPool.getConnection()
        partitionOfRecords.foreach(record => connection.send(record))
        ConnectionPool.returnConnection(connection)  // return to the pool for future reuse
      }
    }
    // 利用sparksql的方式写入 最推荐
    // 性能也很好因为用的是spark的
    wordCounts.foreachRDD(rdd=>{
      val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()
      import spark.implicits._
      // Convert RDD[String] to DataFrame
      val wordsDataFrame = rdd.toDF("word","cnt")
      val srray:Array[String] = Array("append","jdbc:mysql://bigdata2:3306/bigdata","root","liuzihan010616","wc","word")
      savefile.savetojdbc(spark,wordsDataFrame,srray)
    })


    ssc.start()             // Start
    ssc.awaitTermination()  // Wait

    // 配置数据源在目标机器上执行nc -lk 9999 然后输入数据就ok了
  }

  def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
      // add the new values with the previous running count to get the new count
    val sum = newValues.sum
    val i = runningCount.getOrElse(0)
    Some(sum+i)
  }

}

```

## transform

### DStream和RDD之间交互的算子

需求 ：

* 一个数据是来自于mysql/文本数据 ： 量小 伪表
* 一个数据 来自kafka sss读取形成的DStream 量大 主业务线

实例：弹幕过滤功能

* 离线
* 实时

### 数据如下

```
主表：
不好看
垃圾
女主真好看
666
过滤的弹幕：
热巴真丑
鸡儿真美
王退出娱乐圈
```

### 离线:

```
package sparkstreaming
import org.apache.spark.sql.SparkSession
import tool._
object sparkstreaming2 {

  val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
  def main(args: Array[String]): Unit = {
    var mainsql = List(
      "不好看",
      "垃圾",
      "女主真好看",
      "666",
      "热巴真丑",
      "鸡儿真美",
      "王退出娱乐圈")
    val maintable = spark.sparkContext.parallelize(mainsql)

    var black = List(
      "热巴真丑",
        "鸡儿真美",
      "王退出娱乐圈"
    )

    val blacktable = spark.sparkContext.parallelize(black)

    val value1 = maintable.map(x => {
      (x, 1)
    })
    val value = blacktable.map(x => {
      (x, true)
    })
    value1.leftOuterJoin(value).filter(_._2._2.getOrElse(false)!=true).map(_._1).foreach(println(_))
  }

}

```

### 实时：

```
private val streamingcontext = new streamingcontext
def main(args: Array[String]): Unit = {
  val ssc = streamingcontext.getstreamcotext()
  val maintable = ssc.socketTextStream("bigdata5", 9099)
  var black = List(
    "热巴真丑",
    "鸡儿真美",
    "王退出娱乐圈"
  )

  val blacktable = ssc.sparkContext.parallelize(black)

  val value = blacktable.map(x => {
    (x, true)
  })

  val value1 = maintable.map(x => {
    (x, 1)
  })
  val value2 = value1.transform(x => {
    x.leftOuterJoin(value).filter(_._2._2.getOrElse(false) != true).map(_._1)
  })

  value2.print()
  
  ssc.start()
  ssc.awaitTermination()
```

## sparkstreaming和kafka整合

通过receiver方式读取kafka数据

kafka版本我们选择的是2.2.1

在sparkstreaming里默认的时候是至少一次

spark消费kafka数据形成的DStream里的分区数量和Kafka里的topic的分区数是一一对应的

分区数和task数是一一对应的-》并行度一一对应

官网：[kafkaonspark](https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration)

idea里的依赖

```
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
      <version>3.2.1</version>
    </dependency>
```

使用：

```
-------------------------------kafka消费数据
kafka-console-consumer.sh \
--bootstrap-server bigdata3:9092,bigdata4:9092,bigdata5:9092 \
--topic dl2262 \
--from-beginning 
-----------------------------kafka创建topic
kafka-topics.sh \
--create \
--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka \
--topic dl2262 \
--partitions 6 \
--replication-factor 3
-------------------------------kafka生产数据
kafka-console-producer.sh \
--broker-list bigdata3:9092,bigdata4:9092,bigdata5:9092 \
--topic dl2262
-------------------------------kafka查看topic
kafka-topics.sh \
--describe \
--zookeeper bigdata3:2181,bigdata4:2181,bigdata5:2181/kafka \
--topic dl2262 
-----------------------------代码
import org.apache.spark.sql.SparkSession
import tool._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
object sparkstreaming2 {

  private val streamingcontext = new streamingcontext

  def main(args: Array[String]): Unit = {
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "bigdata3:9092,bigdata4:9092,bigdata5:9092", // kafka地址
      "key.deserializer" -> classOf[StringDeserializer], // 反序列化
      "value.deserializer" -> classOf[StringDeserializer], // 反序列化
      "group.id" -> "dl2262-1", // 指定消费者组
      "auto.offset.reset" -> "latest", // 从什么地方开始消费
      "enable.auto.commit" -> (false: java.lang.Boolean) // offset的提交 是不是自动提交
    )

     val example = streamingcontext.getstreamcotext()
    val topics = Array("dl2262")
    val stream = KafkaUtils.createDirectStream[String, String](
      example,
      PreferConsistent, // 数据存储策略 Kafka数据均匀分在各个exector上，一共有三种
      Subscribe[String, String](topics, kafkaParams) // 固定写法
    )

    stream.map(record => (record.value)).print()

    example.start()
    example.awaitTermination()
  }
}
```

上述用的是新版本的kafka的api

消费kafka数据做wc 将结果-》mysql

```
package sparkstreaming
import org.apache.spark.sql.SparkSession
import tool._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

object sparkstreaming2 {

  private val streamingcontext = new streamingcontext
  private val savefile = new savefile
  def main(args: Array[String]): Unit = {
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "bigdata3:9092,bigdata4:9092,bigdata5:9092", // kafka地址
      "key.deserializer" -> classOf[StringDeserializer], // 反序列化
      "value.deserializer" -> classOf[StringDeserializer], // 反序列化
      "group.id" -> "dl2262-1", // 指定消费者组
      "auto.offset.reset" -> "latest", // 从什么地方开始消费
      "enable.auto.commit" -> (false: java.lang.Boolean) // offset的提交 是不是自动提交
    )


    val example = streamingcontext.getstreamcotext()
    val topics = Array("dl2262")
    val stream = KafkaUtils.createDirectStream[String, String](
      example,
      PreferConsistent, // 数据存储策略 Kafka数据均匀分在各个exector上，一共有三种
      Subscribe[String, String](topics, kafkaParams) // 固定写法
    )

    stream.map(record => (record.value)).print()

    val value = stream.map(record => (record.value)).flatMap(x => {
      x.split(",")
    }).map(word => {
      (word, 1)
    }).reduceByKey(_ + _)

    value.foreachRDD(rdd=>{
      val spark = SparkSession.builder().config(rdd.sparkContext.getConf).getOrCreate()
      import spark.implicits._
      val wordsDataFrame = rdd.toDF("word","cnt")
      val srray:Array[String] = Array("append","jdbc:mysql://bigdata2:3306/bigdata","root","liuzihan010616","wc","word")
      savefile.savetojdbc(spark,wordsDataFrame,srray)
    })

    example.start()
    example.awaitTermination()
  }}
```

消费完之后如果重启从上次挂掉的位置继续消费

要设置

* enable.auto.commit
* auto.offset.reset

才可以从断掉的位置开始

解决：

* 获取kafka offset
* 提交kafka offset

获取kafka的offset信息

```
    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      rdd.foreachPartition { iter =>
        val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
        println(rdd.partitions.size)
        println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
      }
    }
```

关于offset信息的解释：只要最后两列数据一样就代表这个topic里的数据都消费完了

```
6
dl2262 5 19 19
dl2262 4 18 18
dl2262 0 19 19
dl2262 2 77 77
dl2262 1 19 19
dl2262 3 46 47
-------------------------------------------
Time: 1673939535000 ms
-------------------------------------------
bidhashdas

6
dl2262 4 18 18
dl2262 3 47 47
dl2262 0 19 19
dl2262 5 19 19
dl2262 2 77 77
dl2262 1 19 19
```

注意：这些操作是获取到数据之后立刻这样做，就可以获得offset信息

其他对数据的进行操作，可以在这个里面做

如下：

```
    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      println(rdd.partitions.size)
      rdd.foreachPartition { iter =>
        val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
        println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
      }
      // wc
      val spark = SparkSession.builder().config(rdd.sparkContext.getConf).getOrCreate()
      import spark.implicits._

      val wordsDataFrame = rdd.map(_.value).flatMap(_.split(",")).map((_, 1)).reduceByKey(_ + _).toDF("word","cnt")
      val srray:Array[String] = Array("append","jdbc:mysql://bigdata2:3306/bigdata","root","liuzihan010616","wc","word")
      savefile.savetojdbc(spark,wordsDataFrame,srray)

      // 存储offset


      // 提交offset


    }
```

接下来，我们要进行提交offset

在提交offset之前

我们要存储offset

spark流处理 默认的就是至少一次

存储offfset

* checkpoints 不能用
* kafka本身 简单高效 -》消费语义-》至少一次/最多一次 ：但是最多一次我们不用 -》因为不支持事务 -》 无法支持精准一次
* 可以使用支持事务的存储结构进行精准一次的交付语义

kafka本身 ： 他存储的offset信息是存储在kafka的一共特殊的offset里比如_customer_offsets

```
stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
-------------------------------------------整体代码
package sparkstreaming
import org.apache.spark.sql.SparkSession
import tool._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.TaskContext
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

object sparkstreaming2 {

  private val streamingcontext = new streamingcontext
  private val savefile = new savefile
  def main(args: Array[String]): Unit = {
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "bigdata3:9092,bigdata4:9092,bigdata5:9092", // kafka地址
      "key.deserializer" -> classOf[StringDeserializer], // 反序列化
      "value.deserializer" -> classOf[StringDeserializer], // 反序列化
      "group.id" -> "dl2262-1", // 指定消费者组
      "auto.offset.reset" -> "latest", // 从什么地方开始消费
      "enable.auto.commit" -> (false: java.lang.Boolean) // offset的提交 是不是自动提交
    )


    val example = streamingcontext.getstreamcotext()
    val topics = Array("dl2262")
    val stream = KafkaUtils.createDirectStream[String, String](
      example,
      PreferConsistent, // 数据存储策略 Kafka数据均匀分在各个exector上，一共有三种
      Subscribe[String, String](topics, kafkaParams) // 固定写法
    )
    // 获取offset信息
    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      println(rdd.partitions.size)
      rdd.foreachPartition { iter =>
        val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
        println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
      }
      // wc
      val spark = SparkSession.builder().config(rdd.sparkContext.getConf).getOrCreate()
      import spark.implicits._

      val wordsDataFrame = rdd.map(_.value).flatMap(_.split(",")).map((_, 1)).reduceByKey(_ + _).toDF("word","cnt")
      val srray:Array[String] = Array("append","jdbc:mysql://bigdata2:3306/bigdata","root","liuzihan010616","wc","word")
      savefile.savetojdbc(spark,wordsDataFrame,srray)

      // 存储offset和提交offset
      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)




    }
    example.start()
    example.awaitTermination()
}
```

其他数据源：官方的

```
// The details depend on your data store, but the general idea looks like this

// begin from the offsets committed to the database
val fromOffsets = selectOffsetsFromYourDatabase.map { resultSet =>
  new TopicPartition(resultSet.string("topic"), resultSet.int("partition")) -> resultSet.long("offset")
}.toMap

val stream = KafkaUtils.createDirectStream[String, String](
  streamingContext,
  PreferConsistent,
  Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets)
)

stream.foreachRDD { rdd =>
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

  val results = yourCalculation(rdd)

  // begin your transaction

  // update results
  // update offsets where the end of existing offsets matches the beginning of this batch of offsets
  // assert that offsets were updated correctly

  // end your transaction
}
```

SSL的一些配置

```
val kafkaParams = Map[String, Object](
  // the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS
  "security.protocol" -> "SSL",
  "ssl.truststore.location" -> "/some-directory/kafka.client.truststore.jks",
  "ssl.truststore.password" -> "test1234",
  "ssl.keystore.location" -> "/some-directory/kafka.client.keystore.jks",
  "ssl.keystore.password" -> "test1234",
  "ssl.key.password" -> "test1234"
)
```
