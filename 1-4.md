---
title: spark
date: 1-4 8.40
categories: 日志
comments: "true"
tag: spark
---
# spark

spark产生背景？
mr,hive批处理，离线处理存在一些局限性：

* mr api 开发复杂
* 只能做离线计算，不能实时计算
* 性能不高

需求：

sql =》 mr

会产生多个job去完成一个需求

mr1=>mr2=>mr3

map => reduce

map处理完数据=》dask数据落盘 =》 reduce mr

kv进行操作 =》 k进行排序

什么是spark？

官网：`spark.apache.org  `

计算特点： 不关注数据存储

### 特点 ：

* Batch/streaming data =》 批流一体
* SQL analytics
* Data science at scale
* Machine learning

### 速度快：

* 基于内存的运算
* DAG =》 链式编程=》mr1=>mr2=>mr3
* pipline通道的
* 编程模型 线程级别的

### 易用性

* 开发语言：java ，Scala ， python ， sql
* 外部数据源
* 80多个高级算子=》Scala算子
* mr=》 去读mysql数据库 =》 DBinputformat
* spark => 封装好了多种外部数据源 =》 jdbc ， json ，csv
* mr map/reduce
* spark :80

通用性

* 子模块
* sparkcore =》 ：离线计算
* sparksql =》 离线计算
* sparkstreaming，structstreaming =》 实时计算
* mllib =》 机器学习
* 图计算 =》 图处理

对于spark的子模块之间可以用交互式使用

运行作业的地方

* yarn ***
* mesos
* k8s ***
* standalone

hadoop生态圈 vs spark生态圈

* Batch： mr ， hive vs sparkcore，sparksql
* Sql：hive，impala vs sparksql
* stream ： Strom vs spark streaming，sss
* MLLib：Mahout vs MLLib
* real存储 ： HBase，cassandra vs DataSource Api

### spark 能不能替换hadoop =》 替换不了=》spark =》 可以mr

spqrk版本：

* spark 1.x
* spark 2.x主流
* spark 3.x主流

编程模型：

sparkcore =》 RDD

sparksql =》 dataframe & dataset

sparkstreaming =》 DS

sparkcore

RDD：rdd开发 降低开发人员的开发成本 vs mr

什么是rdd？

lower level = 》mr

high level =》 spark 高级算子

### 优点：

* 弹性 分布式 数据集
* 数据集 =》 partitions 元素 =》 一条一条数据
* 可以用并行的方式进行计算

### 弹性？

* 容错 =》 计算的时候 可以重试

### 分布式？

* 存储
  * rdd：1 2 3 4 5 6
    * partition1:1 2 3
    * partition2: 4 5
    * partition:6
  * bigdata3:p1
  * bigdata4:p2
  * bigdtat5:p3
* 计算
  * 对rdd的操作是操作里面的数据
* 数据集
  * 就是构建rdd本身的数据
* immutable 不可变的
  * scala ： val var
  * rdda =》 rddb
  * 不可变 = 》rdda 通过一个计算到新的rdd
* partition collection of elements => rdd可以被分区存储/计算
* 一个rdd 是由多个partition所构成的
* rdd数据存储是分布式的，是跨节点进行存储的

abstract

T泛型 =》 限定在人dd里面数据 是什么类型的如：RDD[String] , RDD[Int],RDD[Student]

Serializable序列化=》可以经过网络传输

@transient注解 这个属性不用序列化 [了解]

### RDD 的特性：

* rdd 的底层存储是系列的partition
* 针对rdd做计算/操作其实就是对rdd底层的partition进行计算/操作
* rdd之间的依赖关系
  * rdda =》rddb
  * rdd 不可变
  * rdda = 》b =》 c
* Partitioner =》 kv类型的rdd
* 默认分区是hash
* 数据本地性 =》 减少数据传输的io ，优点
  * rdd进行操作的好处 ：
    * 有限的作业调度在数据所在的节点上 =》 理想状况
    * 常见计算 =》 作业调度在别的节点上 ，数据另外存储在一台节点上，只能把数据通过网络传过去再进行计算

在rdd中可以用scala的map和其他高级函数

RDD操作 ：

构建sparkcore 作业 ：idea

添加依赖 ：

```
 <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.12</artifactId>
      <version>3.2.1</version>
    </dependency>
```

mapreduce  ：程序入口：job

### 初始化spark：

* sparkContext =》 sparkcore 程序入口
* SparkConf => 指定spark app 详细信息

  * AppName = 》作业名字
  * Master =》作业运行在什么地方 spark作业运行模式
    * local，yarn，stanalone，k8s,mesos
    * 公共中：yarn，k8s，local
    * 测试的时候：local
    * 一个spark作业里面只能由一个sparkcontext
* 如和指定Master spark作业运行模式

  * local【K】模式

    * k指的是线程数
  * standalone => spark://HOST:PORT
  * yarn 两种模式：

    * client模式
    * cluster模式
  * k8s

    * k8s://HOST:PORT

rdd进行编程

创建rdd

parallelize existing collection  =》 已经存在的集合

referencing a dataset in an external storage system，hdfs、hbase、其他数据存储系统

外部数据源存储
			hdfs、local、hbase、s3、cos、
数据文件类型：
			text files, SequenceFiles, and any other Hadoop InputFormat.

spark部署：

spark不是部署分布式的 参考hive

spark支持分布式部署 =》 standalone

步骤 ： 解压 =》 软连接 =》 source

spark-core的脚本spark-shell

例子 ： `spark -shell --master local[2]`

启动spark-shell：测试code

* web ui =》 每个spark作业的 http://bigdata32:4040
* 参数：`--master => spark shell 以什么模式去运行`
* 可以用 `--name `更改spark shell的名字

以下是关于参数的详细情况

```
spark-shell : 
	--master  spark作业运行环境 
	--deploy-mode yarn模式 运行选择
	--class  spark作业包 运行主类main  class 包名 
	--name  指定spark作业的名字 
	--jars 指定第三方的依赖包
	--conf 指定spark作业配置参数 
yarn 参数补充： 
	--num-executors 指定 申请资源的参数 
	--executor-memory 指定 申请资源的参数 
	--executor-cores 指定 申请资源的参数
	--queue 指定作业 运行在yarn的哪个队列上 

spark-shell 交互式命令 底层调用 =》 spark-submit 
	开发者 主要使用的脚本 用于提交用户自己开发的spark作业

spark-shell: 
	spark-submit \
	--class org.apache.spark.repl.Main \
	--name "Spark shell" "$@"

spark-shell --master "local[2]": 
	spark-submit \
	--class org.apache.spark.repl.Main \
	--name "Spark shell"  --master "local[2]"
```

### 算子：

filter：

```
scala> test.collect
res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171...

scala> test.filter(_>999).collect
res16: Array[Int] = Array(1000)
scala> test.collect
res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171...

scala> test.filter(_>999).collect
res16: Array[Int] = Array(1000)

```

mapPartitionsWithIndex：就是和mappartition是基本上一样的就是多了个索引

```
val rdd = sc.makeRDD(List(1,2,3,4),numSlices = 2)//分区为2
scala> rdd.mapPartitionsWithIndex((index,iter)=> {if(index ==1) { iter } else { Nil.iterator}}).collect.foreach(println)
3
4
scala> rdd.mapPartitionsWithIndex((index,iter)=> {iter.map(num => {(index , num)})}).collect.foreach(println)
(0,1)
(0,2)
(1,3)
(1,4)
-=--------------------------------------------------------mappartition
scala> test1.mapPartitions(_.map(_._2),true).collect.foreach(print)
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000

```

一般的使用场景是查看partition里的元素

每个rdd的数据落到是什么分区上，我们不用太管，后面会讲

mapValues：只对kv类型的数据进行操作，相当于是单独对每个values做处理

```
scala> test1.mapValues(_+3).collect.foreach(print)
(0,4)(0,5)(0,6)(0,7)(0,8)(0,9)(0,10)(0,11)(0,12)(0,13)(0,14)(0,15)(0,16)(0,17)(0,18)(0,19)(0,20)(0,21)(0,22)(0,23)(0,24)(0,25)(0,26)(0,27)(0,28)(0,29)(0,30)(0,31)(0,32)(0,33)(0,34)(0,35)(0,36)(0,37)(0,38)(0,39)(0,40)(0,41)(0,42)(0,43)(0,44)(0,45)(0,46)(0,47)(0,48)(0,49)(0,50)(0,51)(0,52)(0,53)(0,54)(0,55)(0,56)(0,57)(0,58)(0,59)(0,60)(0,61)(0,62)(0,63)(0,64)(0,65)(0,66)(0,67)(0,68)(0,69)(0,70)(0,71)(0,72)(0,73)(0,74)(0,75)(0,76)(0,77)(0,78)(0,79)(0,80)(0,81)(0,82)(0,83)(0,84)(0,85)(0,86)(0,87)(0,88)(0,89)(0,90)(0,91)(0,92)(0,93)(0,94)(0,95)(0,96)(0,97)(0,98)(0,99)(0,100)(0,101)(0,102)(0,103)(0,104)(0,105)(0,106)(0,107)(0,108)(0,109)(0,110)(0,111)(0,112)(0,113)(0,114)(0,115)(0,116)(0,117)(0,118)(0,119)(0,120)(0,121)(0,122)(0,123)(0,124)(0,125)(0,126)(0,127)(0,128)(0,129)(0,130)(0,131)(0,132)(0,133)(0,134)(0,135)(0,136)(0,137)(0,138)(0,139)(0,140)(0,141)(0,142)(0,143)(0,144)(0,145)(0,146)(0,147)(0,148)(0,149)(0,150)(0,151)(0,152)(0,153)(0,154)(0,155)(0,156)(0,157)(0,158)(0,159)(0,160)(0,161)(0,162)(0,163)(0,164)(0,165)(0,166)(0,167)(0,168)(0,169)(0,170)(0,171)(0,172)(0,173)(0,174)(0,175)(0,176)(0,177)(0,178)(0,179)(0,180)(0,181)(0,182)(0,183)(0,184)(0,185)(0,186)(0,187)(0,188)(0,189)(0,190)(0,191)(0,192)(0,193)(0,194)(0,195)(0,196)(0,197)(0,198)(0,199)(0,200)(0,201)(0,202)(0,203)(0,204)(0,205)(0,206)(0,207)(0,208)(0,209)(0,210)(0,211)(0,212)(0,213)(0,214)(0,215)(0,216)(0,217)(0,218)(0,219)(0,220)(0,221)(0,222)(0,223)(0,224)(0,225)(0,226)(0,227)(0,228)(0,229)(0,230)(0,231)(0,232)(0,233)(0,234)(0,235)(0,236)(0,237)(0,238)(0,239)(0,240)(0,241)(0,242)(0,243)(0,244)(0,245)(0,246)(0,247)(0,248)(0,249)(0,250)(0,251)(0,252)(0,253)(0,254)(0,255)(0,256)(0,257)(0,258)(0,259)(0,260)(0,261)(0,262)(0,263)(0,264)(0,265)(0,266)(0,267)(0,268)(0,269)(0,270)(0,271)(0,272)(0,273)(0,274)(0,275)(0,276)(0,277)(0,278)(0,279)(0,280)(0,281)(0,282)(0,283)(0,284)(0,285)(0,286)(0,287)(0,288)(0,289)(0,290)(0,291)(0,292)(0,293)(0,294)(0,295)(0,296)(0,297)(0,298)(0,299)(0,300)(0,301)(0,302)(0,303)(0,304)(0,305)(0,306)(0,307)(0,308)(0,309)(0,310)(0,311)(0,312)(0,313)(0,314)(0,315)(0,316)(0,317)(0,318)(0,319)(0,320)(0,321)(0,322)(0,323)(0,324)(0,325)(0,326)(0,327)(0,328)(0,329)(0,330)(0,331)(0,332)(0,333)(0,334)(0,335)(0,336)(0,337)(0,338)(0,339)(0,340)(0,341)(0,342)(0,343)(0,344)(0,345)(0,346)(0,347)(0,348)(0,349)(0,350)(0,351)(0,352)(0,353)(0,354)(0,355)(0,356)(0,357)(0,358)(0,359)(0,360)(0,361)(0,362)(0,363)(0,364)(0,365)(0,366)(0,367)(0,368)(0,369)(0,370)(0,371)(0,372)(0,373)(0,374)(0,375)(0,376)(0,377)(0,378)(0,379)(0,380)(0,381)(0,382)(0,383)(0,384)(0,385)(0,386)(0,387)(0,388)(0,389)(0,390)(0,391)(0,392)(0,393)(0,394)(0,395)(0,396)(0,397)(0,398)(0,399)(0,400)(0,401)(0,402)(0,403)(0,404)(0,405)(0,406)(0,407)(0,408)(0,409)(0,410)(0,411)(0,412)(0,413)(0,414)(0,415)(0,416)(0,417)(0,418)(0,419)(0,420)(0,421)(0,422)(0,423)(0,424)(0,425)(0,426)(0,427)(0,428)(0,429)(0,430)(0,431)(0,432)(0,433)(0,434)(0,435)(0,436)(0,437)(0,438)(0,439)(0,440)(0,441)(0,442)(0,443)(0,444)(0,445)(0,446)(0,447)(0,448)(0,449)(0,450)(0,451)(0,452)(0,453)(0,454)(0,455)(0,456)(0,457)(0,458)(0,459)(0,460)(0,461)(0,462)(0,463)(0,464)(0,465)(0,466)(0,467)(0,468)(0,469)(0,470)(0,471)(0,472)(0,473)(0,474)(0,475)(0,476)(0,477)(0,478)(0,479)(0,480)(0,481)(0,482)(0,483)(0,484)(0,485)(0,486)(0,487)(0,488)(0,489)(0,490)(0,491)(0,492)(0,493)(0,494)(0,495)(0,496)(0,497)(0,498)(0,499)(0,500)(0,501)(0,502)(0,503)(1,504)(1,505)(1,506)(1,507)(1,508)(1,509)(1,510)(1,511)(1,512)(1,513)(1,514)(1,515)(1,516)(1,517)(1,518)(1,519)(1,520)(1,521)(1,522)(1,523)(1,524)(1,525)(1,526)(1,527)(1,528)(1,529)(1,530)(1,531)(1,532)(1,533)(1,534)(1,535)(1,536)(1,537)(1,538)(1,539)(1,540)(1,541)(1,542)(1,543)(1,544)(1,545)(1,546)(1,547)(1,548)(1,549)(1,550)(1,551)(1,552)(1,553)(1,554)(1,555)(1,556)(1,557)(1,558)(1,559)(1,560)(1,561)(1,562)(1,563)(1,564)(1,565)(1,566)(1,567)(1,568)(1,569)(1,570)(1,571)(1,572)(1,573)(1,574)(1,575)(1,576)(1,577)(1,578)(1,579)(1,580)(1,581)(1,582)(1,583)(1,584)(1,585)(1,586)(1,587)(1,588)(1,589)(1,590)(1,591)(1,592)(1,593)(1,594)(1,595)(1,596)(1,597)(1,598)(1,599)(1,600)(1,601)(1,602)(1,603)(1,604)(1,605)(1,606)(1,607)(1,608)(1,609)(1,610)(1,611)(1,612)(1,613)(1,614)(1,615)(1,616)(1,617)(1,618)(1,619)(1,620)(1,621)(1,622)(1,623)(1,624)(1,625)(1,626)(1,627)(1,628)(1,629)(1,630)(1,631)(1,632)(1,633)(1,634)(1,635)(1,636)(1,637)(1,638)(1,639)(1,640)(1,641)(1,642)(1,643)(1,644)(1,645)(1,646)(1,647)(1,648)(1,649)(1,650)(1,651)(1,652)(1,653)(1,654)(1,655)(1,656)(1,657)(1,658)(1,659)(1,660)(1,661)(1,662)(1,663)(1,664)(1,665)(1,666)(1,667)(1,668)(1,669)(1,670)(1,671)(1,672)(1,673)(1,674)(1,675)(1,676)(1,677)(1,678)(1,679)(1,680)(1,681)(1,682)(1,683)(1,684)(1,685)(1,686)(1,687)(1,688)(1,689)(1,690)(1,691)(1,692)(1,693)(1,694)(1,695)(1,696)(1,697)(1,698)(1,699)(1,700)(1,701)(1,702)(1,703)(1,704)(1,705)(1,706)(1,707)(1,708)(1,709)(1,710)(1,711)(1,712)(1,713)(1,714)(1,715)(1,716)(1,717)(1,718)(1,719)(1,720)(1,721)(1,722)(1,723)(1,724)(1,725)(1,726)(1,727)(1,728)(1,729)(1,730)(1,731)(1,732)(1,733)(1,734)(1,735)(1,736)(1,737)(1,738)(1,739)(1,740)(1,741)(1,742)(1,743)(1,744)(1,745)(1,746)(1,747)(1,748)(1,749)(1,750)(1,751)(1,752)(1,753)(1,754)(1,755)(1,756)(1,757)(1,758)(1,759)(1,760)(1,761)(1,762)(1,763)(1,764)(1,765)(1,766)(1,767)(1,768)(1,769)(1,770)(1,771)(1,772)(1,773)(1,774)(1,775)(1,776)(1,777)(1,778)(1,779)(1,780)(1,781)(1,782)(1,783)(1,784)(1,785)(1,786)(1,787)(1,788)(1,789)(1,790)(1,791)(1,792)(1,793)(1,794)(1,795)(1,796)(1,797)(1,798)(1,799)(1,800)(1,801)(1,802)(1,803)(1,804)(1,805)(1,806)(1,807)(1,808)(1,809)(1,810)(1,811)(1,812)(1,813)(1,814)(1,815)(1,816)(1,817)(1,818)(1,819)(1,820)(1,821)(1,822)(1,823)(1,824)(1,825)(1,826)(1,827)(1,828)(1,829)(1,830)(1,831)(1,832)(1,833)(1,834)(1,835)(1,836)(1,837)(1,838)(1,839)(1,840)(1,841)(1,842)(1,843)(1,844)(1,845)(1,846)(1,847)(1,848)(1,849)(1,850)(1,851)(1,852)(1,853)(1,854)(1,855)(1,856)(1,857)(1,858)(1,859)(1,860)(1,861)(1,862)(1,863)(1,864)(1,865)(1,866)(1,867)(1,868)(1,869)(1,870)(1,871)(1,872)(1,873)(1,874)(1,875)(1,876)(1,877)(1,878)(1,879)(1,880)(1,881)(1,882)(1,883)(1,884)(1,885)(1,886)(1,887)(1,888)(1,889)(1,890)(1,891)(1,892)(1,893)(1,894)(1,895)(1,896)(1,897)(1,898)(1,899)(1,900)(1,901)(1,902)(1,903)(1,904)(1,905)(1,906)(1,907)(1,908)(1,909)(1,910)(1,911)(1,912)(1,913)(1,914)(1,915)(1,916)(1,917)(1,918)(1,919)(1,920)(1,921)(1,922)(1,923)(1,924)(1,925)(1,926)(1,927)(1,928)(1,929)(1,930)(1,931)(1,932)(1,933)(1,934)(1,935)(1,936)(1,937)(1,938)(1,939)(1,940)(1,941)(1,942)(1,943)(1,944)(1,945)(1,946)(1,947)(1,948)(1,949)(1,950)(1,951)(1,952)(1,953)(1,954)(1,955)(1,956)(1,957)(1,958)(1,959)(1,960)(1,961)(1,962)(1,963)(1,964)(1,965)(1,966)(1,967)(1,968)(1,969)(1,970)(1,971)(1,972)(1,973)(1,974)(1,975)(1,976)(1,977)(1,978)(1,979)(1,980)(1,981)(1,982)(1,983)(1,984)(1,985)(1,986)(1,987)(1,988)(1,989)(1,990)(1,991)(1,992)(1,993)(1,994)(1,995)(1,996)(1,997)(1,998)(1,999)(1,1000)(1,1001)(1,1002)(1,1003)

```

flatMap：和Scala里的是一样的

```
scala> test.flatMap(x=>x.to(3)).collect
res98: Array[Int] = Array(1, 2, 3, 2, 3, 3)


```

其他的算子

glom：把每个分区的数据形成一个数组，比mapPartitionWithIndex好用

```
scala> test.glom.collect
res62: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, ...
```

sample：抽样，随机抽样的

```
scala> (test.sample(false,0.77)).collect.foreach(print)
236781112141516182022232425262728303132333435363738394041434446474850515254555657596061626364666768697172747576777879818285868789919293969798991001011021031041051061071081091101111121131161181191201211221241251271281301311321331341351361371381391401411441461471491501531541551561611621631641651661671681691701711721731741751761781791801811831841861871891901911931951961971982002012022042052072082092112122132142162172182202212222242252262272282302322332352362372382402412432442452472482502512522532542552562572582602612632642652662672682692702712732742752762772782812822832842892902922932942952962972982993003013023033043053063073083093113123133143153163173183193203213223233263293303313323333343353363373383393403413423443463473483493513523543573583593603613623633663673683693703713723743763773783793803813823833873903913923943954004014024034044054074084094114124134144154164174184194224234244264274284294304314324334344354364374384394404414424454464494504514524534544554564574584604614634644654664674684694714724744754764794804814824834844854864874884904914924934944954974984995005015025065075085105115125135145165175185195225235245255265275295305315325355375385395405415425445455465475485495515525545565575585595605625635645655675685715735755765795805815825835845855895905925935955965975985996016026036046066086096106116126136146156166176206216226236246256266276296316326336346356366376386396406416436446456466476486496506516536546556576586596606616626636646676696706716726736746756766776796806816836846856866876896906916926936946976986997007037047057067087097107117127157167177187197217227237267277297307327337347357367377387417427447457467477487497517527537547557567577587597607627637667677697707727737747757777787797807817827837847867887897907917937957967987998018028038048058068078098108128138148178198208228248258268278288308318328348358368378388408418428438458468478488498508528538548558588598608618628638648658668678688698708718758768778788798808828838848878888898908918958988999009019029039049059069079099129149159179189199209219229249259269279289299309319329339349359369379389399419429439449469489499509519549559569589599609619629639649659679689699709729749759769799809819829839859869879889899909919939949969979989991000

```

union：简单的数据合并，不去重

intersection：两个rdd的交集

subtract：出现在a里的没有出现在b里

.collect:把结果以数组的形式转到控制台

distinct：去重和sql效果一样 =》 底层去重的方法是用reduceByKey进行去重的目的

```
scala> val dd = sc.parallelize(List(1,2,2,3,4,5,6,7,8))
dd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at <console>:24

scala> dd.collect.foreach(print)
122345678
scala> dd.collect.foreach(println)
1
2
2
3
4
5
6
7
8

scala> dd.distinct.collect.foreach(println)
4
6
8
2
1
3
7
5

```

kv算子 ： groupbykey =》 就是对key进行分组，和wordcount里的分组是一样的=》工作中不要使用，效率低，不灵活

预聚合：

* mr ： input = > map => combine(调优的过程) => reduce => output
* combine => 预聚合 按照map的输出的key进行数据聚合

mapSideCombine = false 代表预聚合关闭

一般groupbykey是关闭预聚合的，reducebykey是开启的

```
scala> test1.collect
res25: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9), (0,10), (0,11), (0,12), (0,13), (0,14), (0,15), (0,16), (0,17), (0,18), (0,19), (0,20), (0,21), (0,22), (0,23), (0,24), (0,25), (0,26), (0,27), (0,28), (0,29), (0,30), (0,31), (0,32), (0,33), (0,34), (0,35), (0,36), (0,37), (0,38), (0,39), (0,40), (0,41), (0,42), (0,43), (0,44), (0,45), (0,46), (0,47), (0,48), (0,49), (0,50), (0,51), (0,52), (0,53), (0,54), (0,55), (0,56), (0,57), (0,58), (0,59), (0,60), (0,61), (0,62), (0,63), (0,64), (0,65), (0,66), (0,67), (0,68), (0,69), (0,70), (0,71), (0,72), (0,73), (0,74), (0,75), (0,76), (0,77), (0,78), (0,79), (0,80), (0,81), (0,82), (0,83), (0,84), (0,85), (0,86), (0,87), (0,88), (0,89), (0,90), (0,91), (0,92), (0,93), (0,...
scala> test1.groupByKey.collect.foreach(print)
(0,CompactBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500))(1,CompactBuffer(501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000))

```

reducebykey：对比groupby是相当于可以统计之后进行计算的

```
scala> test1.reduceByKey((x,y)=>{x+y}).collect.foreach(print)
(0,125250)(1,375250)
--------------------------------------其中的x+y代表拉完成之后，对他们进行相加
--------------------------------------通过reduceByKey实现distinct
scala> test1.reduceByKey((x,_)=>{x}).map(_._1).collect.foreach(println)
0
1

```

groupby：自定义分组

```
scala> test.groupBy(x=>{if(x%2==0){"2e"}else{"e2"}}).collect
res99: Array[(String, Iterable[Int])] = Array((e2,CompactBuffer(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 30...

```

sortbykey：按照key进行排序，分区排序，如果想达到全局排序，则要求你rdd里的只有一个的分区，降序就是把true变成false

```

scala> val r2 = sc.parallelize(List(("zuan",18),("kaige",20),("zihang",21)),1)
r2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[69] at parallelize at <console>:24

scala> r2.sortByKey(true)
res89: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[70] at sortByKey at <console>:25

scala> res89.collect
res90: Array[(String, Int)] = Array((kaige,20), (zihang,21), (zuan,18))

```

自定义排序：sortby

```
scala> r2.sortBy(x=>x._2,true).collect
res92: Array[(String, Int)] = Array((zuan,18), (kaige,20), (zihang,21))


```

join：他默认就是按照key进行关联的=》底层调用的是cogroup

```
scala> val r3 = sc.parallelize(List(("zuan","广西"),("kaige","中国"),("zihang","黑龙江")))
r3: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[77] at parallelize at <console>:24

scala> r1.join(r3).collect
res93: Array[(String, (Int, String))] = Array((zuan,(18,广西)), (kaige,(20,中国)), (zihang,(21,黑龙江)))

-------------------------cogroup
scala> r1.cogroup(r3).collect
res94: Array[(String, (Iterable[Int], Iterable[String]))] = Array((zuan,(CompactBuffer(18),CompactBuffer(广西))), (kaige,(CompactBuffer(20),CompactBuffer(中国))), (zihang,(CompactBuffer(21),CompactBuffer(黑龙江))))



```

都是根据key进行关联

但是cogroup的返回是集合当作value的

join则是返回的是值当value

分区规则：

```
分区号：0，元素是4 4%4=0
分区号：1，元素是9 9%4=1
```

action算子：会执行job的算子

collect=》把rdd的数据集拉回控制台 = 》driver端

foreach

foreachpartition：对每个分区进行处理=》首选的mysql数据集导入是它，因为获取mysql的次数能少一点

```
  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
  }
-----------------------------------使用
scala> test.foreachPartition(ax=>ax.foreach(print))
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000

```

reduce：mr里的reduce，这里不可以接collect，因为已经完成了

```
scala> test.reduce((x,y)=>x+y)
res108: Int = 500500

```

first：取出数据集里的第一个元素底层是take

```
scala> test.first()
res109: Int = 1
-------------------------take
scala> test.take(77)
res112: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77)


```

takeOrderd:升序获取前n个

```
scala> test.takeOrdered(2)
res127: Array[Int] = Array(1, 2)

scala> test.takeOrdered(55)
res128: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55)

```

top:就是取排名前几的数据底层是takeOrederd=》数据量比较小的时候可以使用

```
scala> test.top(1)
res113: Array[Int] = Array(1000)

scala> test.top(5)
res114: Array[Int] = Array(1000, 999, 998, 997, 996)

```

saveASTextFile

saveASSequenceFile

saveAsObjectFile

countByKey:统计key的个数

```
scala> test1.countByKey
res125: scala.collection.Map[Int,Long] = Map(0 -> 500, 1 -> 500)

```

collectAsMap：和countByKey有点类似

count:返回rdd里有多少个数

判断action算子和普通算子的方法=》源码底层有runjob

=》调用collect/其他action算子

### 案例：

```
一张表：
name price num
diar 300 1000
香奈儿 4000 2
螺蛳粉 200 98
30显卡 200 10
-----------------------------------------------按照价格进行排序【desc】，如果价格相同，按照库存排序【asc】

```

解决：

```
数据类型：tuple【推荐】 ， class ， case class【推荐】
用tuple做
-----------------------------------------------------------------
    val value = sc.parallelize(List("diar 300 1000",
      "香奈儿 4000 2",
      "螺蛳粉 200 98",
      "30显卡 200 10")，1)

    val etlData = value.map(x=>{
      val strings=x.split(" ")
      val name=strings(0)
      val price=strings(1).toInt
      val store=strings(2)
      (name,price,store)
    })

    etlData.sortBy(x => ( -x._2 , x._3)).saveAsTextFile("hdfs://bigdata3:9000/data")
----------------------------------------------------------class

import org.apache.spark.{SparkConf, SparkContext}
import sparkfirst.ContextUtils
object sparktest {
  def main(args: Array[String]): Unit = {
    val sc:SparkContext = ContextUtils.getSparkContext("test")
    val bb = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))

    val value = sc.parallelize(List("diar 300 1000",
      "香奈儿 4000 2",
      "螺蛳粉 200 98",
      "30显卡 200 10")，1)

    val etlData = value.map(x=>{
      val strings=x.split(" ")
      val name=strings(0)
      val price=strings(1).toDouble
      val store=strings(2).toInt
      new skuu(name,price,store)
    })
    etlData.sortBy(x=>(-x.d,x.str1)).collect.foreach(print(_))

    sc.stop()
  }
  class skuu(val str: String,val d: Double,val str1: Int) extends Serializable{
    override def toString: String =str + "\t" + d + "\t" + str1
  }
}
------------------------------------------------------------------------------------------------------case class
case好处=》重写了toString，hashcode方法，自动实现了序列化，不用实例化
package sparkfirst

import org.apache.spark.{SparkConf, SparkContext}
import sparkfirst.ContextUtils
object sparktest {
  def main(args: Array[String]): Unit = {
    val sc:SparkContext = ContextUtils.getSparkContext("test")
    val bb = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))

    val value = sc.parallelize(List("diar 300 1000",
      "香奈儿 4000 2",
      "螺蛳粉 200 98",
      "30显卡 200 10")，1)

    val etlData = value.map(x=>{
      val strings=x.split(" ")
      val name=strings(0)
      val price=strings(1).toDouble
      val store=strings(2).toInt
      skuu(name,price,store)
    })
    etlData.sortBy(x=>(-x.d,x.str1)).collect.foreach(print(_))

    sc.stop()
  }
 case class skuu(val str: String,val d: Double,val str1: Int)
}


```

当用 `saveAsTextFile("hdfs://bigdata3:9000/data/test")`

的时候它会根据你的分区数来生成文件

### 需求：用两类进行 对比：

```
package sparkfirst

import org.apache.spark.{SparkConf, SparkContext}
import sparkfirst.ContextUtils
object sparktest {
  def main(args: Array[String]): Unit = {
    val sc:SparkContext = ContextUtils.getSparkContext("test")
    val bb = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))

    val value = sc.parallelize(List(
      "diar 300 1000",
      "香奈儿 4000 2",
      "螺蛳粉 200 98",
      "30显卡 200 10"),1)

    val etlData = value.map(x=>{
      val strings=x.split(" ")
      val name=strings(0)
      val price=strings(1).toDouble
      val store=strings(2).toInt
      skuu(name,price,store)
    })
    etlData.sortBy(x=>(-x.d,x.str1)).collect.foreach(print(_))

    sc.stop()
  }
 case class skuu(val str: String,val d: Double,val str1: Int) extends Ordered[skuu]{
   override def compare(that: skuu): Int = {
     if (this.d == that.d){
       this.str1-that.str1
     }else {
       -(this.d - that.d).toInt
     }
   }
 }
}



```

隐式转换

```
package sparkfirst

import org.apache.spark.{SparkConf, SparkContext}
import sparkfirst.ContextUtils
object sparktest {
  def main(args: Array[String]): Unit = {
    val sc:SparkContext = ContextUtils.getSparkContext("test")
    val bb = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))

    val value = sc.parallelize(List(
      "diar 300 1000",
      "香奈儿 4000 2",
      "螺蛳粉 200 98",
      "30显卡 200 10"),1)

    val etlData = value.map(x=>{
      val strings=x.split(" ")
      val name=strings(0)
      val price=strings(1).toDouble
      val store=strings(2).toInt
      skuu(name,price,store)
    })
    etlData.sortBy(x=>(-x.d,x.str1)).collect.foreach(print(_))

    sc.stop()

    implicitly def skutooreder(sku:skuu):Ordered[skuu]={
      new Ordered[skuu]{
        override def compare(that: skuu): Int = {
          if (sku.d == that.d){
            sku.str1-that.str1
          }else {
            -(sku.d - that.d).toInt
          }
        }
      }

    }


  }
  
  
  
  
  
  
  
  
  
  
  
  
 case class skuu(val str: String,val d: Double,val str1: Int)
}



```

例子：

```
数据：
word show click
a,2,3
b,1,1
c,4,5
f,5,6
g,7,8
k,8,9
a,1,2
a,1,1
a,4,5
b,5,6
------------------------------------------------------------------------------
package sparkfirst

import org.apache.spark.{SparkConf, SparkContext}
import sparkfirst.ContextUtils
object sparktest {

  def sub(name: String, tuple: (Double, Int))={
    (name , tuple)
  }

  def main(args: Array[String]): Unit = {
    val sc:SparkContext = ContextUtils.getSparkContext("test")
    val bb = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))

    val value = sc.parallelize(List(
      "a,2,3",
      "b,1,1",
      "c,4,5",
      "f,5,6",
      "g,7,8",
      "k,8,9",
      "a,1,2",
      "a,1,1",
      "a,4,5",
      "b,5,6"),1)

    val etlData = value.map(x=>{
      val strings=x.split(",")
      val name=strings(0)
      val price=strings(1).toDouble
      val store=strings(2).toInt
      sub(name,(price,store))
     // (name,(price,store))
    })

    etlData.reduceByKey((x,y)=>{
      (x._1+y._1,x._2+y._2)
    }).map(x=>x._1+"\t"+x._2._1+"\t"+x._2._2+"\t")
   // etlData.reduceByKey((x,y)=>{(x._1+y._1,x._2+y._2)}).map(x=>x._1+"\t"+x._2._1+"\t"+x._2._2+"\t")
  
    sc.stop()
  }
```

### 理论：

spark架构：

* Application =》 spark作业 =》 driver program 和 executor on the cluster两个进程
  * driver：运行sparkContext
  * exacutor：运行任务并保存数据在内存和磁盘中
* web ui
* sparkcontext
* application jar =》 开发好的代码生成的jar包 =》 包含spark作业 =》 包含 main方法 =》 用户自己开发完spark之后可以部署到服务器上
* driver program =》 运行 jar包里的main方法 =》 创建sparkContext
* Custer manager =》 集群管理者 =》 通过集群获取资源
* Deploy mode =》 当把作业提交到yarn上的时候
  * cluster模式：是跑在集群内布的（driver）=》 yarn所在的机器里面
  * client模式：跑在集群外外的
* Worker node =》 工作节点=》打工人=》运行集群代码==node manager
* excutor = 》相当于yarn的container=》每个spark都有自己的excutor
* task =》partition =》 rdd
* job =》 spark 里的job =》 application里的job =》 一个application里可能会有多个job
* stage =》 job的小单位，且每个stage之间是有依赖关系的
* 一个application会包含1-n个job，一个job包含1-n个stage
* 一个stage可以包含1-n个task
* task和rdd里的分区数一一对应

### spark的执行流程

sc去链接cluster manager

cluster manager 会给spark作业分配资源

spark一旦连接上Custer

启动=》exector=》存储和计算

sc发送代码 给 exector 发送task给他去运行

每个作业都是有自己的exector的

exector是相当于container=》资源隔离=》调度隔离

多个作业之间产生的数据是不可以进行共享的，但是当写到一个外部存储上，就可以了

spark-shell简单就是=》 提交很多个job =》相当于外部存储

用户无感知的连接到集群

可以监听exector的生命周期=》和driver之间有 通信

只要driver可以连接到Custer集群上，他就可以=》也就是说外部dirver也可以

提议：driver和work节点靠近一点，这样可以减少网络发送的时间，就是可以用本地来实现

### spark整合yarn

首先在spark的conf文件夹里执行 `cp spark-env.sh.template spark-env.sh`

然后 `vim spark-env.sh`

把 `HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop 和 YARN_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop`

加上之后重新启动spark执行 `spark-shell --master yarn`

就可以了

一般启动之后，不配别的，一般这样是占用5G的内存

### 案例：

spark统计用户行为分析

```
    val value = sc.parallelize(List(
      "u01,英雄联盟|绝活&职业|云顶|奴神,1,1",
      "u01,英雄联盟|绝活&职业|云顶|金潺潺,1,1",
      "u01,英雄联盟|绝活&职业|云顶|带粉上车,1,0",
      "u02,星秀|好声音|女团|三年一班,1,1",
      "u02,星秀|好声音|女团|奴神,1,1",
      "u02,星秀|好声音|女团|将神,1,0",
      "u02,星秀|好声音|女团|西索,1,1"),1)

    val etlData = value.flatMap(x=>{
      val strings=x.split(",")
      val name=strings(0)
      val type_log_total=strings(1)
      val show=strings(2).toInt
      val click=strings(3).toInt
      val type_log_total_ni=type_log_total.split("\\|")
      type_log_total_ni.map(x=>{
        ((name,x),(show,click))
      })
      //sub(name,(price,store))
      //(name,(price,store))
    })

//    etlData.reduceByKey((x,y)=>{
//      (x._1+y._1,x._2+y._2)
//    }).map(x=>x._1+"\t"+x._2._1+"\t"+x._2._2+"\t")
   //etlData.reduceByKey((x,y)=>{(x._1+y._1,x._2+y._2)}).map(x=>x._1+"\t"+x._2._1+"\t"+x._2._2+"\t")

    etlData.reduceByKey((x,y)=>{
      (x._1+y._1,x._2+y._2)
    }).collect().foreach(println)
```

### spark持久化:

* RDD持久化 =》 一个操作会有很多次的生成rdd，比如会有100个rdd，我们为了节省时间以及链路资源，可以把第99个rdd持久化，然后只用对第99个操作就好了
* 持久化也是可以容错的
* 就是保存在内存中
* 是针对rdd的每个分区来的
* 持久化操作，是下次操作的时候才会从持久化的地方加载数据
* 默认存储级别是在内存中存储
* 用persist或者cache都可以 =》 是lazy的
* 当内存不足的时候是不能进行持久化的 我们可以通过序列化，进行配置= 》 减少使用空间
* 一般我们要在内存和cpu之间做权衡 =》4步选择法
  * 官方=》memory_only
  * memory_only_ser => 对空间会节省
  * 磁盘上 = 》 memory_only_desk => 没有不做序列化快
  * 容错 =》 带有副本的方式，的确是更安全可是数据太大对磁盘是负担
* 移除持久化数据：可以设置自动的级别（[LRU](https://blog.csdn.net/qq_46517733/article/details/123092629)），就会过一段时间，然后自动/或者通过rdd调用 `unpersist(true)`他是立即执行的
* 

启动spark-shell本身也是一个spark作业

```
scala> val test = sc.parallelize("hdfs://bigdata3:9000/flume/events/2022-12-13/events.1670898548750.log")
test: org.apache.spark.rdd.RDD[Char] = ParallelCollectionRDD[0] at parallelize at <console>:23

scala> test.collect
collect   collectAsync

scala> test.collect
res0: Array[Char] = Array(h, d, f, s, :, /, /, b, i, g, d, a, t, a, 3, :, 9, 0, 0, 0, /, f, l, u, m, e, /, e, v, e, n, t, s, /, 2, 0, 2, 2, -, 1, 2, -, 1, 3, /, e, v, e, n, t, s, ., 1, 6, 7, 0, 8, 9, 8, 5, 4, 8, 7, 5, 0, ., l, o, g)

scala> test.persist
res1: test.type = ParallelCollectionRDD[0] at parallelize at <console>:23

scala> test.ca
cache   cartesian

scala> test.cache
res2: test.type = ParallelCollectionRDD[0] at parallelize at <console>:23
---------------------------------------------------------------------------------------java的序列化方法
    val names = Array[String]("刘子航","李信","花木兰","达摩","耀","貂蝉","吕布")
    val gar = Array[String]("男","女")
    val addres= Array[String]("山东","广西","大连")


    val value1 = sc.parallelize(1 to 300000)

    val value2 = new ArrayBuffer[persion]()
    val value3 = value1.map(x => {
      val name = names(Random.nextInt(6))
      val s = gar(Random.nextInt(1))
      val s1 = addres(Random.nextInt(2))
      value2 += (persion(name, s, s1))
    })

    value3.persist(StorageLevel.MEMORY_ONLY_SER)
    value3.count()


case class persion(name: String,gre:String,add:String){

}
-------------------------------------------------------Kyro序列化
速度比java快，但是不是支持所有的序列化的，有的没有，使用前要加上注册类

要先在conf里使用它
 conf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")
然后要注册所用的case class或者class 
 conf.registerKryoClasses(Array(classOf[Info]))
然后其余和上述一样

    val names = Array[String]("刘子航","李信","花木兰","达摩","耀","貂蝉","吕布")
    val gar = Array[String]("男","女")
    val addres= Array[String]("山东","广西","大连")


    val value1 = sc.parallelize(1 to 300000)

    val value2 = new ArrayBuffer[persion]()
    val value3 = value1.map(x => {
      val name = names(Random.nextInt(6))
      val s = gar(Random.nextInt(1))
      val s1 = addres(Random.nextInt(2))
      value2 += (persion(name, s, s1))
    })

    value3.persist(StorageLevel.MEMORY_ONLY_SER)
    value3.count()


case class persion(name: String,gre:String,add:String){

}













```

### 依赖关系和血缘关系

血缘关系 ： =》就是不同rdd之间的转化

依赖关系 =》

* 宽依赖 ： 一个父RDD里的parttion会被子RDD的partition使用多次 =》 会产生shuffer =》 有新的stage产生 = 》一个shuffle会划分两个stage =》引起shuffle就会产生stage
* 窄依赖 ： 同一个父RDD里的分区最多被子RDD使用一次 =》一个stage里完成的 =》 无shuffle

### 补充算子 ：

repartition ： `repartition（num）`重新分区 ，引起shuffle =》 底层调用的是coalesce =》 不可以减少分取数

coalesce ： 一般用于减少RDD的分区数量，`coalesce(num)`窄依赖的 =》 不引起shuffle =》 可以增加分区数

生产上用于调整计算的并行度

### 判断执行位置

判断是不是对rdd里的元素进行操作，如果是操作，则是executor端的，如果不是则是driver端

例子 ：求top2

```

    val value = sc.parallelize(List(
      "www.bvaidu,u01,20",
      "www.githuba,u02,2",
      "www.bvaidu,u02,100",
      "www.bibi,u02,199",
      "www.githuba,u01,100",
      "www.githuba,u01,1",
      "www.githuba,u01,10",
      "www.bibi,u02,19",
      "www.bibi,u01,199",
      "www.baidu.com,uid01,1",
      "www.baidu.com,uid01,10",
      "www.baidu.com,uid02,3",
      "www.baidu.com,uid02,5",
      "www.github.com,uid01,11",
      "www.github.com,uid01,10",
      "www.github.com,uid02,30",
      "www.github.com,uid02,50",
      "www.bibili.com,uid01,110",
      "www.bibili.com,uid01,10",
      "www.bibili.com,uid02,2",
      "www.bibili.com,uid02,3"),1)

    val etlData = value.map(x=>{
      val strings=x.split(",")
      val yuming=strings(0)
      val user=strings(1)
      val cishu=strings(2).toInt
      ((yuming,user),(cishu))
      //sub(name,(price,store))
      //(name,(price,store))
    })
etlData.reduceByKey((x,y)=>{
      x+y
    }).sortBy(x=> -x._2,true).map(x=>{
      (x._1._2,(x._1._1,x._2))
    }).groupByKey().map(x=>{
        x._2.map(s=>{
          (x._1,s._1,s._2)
      }).take(2)}).saveAsTextFile("hdfs://bigdata3:9000/input/10.txt")
------------------------------------------------------------------------------简化版本
    val value = sc.parallelize(List(
      "www.bvaidu,u01,20",
      "www.githuba,u02,2",
      "www.bvaidu,u02,100",
      "www.bibi,u02,199",
      "www.githuba,u01,100",
      "www.githuba,u01,1",
      "www.githuba,u01,10",
      "www.bibi,u02,19",
      "www.bibi,u01,199",
      "www.baidu.com,uid01,1",
      "www.baidu.com,uid01,10",
      "www.baidu.com,uid02,3",
      "www.baidu.com,uid02,5",
      "www.github.com,uid01,11",
      "www.github.com,uid01,10",
      "www.github.com,uid02,30",
      "www.github.com,uid02,50",
      "www.bibili.com,uid01,110",
      "www.bibili.com,uid01,10",
      "www.bibili.com,uid02,2",
      "www.bibili.com,uid02,3"),1)

    val etlData = value.map(x=>{
      val strings=x.split(",")
      val yuming=strings(0)
      val user=strings(1)
      val cishu=strings(2).toInt
      ((yuming,user),(cishu))
      //sub(name,(price,store))
      //(name,(price,store))
    })
    val value4 = etlData.map(x => {
      (x._1._2)
    }).distinct().collect()


    for (elem <- value4){
     etlData.filter(_._1._2 == elem).reduceByKey(_ + _).sortBy( -_._2).take(2).foreach(println(_))
    }

```

累加器：广播变量=》后续再补充

案例 ： wordcount

```
    val wc = sc.textFile("hdfs://bigdata3:9000/3.log")

    wc.flatMap(x=>{
      x.split(",")
    }).map(x=>{
      (x,1)
    }).reduceByKey((x,y)=>{
      x+y
    }).saveAsTextFile("hdfs://bigdata3:9000/input/11.txt")
```

部署spark作业

* jar
* spark-submit

```
spark-submit \
--class 包名 \
--master 模式 \
--name 作业名字 \
jar包在机器上的路径 \
想传的参数
-------------------------------------例子
spark-submit \
--class tool.jdbc.readjdbc \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
"jdbc:mysql://bigdata2:3306/try" "root" "liuzihan010616" "emp"
-----------------------------------------我这里不加msater是因为我再代码里已经加了
```

还可以用其内封装的方法进行传值=》不用 args

就是在submit的时候用 `--conf`传入

代码如下  :

```
package com.dl2262.sparkcore.day02

import com.dl2262.sparkcore.util.{ContextUtils, FileUtils}
import org.apache.spark.SparkContext
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD

/**
  *
  * @author sxwang
  *         01 05 8:28
  */
object WCApp extends Logging{

  def main(args: Array[String]): Unit = {

//    if(args.size != 2){
    //      logError("请正确输入2个参数：<input> <output>")
    //      System.exit(0)
    //    }
    //    val in = args(0)
    //    val out = args(1)


    val sc: SparkContext = ContextUtils.getSparkContext(this.getClass.getSimpleName)

    val in = sc.getConf.get("spark.input.path","hdfs://bigdata32:9000/input/")
    val out = sc.getConf.get("spark.output.path","hdfs://bigdata32:9000/output/")


    val input = sc.textFile(in)

    FileUtils.deletePath( sc.hadoopConfiguration,out)

    input.flatMap(line  => {
      line.split(",")
    }).map(word => (word,1))
      .reduceByKey(_+_)
      .saveAsTextFile(out)

    sc.stop()
  }





}

```

例子：

```
数据类型如下：
域名，用户，用户所在地，展示次数，点击次数
求分别按照域名，用户，用户所在地进行统计其top2
-----------------------------------------------------------------------------------制造数据
  val domin = Array[String]("www.baidu.com","www.taobao.com","www.github.com","www.bilbil.com","www.csdn.com","www.zihang.com")
    val userList = Array[String]("zihang","zuan","zihao","shuangxi","yuhang")
    val beianlocal = Array[String]("广州","江西","太原","新疆","上海")
    var stringe:List[String] = Nil
    for(i <- 1 to(30000)){
      val Randomdomin = domin(Random.nextInt(domin.length-1))
      val RandomUserList = userList(Random.nextInt(userList.length-1))
      val Randombeianlocal = beianlocal(Random.nextInt(beianlocal.length-1))
      val tmp = List(Randomdomin + "," + RandomUserList + "," + Randombeianlocal )
      stringe = stringe++tmp
    }
-------------------------------------------------------------------------------域名 =>其他有时间再补充把，和这个一样
 val basicdata = sc.parallelize(stringe)
-------------------------------------------------------------------------------解析数据
    val ETLDATA = basicdata.map(x=>{
      val strings = x.split(",")
      val currentdomin = strings(0)
      val currentuser = strings(1)
      val currentadd = strings(2)
      val click = Random.nextInt(100).toInt
      val show = Random.nextInt(200).toInt
      ((currentdomin,currentuser,currentadd),(click,show))
    })
--------------------------------------------------------------------------------处理并排序
      for (elem <- domin){
        ETLDATA.filter(_._1._1==elem).reduceByKey((x,y)=>{
          (x._1+y._1,x._2+y._2)
        }).sortBy( -_._2._1).take(2).foreach(println(_))
      }
---------------------------------------------------------------------------------方法2
    ETLDATA.reduceByKey((x,y)=>{
      (x._1+y._1,y._2+x._2)
    }).sortBy(x=> -x._2._1).map(x=>{
      (x._1._1,(x._1._2,x._1._3,x._2))
    }).groupByKey().map(x=>{
      x._2.map(s=>{
        (x._1,(s._1,s._2,s._3))
      }).take(2)
    }).collect
```
