---
title: hadoop
date: 11/11/2022 10:07:37 AM  
categories: 日志
comments: "true"
---
# hadoop简介 #
- 以阿帕奇软件 ，hadoop为主的生态圈 
- 狭义就是Hadoop
## 组件 ##
- hdfs ： 存储海量的数据
- mapreduce ： 计算分析
- yarn ： 资源和作业的调度
### 观点 ###
- 存储是第一位的
- 计算是第二位的
## 学习介绍 ##
- 官网进行学习
- `hadoop.apache.org`
- 但是后面的其余框架对应的官网就是 把hadoop 改掉
 
```

		The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.


```
### hadoop模块的简介 ###
- hdfs ：


```

		A distributed file system that provides high-throughput access to application data.
```

- mapredurce :

```

		A YARN-based system for parallel processing of large data sets
```

- yarm : 

```

		A framework for job scheduling and cluster resource management

```
### 版本以及主流 ###
- 版本 ： 1.x ， 2.x ， 3.x
- 主流 ： 2.x -》 3.x
- 公司所用 ： 
- apache 原生
- cdh ： 5.x ， 6.x 从6.3之后开始收费

## 部署 ##
- 安装的是什么？
- Hadoop有什么？
- 我们只用部署 yarn hdfs 因为mapredurce是java代码人员给我们的 ：都是主从架构的
- hdfs ： 
- namenode : 老大 负责指挥数据的存储
- datanode ： 主要负责数据的存储
- seconderynamenode ： 负责辅助namenode的
- yarn :
- resourcemanager : 老大 负责资源分配
- nodemanager ： 小弟负责资源分配给xxx
### 部署模式 ###
- 单点模式 ：所有都在一台机器上
- 完全分布式模式 ：分布在多台机器上
## 要求 ##
- 部署平台 ： windows 和linux
- 一般linux用的多 ，而且在linux上最少2000台
- jdk ： 安装java的要求
- 3.3-目前 ： java8 - 11
- 3.0-3.2 ： java8
- 2.7-2.10 ： java8
- 但是有些的java8 的版本也不行 ，详细参考官网
- 要下载补丁安装就好
- ssh ： 默认centos是安装的，但是ubantu是没安装的
- 个人要求 ：创建hadoop用户 ，以后我们都用那个用户开发
- 而且创建 几个文件夹进行规范
- app ：app
- data ：数据 
- log ：监控日志
- project ：项目 
- shell ： shell脚本
- software ： 安装包
- 以前版本的apche 版本框架 在
- `archive.apache.org/dist`
- 部署jdk
- hadoop ： 无要求
### 配置开始 ###
- 把文件解压
- 配置环境变量
- 个人 ： 修改 ./bashrc 
- 添加 exprot JAVA_HOME= xxxx
- export PATH=${JAVA_HOME}/bin:$PATH
- export HADOOP_HOME=mmm
- export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH
- 上面xxxx和mmm
- 分别代表我们的java的和hadoop的解压目录
- 然后我们导入个人环境变量 ， 
- source ~/.bashrc
- 然后验证安装 java -version 和 hadoop version
- 如果成功就代表安装成功了
- 接下来我们更改hadoop的配置文件
- 进入到hadoop的etc文件夹里
- 编辑 core-site.xml
- 在两个标签之间输入 


```


		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://你的机器名:9000</value>
		</property>


``` 

- 编辑hdfs-site.xml
- 在两个标签之间输入



```


		<property>
			<name>dfs.replication</name>
			<value>1</value>
		</property>
```

- 接下来我们进行开放访问 ， 我们先执行
- `ssh user@hostname [com]` 进行登录
- 然后通过输入密码登陆一次
- 上一条的的命令如果加上com则代表登录并执行这个命令
- 然后我们要设置免密登录


```


		ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
		cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
		chmod 0600 ~/.ssh/authorized_keys



```
- 然后继续上述的命令：看看能不能免密登录
- 成功之后，我们要更改文件格式 ： 相当于初始化操作
- 然后我们启动dhfs
- `start-dfs.sh`
- 然后我们访问 `http://你的虚拟机ip:9870/`
- 成功看见hadoop的web页面就好了
- 然后可以更简便的方法
- 在c盘找到 windows 然后进去system32
- 然后进去driver
- 然后进入etc文件
- 然后在hosts文件最后的加上我们在linux里的主机和ip映射
- 就可以通过 `http://你的虚拟机名称:9870/`访问我们的hadoop了
### 部署yarn ###
- 对于部署单点的yarn
- 我们可以通过配置yarn的配置文件
- `mapred-site.xml` 和`yarn-site.xml`
#### mapred-site.xml ####
- 对于这个文件我们这样更改


```


		<configuration>
		<property>
		        <name>mapreduce.framework.name</name>
		        <value>yarn</value>
		    </property>
		    <property>
		        <name>mapreduce.application.classpath</name>
		        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
		    </property>
		</configuration>


```
#### yarn-site.xml ####
- 对于这个文件我们进行


```
		
		<configuration>
		#设置web访问的端口
		 <property>
		        <name>yarn.resourcemanager.webapp.address</name>
		        <value>你的主机名称:9999</value>
		    </property>
		
		#设置运行在那个虚拟机上
		    <property>
		        <name>yarn.resourcemanager.hostname</name>
		        <value>你的主机名</value>
		    </property>
		
		
		#下面两个要基本配置
		    <property>
		        <name>yarn.nodemanager.aux-services</name>
		        <value>mapreduce_shuffle</value>
		    </property>
		    <property>
		        <name>yarn.nodemanager.env-whitelist</name>
		        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
		    </property>
		</configuration>


```
- 配置完成之后我们要进行`start-yarn.sh`
- 然后开启这个服务 ： 我们可以通过访问浏览器的 `xxx你的ip:9999` ,访问这个服务
- 这样我们的单点就算配置完成了

## 关于分布式的配置 ##
- 分布式的配置就是把单点的配置分到多台机器上
- 比如 ： 把hdfs的namenode 和 datanode 和secondnamenode分到三台机器上
- 把yarn的部署也分到三台机器上
### 开始分布式 ###
- 首先我们要明确一点 ： namenode是老大 ，只能有一个
- datanode是小弟 ：  可以有多个
- secondnamenode ：是秘书只能有一个
- 对于yarn ： resourcemanager只能有一个
- 而 ：nodemanagers ：可有多个
#### 开始之前的配置 ####
- 关于hdfs和上面部署的一样
- 只不过在配置免密登录的时候不同
- 因为我们现在要做三台机器互相免密
- 所以我们要用
- `ssh-keygen -t rsa` 然后三次回车生成公钥和私钥
- 然后三台机器都要用一遍

```

	ssh-copy-id 第一台机器名
	ssh-copy-id 第二台机器名
	ssh-copy-id 第三台机器名

```

- 然后分别对三台机器用

```

		ssh hadoop@第一台机器名
		ssh hadoop@第二台机器名
		ssh hadoop@第三台机器名

```

- 最后分别ssh一下然后如果能成功就代表成功了

#### hdfs ####
- `core-site.xml`


```


		<configuration>
		#设置存储位置
		<property>
		        <name>hadoop.tmp.dir</name>
		        <value>/home/hadoop/data/hadoopdate</value>
		 </property>
		#设置namenode在哪一台机器上运行
		<property>
		        <name>fs.defaultFS</name>
		        <value>hdfs://主机器的名称:9000</value>
		 </property>
		</configuration>

```

-`workers` 


```

		第一台机器的名称
		第二台机器的名称
		第三台机器的名称

```

- `hdfs-site.xml`

```


		<configuration>
		 #下面的values代表是几台机器，我这个是三台机器
		 <property>
		        <name>dfs.replication</name>
		        <value>3</value>
		  </property>
		  #设置secondarynamenode的端口和在哪一台机器上
		 <property>
		        <name>dfs.namenode.secondary.http-address</name>
		        <value>bigdata4:9868</value>
		  </property>
		#同上一个
		 <property>
		        <name>dfs.namenode.secondary.https-address</name>
		        <value>bigdata4:9869</value>
		  </property>
		</configuration>


```

- 然后三台机器都要配置相同的环境变量

#### yarn ####
- `mapred-site.xml`

```

		<configuration>
		<property>
		        <name>mapreduce.framework.name</name>
		        <value>yarn</value>
		    </property>
		    <property>
		        <name>mapreduce.application.classpath</name>
		        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
		    </property>
		</configuration>


```

- `yarn-site.xml`


```


		<configuration>
		#设置resourcemanager其所占用的端口
		 <property>
		        <name>yarn.resourcemanager.webapp.address</name>
		        <value>bigdata5:9999</value>
		  </property>
		#设置resourcemanager其所在的机器
		    <property>
		        <name>yarn.resourcemanager.hostname</name>
		        <value>bigdata5</value>
		    </property>
		#下面两个是正常yarn的配置文件
		    <property>
		        <name>yarn.nodemanager.aux-services</name>
		        <value>mapreduce_shuffle</value>
		    </property>
		    <property>
		        <name>yarn.nodemanager.env-whitelist</name>
		        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
		    </property>
		</configuration>
```
- 同样三台机器也要配置相同的文件
- 然后我们要在我们的namenode机器上
- 开始初始化： `hdfs namenode -format`
- 然后我们在namenode上开始运行`start-dfs.sh`
- 然后我们在resourcemanager上开始运行`start-yarn.sh`
- 然后我们可以用jps查看每个机器的进程
- 查看是不是符合我们的想法
### 关于datanode缺失 ###
- 因为默认hadoop有一个id文件是在`/tmp/hadoop-hadoop/dfs`下的
- 我们默认启动的时候可能有多个原因 ，造成生成的id不一致
- 比如 ； 我们忘记关闭hadoop服务之类的，或者卸载的时候忘记删掉他了
- 这些都会造成id不一致的问题
- 解决方法： 
- 如果dfs文件夹中没有重要的数据，那么删除dfs文件夹，再重新运行下列指令（格式化指令）
- 如果dfs文件中有重要的数据，那么在dfs/name目录下找到一个current/VERSION文件，记录clusterID并复制。然后dfs/data目录下找到一个current/VERSION文件，将其中clustreID的值替换成刚刚复制的clusterID的值即可
## hdfs的命令 ##
- 创建文件夹 `hdfs dfs -mkdir xxx` : 创建xxx文件夹，可以多层创建文件夹
- 创建文件`hdfs dfs -touchz path`
- 复制文件`hdfs dfs -cp 源目录 目标路径` ：这个是把整个文件夹结构都cp过去：属于hdfs的内部操作不是上传下载
- 移动文件`hdfs dfs -mv 源目录 目标目录`
- 赋予权限`hdfs dfs -chmod 权限参数 `
- 上传文件`hdfs dfs -put 源文件夹路径 目标文件夹路径`
- 上传文件`hdfs dfs -copyFromLocal 源文件夹 目标文件夹`
- 上传文件且删除本地文件`hdfs dfs -moveFromLocal 源文件 目标文件`
- 下载文件`hdfs dfs -get 源文件夹路径 目标文件夹路径`
- 查看文件内容 `hdfs dfs -cat path`从头看这个文件
- 查看文件内容 `hdfs dfs -tail path`查看这个文件的最后1k
- 删除文件`hdfs dfs -rm 文件路径`
- 删除文件夹`hdfs dfs -rm -R 文件夹`
## javaapi的方式操作hdfs ##
- 单元测试 ： 代表我们可以 ， 单独运行某个方法
- 进行部份调试
```

		
		package org.example;
		
		import org.apache.hadoop.conf.Configuration;
		import org.apache.hadoop.fs.FileSystem;
		import org.apache.hadoop.fs.Path;
		import org.junit.Assert;
		import org.junit.Test;
		
		import java.io.IOException;
		import java.net.URI;
		import java.net.URISyntaxException;
		
		
		public class hdfsapi {
		    @Test
		    public  void  mkdir() throws IOException, URISyntaxException, InterruptedException {
		        // 获取程序入口
		        Configuration conf = new org.apache.hadoop.conf.Configuration(); //  配置参数
		        URI uri = new URI("hdfs://192.168.41.132:9000"); //创建uri作为要连接的地址和端口
		        FileSystem fs = FileSystem.get( uri , conf , "hadoop"); // 开始链接 ，三个参数分别是其所在地地方，配置参数 ，用户名
		        Path path = new Path("/ggd"); // 路径
		        boolean mkdir = fs.mkdirs(path); // 判断是不是执行成功
		        Assert.assertEquals(true , mkdir);
		
		    }
		
		}


```

- 实现创建文件夹并移动且改名



```

	package org.example;
	
	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.fs.FileSystem;
	import org.apache.hadoop.fs.Path;
	
	import java.io.IOException;
	
	public class hdfsapi {
	
	    static FileSystem  fs;
	
	    static {
	        Configuration conf = new Configuration();
	        conf.set("fs.defaultFS" , "hdfs://192.168.41.132:9000");
	        System.setProperty("HADOOP_USER_NAME" ,"hadoop");
	        try {
	            fs = FileSystem.get(conf);
	        } catch (IOException e) {
	            e.printStackTrace();
	        }
	    }
	
	    public static void main(String[] args) {
	
	
	        // 移动之前的文件存储路径
	        String url = args[1];
	        // 移动之后的文件存储路径
	        String det = args[2];
	
	        String hozhui = args[0];
	
	        // 要上传的文件目录
	        String pathlocalfile = args[3];
	
	        Integer date = Integer.parseInt(args[4]);
	
	        Integer up = Integer.parseInt(args[5]);
	
	        Integer down = Integer.parseInt(args[6]);
	
	        String[] string = new String[args.length - 7];
	
	        System.arraycopy(args , 7 , string , 0 , args.length - 7);
	
	        makeream(hozhui,url,det,pathlocalfile,date , up ,down , string);
	
	      //rm(20221115 , 1 , 1);
	    }
	
	    private static void rm(int m , int up ,int down ) {
	
	
	        // 通过递归调用本身函数，进行递归调用，删除不同日期的文件夹以及
	        while (up > 0){
	            rm(m+1 , --up , 0);
	        }
	        // 同上
	        while (down > 0){
	            rm(m-1 , 0 ,--down);
	        }
	
	        // 把数字改变成字符串
	        String uri = String.valueOf(m);
	
	        // 基础路径
	        Path path = new Path("/data/dt=" + uri);
	        Path path1 = new Path("/data/hive" + "/" + uri + "-01.data");
	
	
	        try {
	            fs.delete(path);
	            fs.delete(path1);
	        } catch (IOException e) {
	            e.printStackTrace();
	        }
	    }
	
	
	    private static void makeream(String hozhui, String url , String det ,String pathlocalfile , int nowday , int up , int down , String ...args) {
	        // 基礎路徑
	        String dataurl = null;
	        if (args.length > 0){
	            String[] arg1 = new String[args.length-1];
	            System.arraycopy(args , 1 , arg1 ,0 ,args.length-1);
	            dataurl = args[0];
	            makeream(hozhui,url,det,pathlocalfile,nowday,up,down,arg1);
	        } else {
	            dataurl = String.valueOf(nowday);
	            // 通过递归的方式 ， 进行创建文件夹等操作
	            while (up > 0){
	                makeream(hozhui,url,det,pathlocalfile,nowday+1 , --up , 0);
	            }
	            while (down > 0){
	                makeream(hozhui,url,det,pathlocalfile,nowday-1 , 0 ,--down);
	            }
	        }
	        // 获取文件名字
	        String[] split = pathlocalfile.split("/");
	        String filname = split[split.length-1];
	        // 上传的文件路径
	        Path pathfile = new Path(pathlocalfile);
	        // 最后的文件的名字
	        String settotal = det+ "/" + dataurl + hozhui;
	        // 刚开始创建的文件目录
	        String urltotal = url + dataurl;
	        // 创建文件夹的Path
	        Path path = new Path(urltotal);
	        // 移动前的文件路径
	        Path pathfilehdfs = new Path( path + "/" + filname);
	        // 新的名字文件路径
	        Path newname = new Path(settotal);
	
	//        // 要移动到的路径
	//        Path detPath = new Path(url1);
	
	        // 后来的文件目录
	        // Path detpathfile = new Path(det + "/" + filname);
	
	        try {
	            if(!fs.exists(new Path(det))){
	                fs.mkdirs(new Path(det));
	            }
	            fs.mkdirs(path);
	            fs.copyFromLocalFile(pathfile , path);
	            // fs.rename(pathfilehdfs , detPath);
	            fs.rename(pathfilehdfs , newname);
	        } catch (IOException e) {
	            e.printStackTrace();
	        }
	
	
	    }
	
	
	
	}


```

- 单词统计

```

	package org.example;
	
	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.fs.FSDataInputStream;
	import org.apache.hadoop.fs.FileSystem;
	import org.apache.hadoop.fs.Path;
	import org.apache.hadoop.io.IOUtils;
	
	import java.io.*;
	import java.util.ArrayList;
	import java.util.HashMap;
	import java.util.Map;
	
	public class wordcount {
	    static FileSystem fs;
	
	    static {
	        Configuration conf = new Configuration();
	        conf.set("fs.defaultFS" , "hdfs://192.168.41.132:9000");
	        System.setProperty("HADOOP_USER_NAME" ,"hadoop");
	        try {
	            fs = FileSystem.get(conf);
	        } catch (IOException e) {
	            e.printStackTrace();
	        }
	    }
	
	    public static void main(String[] args) {
	
	        wordcounts("/2.log" , ",");
	    }
	
	    private static void wordcounts( String path ,  String regx ) {
	
	
	        String basic = null;
	
	        try( FSDataInputStream fis = fs.open(new Path(path));
	             OutputStream outputStream = new FileOutputStream( new File("D:\\ bg1.txt") , false);
	             InputStream inputStream = new FileInputStream(new File("D:\\ bg1.txt"));
	             )
	        {
	
	            IOUtils.copyBytes(fis,outputStream,4096 , true);
	            byte[] buffer = new byte[1024];
	            int len = 0;
	            while((len = inputStream.read(buffer)) != -1){
	                basic = new String(buffer, 0, len);
	                System.out.println(basic);
	            }
	
	
	            String[] split = basic.split(regx);
	
	            Map<String , Integer> result = new HashMap<>();
	
	
	            for (int i=0;i<split.length;i++){
	                result.put(split[i] , result.getOrDefault(split[i] , 0 )+1);
	            }
	                result.forEach((k,v)->System.out.println(k+","+v));
	
	            int[] flag = new int[split.length];
	            for (int i =0; i< split.length; i++){
	                for (int j = 0; j< split.length;j++){
	                    if (split[i].equals(split[j])){
	                        flag[i]++;
	                    }else{
	
	                    }
	                }
	            }
	
	            System.out.println("--------------------------------------------------------");
	            for (int i =0; i<split.length-1;i++){
	
	                if (!(split[i].equals(split[i+1]))){
	                    System.out.println(split[i] + "\t" + flag[i]);
	                }
	
	            }
	
	//            System.out.print("\n");
	
	//            for (int i =0; i< flag.length;i++){
	//                System.out.print(flag[i]);
	//                System.out.print("\t");
	//            }
	
	            System.out.println("--------------------------------------");
	
	        } catch (IOException e) {
	            e.printStackTrace();
	        }
	    }
	}
	
	


```

### 关于文件的存储 ###
- 架构设计 ： 面试必问
- namenode -----nn
- 负责对外的访问接口
- 负责块的映射
- 元数据 ； 描述数据的数据
- 文件名称
- 文件的目录
- 文件的属性，权限，创建时间，副本数据
- blockmap ： 块映射
- 一个 文件被分割成多个数据块，
- 块映射不会永久化这个存储
- 是通过集群运行的时候dn定期发送blockreport给nn进行维护
- 控制其数据块在哪一个节点上的
- nn作用 ： 管理文件的命名空间 ，其实就算维护文件系统树的文件以及文件夹
- 是以两种的方式永久的存储在磁盘
- 镜像文件 ： fsimage
- 编辑日志文件 editlogs
- seconderynamenode ------snn
- 去老大的节点上拿镜像文件和日志文件，进行合并和备份，然后换给nn
- datanode -----dn
- 每个节点都会有这个进程
- 负责关于客户端的文件的读写
- 负责存储数据
- 存储数据块，以及对于数据块的校验
- 每隔3秒发送一次心跳给namenode ，告诉你我还在
- 每个一定时间（6h）发送一次块报告,这个报告，是扫描磁盘和内存之中的数据一不一样
- 目的 ： 生产上 ： 可能会发生文件块丢失或者损坏
- 
## mapreduce ##
- 简介 ： 其设计理念是计算向数据靠拢，采用分而治之的策略，将庞大的数据分为很多个很多个小切片，并且为每个小切片单独的启动一个map任务
- 适合mapredurce处理的数据集要满足一个前提：待处理的数据集可以分解成许多更小的数据集，且每一个更小的数据集都可以并行的处理
- 其采用的是主从架构（master/Slave），就是一个主服务器多个从服务器（salve），master上运行jobTracker，slaver运行TaskTracker
### mapreduce体系架构 ###
- 其主要是由四个部分组成 ： 分别是Client , JobTracker,TaskTracker以及Task
- Client ： 用户编写的Mapredurce程序通过Client提交到jobTracker
- jobTracker(运行在主服务器上) : 负责监控和资源调度
- 监控所有的TaskTracker与job的健康情况，一旦发现失败，就把相应的任务转移到其他的节点
- 其会跟踪任务的执行调度，资源使用量，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源调度器出现空闲时，选择合适的任务去使用这些资源
- jobTracker（运行在服务器） 
- TaskTracker会周期性的通过心跳将本节点上资源的使用情况和任务运行进度汇报给jobTracker，同时接受jobTracker发送过来的命令并执行相应的操作（如 ： 启动新任务 ，杀死任务等）
- TaskTracker使用“slot”等量划分本节点上的资源量（CPU ，内存）。一个Task获取到一个slot后才有机会运行，而hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别提供Map Task和Redurce Task使用
- Task
- Map Task和Redurce Task 均由TaskTracker
### mapreduce的工作流程 ###
- 一个大的mapredurce任务，首先会被分为为若干个Map任务在多台机器上执行运行（map任务通常运行在存储节点上），每一个map任务会输出一个<key , value>形式的中间结果，一个map任务只有全部执行完成之后才会进行reduce任务，map的输出结果<key, value>（存储在本地磁盘） ， 具有相同的key会被发送到同一个reduce任务
- 注意 ：不同的map之间不会有通信
- 不同的reduce之间也不会
- 用户不能显式的从一台机器向另外一台机器发送消息，所有的信息交换都是通过Mapredurce框架自身去实现
- map的输入文件，redurce任务的输出结果都是保存在hdfs分布式文件系统中，map的输出结果保存在本地磁盘文件中
- 当一个map任务处理全部结束之后，reduce任务才能开始去取相应的数据
- 只有map任务需要考虑数据局限性，实现计算向数据靠拢，reduce无需考虑数据局限性
#### 各阶段执行内容 ####
- 分为 ：5个阶段 预处理，map，shuffle，reduce，输出
- 预处理 ： 由inputFormat 进行格式验证以及逻辑上的分区inputSplit ， inputSplit经过redodrdreader
- 根据inputSplit的信息来处理inputSplit中的具体记录，转换为键值对 ，输入给map
- map ： 接受来自RR键值对，进行分区，排序，合并，归并，得到<key , value-list>形式的中间结果，输入给reduce，此处包括map端的shuffle和reduce端的shuffle
- reduce ： 接受shuffle输出的<key , value>,执行用户子自定义，输出给outputFromat
- 输出 ： outputFromat，模块会验证输出目录是不是已经存在以及输出结果是不是符合配置文件中的配置类型，如果满足，就输出reduce的结果到分布式文件系统
#### map的shuffle ####
- 过程有四部
- 输入数据并执行map任务
- map任务结果输出，写入缓存
- 溢写 ： 如果输出结果超过了一定的比例，每一次溢写会在磁盘上生成一个磁盘文件，写入之前进行会分区，，排序，如果指定了commbiner还可以进行合并，这样经过溢写的磁盘文件就包含了多个分区，且分区内部都是经过排序的
- merge ： 随着map任务的进行，当有多个溢写文件时（就是大于等于2的适合），就会发现merge，生成一个更大的磁盘文件，这个大的溢写文件也是经过排序和分区的，默认情况下每10个溢写会变成一个大文件，通常在merge过程中，如果文件数量大于3则进行combine操作，从而减少磁盘的数据量，如果只有一两个溢写，合并操作得不偿失
#### reduce的shuttle ####
- 领取数据 ： 经过map的suffle后，map的输出结果保存在磁盘，此时，需要将磁盘数据取回到reduce机器，此时如果线程被占满，同样会和map端一样开启溢写操作，reduce通过RPC询问JobTracker是不是还拥有未完成的map任务，如果有，将数据提取到reduce机器上，此时实际上是多个reduce机器，同时多线程从map机器领回数据
- merge ： 一个map的shuffle结果因为拥有多个分区，所以会有不同的reduce机器取回自己的数据，而每一个reduce也从不同的map机器取回数据，如果每个reduce机器内存达到阈值，就进行溢写操作，溢写的时候一般有很多键值对可以进行merge，如果定义了combiner还可以进行合并，进行溢写操作的过程中还可以进行combine。并非是一个reduce程序从map机器上取回数据就生成一个溢写文件，而是缓存不够用，则发生溢写，如果缓存够用，则是直接在内存里进行操作
- 把数据输出给map任务（对一个reduce而言）：执行用户自定义的逻辑，最终输出

## mapredurce里的数据类型 ##
- intWritable
- longWritable
- 等等，
- 就是基本数据类型加上Writable
- 在redurce里数据类型也是一样的