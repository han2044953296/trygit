---
title: mapreduce
date: 11-21 8.49 
categories: 日志
comments: "true"
tag: hadoop
---
# 续讲Hadoop

## 模板模式

关于mapreduce的操作模式就是模板模式，加上自定义的变量

### 模板模式

关于模板模式，就是三步走

开始阶段 ： map阶段

处理阶段 ： reduce阶段

结束阶段 ： 关闭流并输出

### 自定义的变量

BooleanWriteable : 布尔类型

ByteWriteable : Byte类型

DoubleWriteable : double类型

FloatWriteable ：float类型

intWriteable ： int类型

...（等等）

### mapreduce的核心方法

* input : InputFormat => 如何加载数据
* 查看源码发现有DBInputFormat 和 FileInputFormat，代表我们可以从db（数据库中），或者文件中加载数据（上述两个是抽象类，我们实例化的时候其实是实例化其子类）
* 简单来说就是读取数据的方法
* 默认实例化的时候是实例化 TextInputFormat
* 关于TextInputFormat ： 其有两个参数
  * key 和 value
    * key 是 读取文件的位置
    * value 是 一行一行的内容
      * 在value中有个属性 判断文件可不可以被切分 ： isSpiltable
        * map task 数据 是由 input 的切片数量决定的 ， 当不可被切片的时候对应的数量就是1，对应一个map task

creatRecordReader

## 一个文件加载的时候会被切分成几个切片

前提 ： 文件可以被切分

当一个文件 在hdfs上的时候是按我们的大小进行切分的 以128m为基础单位

而我们的map的切片是按照blocksize进行切分的简单来说就是以blocksize的大小进行切分

总结  ：

* 文件大小小于128m
  * 那么切分成1片
* 如果文件大于128m
  * filesize/splitesize = num 切片数
  * filesize剩余的部分 ： 和splitesize的10%比较
    * 如果大，则开启一个切片文件
    * 如果小，则是和上面最后一个合并到一起

不能被切分的文件： 某些被压缩文件

* 关于这个切片大小的标准 ：
  * 在hdfs上是128m
  * 但是在idea上是32m（在源码里能看）

## redurce task 个数是由上面决定?

默认是 1

如果要更改要手动更改

setNumReduceTasks(数量)

如果 reduce阶段数量变多

则会把相同的文件拉到一起，就是按照一个规则进行的分区

默认是走的hashcode ： 这个按照规则来分的就是分区

一般来说 ： 分区的结果是 suffler 的输出结果是 reduce的输入

### 简单需求

基于phone的存储数据 ，进行分文件存储

比如

13开头的存储在一个上

15开头的在一个文件上

这就要自定义分区

通过继承Partitioner类实现方法然后导入方法

实现getPartition方法进行分区 ： 以下是一个简单的分区函数

```java
package org.example;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;
public class myPartionit extends Partitioner<Text , IntWritable> {



    @Override
public int getPartition(Text text, IntWritable intWritable, int i) {
        if (text.toString().equals("ANALYST")){
            return 0;
}else if (text.toString().equals("CLERK")){
            return 1;
}else if (text.toString().equals("MANAGER")){
            return 2;
}else if (text.toString().equals("PRESIDENT")){
            return 3;
}else if (text.toString().equals("SALESMAN")){
            return 4;
}else {
            return 5;
}
    }
}
```

然后我们要在我们的主程序中调用这个类 ， test类：就是我们的主类，如下

```java
package org.example;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.util.StringTokenizer;

/**
 * @author sxwang
 * 11 18 14:00
 */
public class test {

    /**
     * driver
     * @param args
*/
public static void main(String[] args) throws Exception {

        if (args.length < 2){
            System.out.println("出现问题最少两个参数");
System.exit(2);
}
        String input=args[0];
String output=args[1];
Configuration conf = new Configuration();
//0.todo... 删除目标路径
FileUtils.deletePath(conf,output);

//1.设置 作业名称
Job job = Job.getInstance(conf, "WCAPP");
//2.设置map reduce 执行代码的主类
job.setJarByClass(test.class);
job.setMapperClass(MyMapper.class);
job.setReducerClass(MyReducer.class);
//3.指定 oupput kv类型
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
job.setNumReduceTasks(6);
job.setPartitionerClass(myPartionit.class);
//4. 设置数据源路径 数据输出路径
FileInputFormat.addInputPath(job, new Path(input));
FileOutputFormat.setOutputPath(job, new Path(output));
//5. 提交mr yarn
System.exit(job.waitForCompletion(true) ? 0 : 1);
}

    /**
     * mapper
     */
public static class MyMapper
            extends Mapper<Object, Text, Text, IntWritable> {

        @Override
protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            /**
             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1
             * (word ,1)
             */

String[] words = value.toString().split("\n");

            for (String word : words) {
                String[] split = word.split(",");
context.write(new Text(split[2]) ,new IntWritable(1));
}

        }
    }

    /**
     * reducer
     */
public static class MyReducer
            extends Reducer<Text,IntWritable,Text,IntWritable> {

        /**
         *  (word ,1)
         *
         *  (word,<1,1,1,1>)
         *
         *  1.聚合value
         *
         *  2.写出去
*  (word ,3)
         */
@Override
protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum=0;
            for (IntWritable value : values) {
                sum +=Integer.parseInt(value.toString());
}
            context.write(key,new IntWritable(sum));
}
    }
}
```

## sql vs mr(mapreduce)

* sql  :
  * group by
  * distinct
  * join
  * order by
  * union
    * group by 用mr实现
    * 就是再map阶段进行处理reduce阶段进行合并
    * distinct
    * 
    * 去重 ： sql ：

      ```sql
      select distinct(ename) from emp;
      #或者可以通过分组进行去重
      select name from group by name;
      ```
    * 通过上述我们可知group by 也可以进行去重 ，所以后面我们能用group by 就要用group by
    * 因为distinct只有一个task进行处理
    * 而group by 则是多个task 进行处理 ， 所以效率会比较高
    * 接下来是在mr里实现
    * order by
    * mr :
    * 全局排序 ： reduce task 是 1
    * 分区排序 ： reduce task 是 多个
    * mr : 实现
    * 

## 坑

如果你的reduce task 大于分区数 ： 会有空白文件

如果 reduce task 小于分区数 且大于1 ： 则会报错

如果 reduce task 且分区数 等于 1 ： 则会把东西整合到一个文件

## 关于mapreduce在linux上运行的学习，官方提供了源码库

地址在github上：
