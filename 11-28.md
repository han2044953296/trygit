---
title: yarn
date: 11-28 8.53 
categories: 日志
comments: "true"
tag: hadoop
---
# yarn

## 架构

主从架构 ： resourcemanager(资源的分配) : nodemanager(资源的供给与隔离)

### 资源调度

通过rm把nm的资源分配给我们的 task上

### 资源隔离

nm按照要求给task提供资源，保证提供的资源具有独占性

### 资源

nm指挥分配的资源

一个task对应一个container ： cpu/mem（cpu和内存）

每个container之间是相互隔离的

## yarn的架构设计

作业提交的流程：
client => 给rm 的apps 发送请求 去运行 jar （app master）

apps 分配一个container 去运行 app master

app master 会向apps manager 去注册我的作业

app master 向resource scheduler 申请资源去运行 我的代码

nodemanager 会开启资源 container 去运行map task 以及reduce task

tsak 会向 app master 汇报代码与运行情况

当代码运行完成 app master 会给apps 发送请求 ，通知我的作业完成了

apps manager 收到请求之后会通知你的客户端 ，告诉已经运行完成

输入阶段 ： map tsak 的个数 =》container 的申请个数 redurce task 同理

## 面试会问

yarn的架构设计 ==mr作业提交流程

## 调度器

FIFO ：

* 先进先出
* 单队列

Capacity ：容量调度器

* 多队列
* 先进先出（针对一个队列）
* 每个队列之间互不影响
* 每个队列之间是事先定好的

Fair：公平调度器

* 多队列
* 每个队列之间的每个job是有影响的 不是先进先出
* 哪一个job的优先级高就执行哪一个
* 如果相同优先级，则是顺序

主流的中小企业：Capacity

大公司会用：fair

### 默认调度器

3.x：默认是容量调度器

通过调度器进行其作业调度

2.x: 版本是fair（默认）

## yarn的web界面的简介

左侧侧边栏 ： 有几个选项卡 ： 分别是

## yarn资源的调优

container?

一定比例的cpu和mem

刀片服务器的配置 ： 128G 16 core :假设一个机器的配置

刀片服务器 装完系统 消耗内存 1G

系统预留 ： 预留 20%左右 包含装完成系统 消耗的1G

原因 ：给未来部署组件预留空间，防止全部使用 ： 会导致系统夯住 就是卡住 ，oom机制【linux系统】：系统会自己杀死进程当内存不足的时候

预留空间 ： 128 * 0.2 = 26G

其余空间用于大数据 102G

hadoop ：

* datanode  进程内存 ： 默认 1G =》 生产上 2G
* nodemanager 进程内存 ： ，默认 1G =》 生产上 4G

接下来还有96G全部给我们的yarn资源 ： 96G

container的资源分配 ：

内存

cpu

相比：cpu更重要一些

container的内存划分 ：默认是86G

其最小是 1G（默认）

最大是 8G（默认） 但是可以设置

注意 container的内存会自动增加 默认以1G递增

container cpu ： 是虚拟核 =》 考虑初衷是不同节点的cpu性能不同

比如 ： 一个cpu是另外一个cpu的2倍

第一机器 ： pcore ：vcore = 1：2 相当于1个物理核当成两个虚拟核用

给container的核数 ： 默认是8core

总数 ：

最小:1c  (默认)

最大:4c（默认）

实际开发角度 ：

mem：最大不要超过32G ，如果超过32G则会导致压缩指针失效

cpu ： cloudera的公司推荐一个container的core最好不要超过5

配置core  ：在yarn-site.xml

```
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
</property>
<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
</property>
<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
</property>
cpu： 
<property>
        <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
        <value>2</value>
</property>
<property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>4</value>
</property>
<property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
</property>
<property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>2</value>
</property>

```

## mapreduce

map task :mem  : 默认是1024m ，一个map task 申请的资源是1024m ， 但是如果实际使用的资源的内存量超过这个值，你的task会强制被杀死 ，reduce task 也一样

map task :vcore ： 默认是1

reduce task :mem : 默认是1024m

reduce task :vcore : 默认是1

mr作业是进程级别 =》 jvm

map task

reduce task

jvm参数调优 ：

存储 hdfs

存储文件

压缩

* 为什么使用压缩？
* 节省空间
* 节省时间
  * 网络io核磁盘io会减少
  * 指的是mapreduce数据计算过程中
  * 经过网络传输的数据会变少，
  * 同样到磁盘上的时候，数据量少也会减少磁盘的io
* 使用场景
  * 采用压缩 ， 对机器的cpu要求比较高
  * 存储数据的空间不够了，才会用压缩
* 两面性
* 采用压缩的确会让空间和时间减少
* cpu消耗 cpu利用率高 =》会导致处理的时间变长一点
* 如何使用压缩？

常见的压缩格式

* gzip
* bzip2
* Lzo
* Snappy
* LZ4

常见的压缩注意点：

压缩比 Bzip2 30%  GZIP    snappy、lzo 50%  ： 这个30%和50%代表能把源文件压缩到源文件的%多少

解压速度 ： snappy、lzo GZIP   Bzip2

压缩文件可不可以一被切分

假设一个 5G文件 不能被切分 split 意味着 只能使用一个map task去处理

map task  5G

假设一个 5G文件 能被切片  splits 10map task 去并行处理

5*1024 /10 = 一个map task 处理的数据

能否被切分 决定了 你的 一个map task处理的数据量有多少

压缩后的文件是否支持分割？
			gzip  不可分割
			bzip2  可分割
			lzo   带索引的可以分割 (默认是不支持分割的)
			snappy 不可分割的

mapreduce 每个阶段该如何采用这些算法？

input =》 maps =》 reduce =》 output

input：
	1.Bzip2 ：支持分割 多个map task 进行出

map out：

* snappy 、lzo
* shuffle 过程 要选择一个解压 速度快的压缩格式

reduce out ：

* 1.高的压缩比 + 支持分片  =》 节省空间
* 2.bzip2  、lzo带索引的

reduce out 数据 作为下一个
map 的输入咋办？
建议使用bzip2【如果采用压缩的话】

进行压缩配置的两种方式 ：

job =》 code 针对少数job生产生效

集群所有的job 在配置文件里配置

对mapreduce 进行配置

* 配置压缩的codec
* map reduce 输出配置
  * 先打开压缩的开关
  * 配置codec即可
* hadoop是不是支持哪些压缩 ，通过命令 ：或者配置文件
* core-site.xml
  * 配置支持的压缩有什么
  * 原生的hadoop默认不支持lzo的算法，因为lzo要把整个hadooop重新编译一遍才可以重新实行

```
<property>
        <name>io.compression.codecs</name>
        <value>org.apache.hadoop.io.compress.BZip2Codec,
		org.apache.hadoop.io.compress.SnappyCodec,
		org.apache.hadoop.io.compress.GzipCodec,
		org.apache.hadoop.io.compress.DefaultCodec
	</value>
</property>

```

mapred-site.xml:

* 1.先打开压缩的开关
* 2.map reduce 输出 压缩算法

reduce： 开关

mapreduce.output.fileoutputformat.compress

```
<property>
        <name>mapreduce.output.fileoutputformat.compress</name>
        <value>true</value>
</property>
```

```
<property>
        <name>mapreduce.output.fileoutputformat.compress.codec</name>
        <value>org.apache.hadoop.io.compress.BZip2Codec</value>
</property>

```

```

```

map阶段的：

```
// 设置在map输出阶段压缩
        conf.set("mapreduce.map.output.compress", "true");

// 设置解压缩编码器
        conf.set("mapreduce.map.output.compress.codec", "org.apache.hadoop.io.compress.DefaultCodec");

```

各个datanode数据节点的平衡

* DN1 存储空间 90%
* DN2 存储空间 60%
* DN3 存储空间 80%

如何做呢？

`sbin/start-balancer.sh`

parameters = Balancer.BalancerParameters

[BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5

数据平衡的默认阈值：threshold = 10.0

每个节点的磁盘使用率 - 平均的磁盘使用率 <10%

DN1 存储空间 90%  -76% = 14% 说明这个节点数据多 往别的节点迁移数据 出
DN2 存储空间 60%  -76% = -12% 说明这个节点数据少 别的节点迁移数据 进
DN3 存储空间 80%  -76% = 4% 	说明这个节点数据多 往别的节点迁移数据

avg=90 + 80 +60 /3 = 76%

生产上 从现在开始 start-balancer.sh -threshold 10  每天要定时做的

放到业务低谷期去做 数据平衡操作

注意： 不要在业务高峰期做

数据平衡 数据传输  带宽有关

调优参数 ：平衡的网络带宽

dfs.datanode.balance.bandwidthPerSec 100m 【2.x 默认是10m】

每个节点数据几十T  需要数据平衡的数据 几十T  可以申请维护窗口时间 然后可以把

`dfs.datanode.balance.bandwidthPerSec 临时调大 200M`

单个DN节点 多块磁盘的数据平衡

投产前规划：
		DN 机器   10块 2T 【不做raid】  =》 20T   副本的

1.dn 配置多个磁盘

```
<property>
        <name>dfs.datanode.data.dir</name>
        <value>/data01,/data02,/data03</value>
</property>
```

挂载磁盘一般最省钱的是 2W 转数 2T的

2.为什么要使用多块物理磁盘？

* 存储
* 因为多个磁盘的io也是叠加的
  每块磁盘 磁盘io 每秒 100m
  三块磁盘 1s 能 300m文件内容
  一块磁盘 1s 100m
* 可以让服务一直运行，加入一个磁盘挂了，服务不会减少

做多个磁盘数据均衡
	`dfs.disk.balancer.enabled  true `【3.x有这个功能 cdh 2.x 也有】 apache 2.x 没有这个功能

得通过命令去解决磁盘数据均衡？

hdfs diskbalancer

步骤

```
hdfs diskbalancer -plan  bigdata32  => 生成一个  bigdata32.plan.json 文件
hdfs diskbalancer -execute bigdata32.plan.json =》 执行disk 数据均衡计划文件
hdfs diskbalancer -query bigdata32
```

生产上 当你发现 磁盘写入不均衡 可以做一下 【一般 一个月 半个月 做一次即可】
