---
title: hive
date: 11-29 8.53 
categories: 日志
comments: "true"
---
# 简介

简单来说就是用sql处理hadoop的数据的

除了hive之外 ： sparksql ，presto ， impala

## 要求掌握

sql

udf

## hadoop包含

hdfs ：命令行

mapreduce ：目前工作中几乎不用，但是关于核心类和思想要掌握

yarn：提交作业 xxx （mr/spark/flink）on yarn ： 必须会

mapreduce的弊端 ：

* 开发大量代码
* 编程基础不错
* 部署麻烦
* 修改code蛮麻烦
* 对于DBA和RDBMS的小伙伴是不友好的

大数据处理来说最终落地最好是sql

大数据开发角度 ：

* 基础平台开发
  * 涉及很多框架底层的面很广
* 应用层面开发
  * 基于基础平台开发 ，写sql

根据你的兴趣点 + 公司的定位

必然有新的东西诞生去解决一个场景的问题

mr主要适用于我们的p计算（离线计算）：不要求时效性 ，但是mr开发太麻烦，就诞生了hive

hive 介绍 ：

hive.apache.org

是由什么人提供出来的？

facebook 开源 去解决结构化的数据统计问题

是什么？

构建在hadoop之上的数据仓库

hdfs：hive 的数据是在hdfs之上的

yarn：可以跑在yarn之上

mmapreduce ： 可以用mr形式去运行

如何使用：

定义了一中类sql的语言，类似sql但是又有不同

适用于离线/p处理的

开发就是写sql =》 mr  =》 运行在yarn上

hive的底层引擎是：

* mr （默认）：sql=》mr
* Tez：sql=》Tez
* Spark：sql=》Spark

hive on spark =》生厂上用的用的不多

spark on hive =》 用sparksql查看hive的数据

hive的存储格式，压缩格式等

官网：

* in distributed storage （分布式存储）：hdfs , cos,oss,aws
* A command line tool and JDBC driver are provided to connect users to Hive.

版本介绍 ：

x.y.z：x是大版本，y是小版本，z是小版本的修复版本

为什么要学习hive

* 简单易用 ： 可以用sql开发
* 扩展性好：
  * 用自定义函数udf
  * 数据存储 和 计算角度 ： 如果表中数据存不下，可以加几个节点就好了
    * 注意hive不是分布式的，它仅仅是个客户端
  * Metastore ：hive的元数据管理
    * sparksql ,presto ,impala 只要可以访问hive的元数据就可以访问里面表的数据
    * 可以分享元数据

hive的架构 ：

* 元数据 ： 描述数据的数据
  * 表的名字，字段的名字，字段的类型，什么人创建的，数据存储在哪里等
  * 元数据的内部内置了一个Derby但是有弊端
  * 元数据都是用mysql进行存储的
  * 测试的时候一个mysql就可以了
  * 但是生产上则一个不够
  * 生产上要遵循 HA ：高可用
  * 就是要一台做备份
  * 两个mysql是主从架构

hive和RDBMS的区别

* 共同点 sql
* 延时性 ： hive 适用于离线计算 慢，千万不要拿hive和mysql的性能做对比，无可比性，mysql要必hive高（数据量小）数据量大的时候就反过来了
* 事务 ： 都支持
* update ，delete
  * 上面两个语句在hive里基本不用，因为性能太差了
* 都支持分布式
* 部署成本 ：
  * hive ：廉价
  * mysql ：成本很高
* 数据体量
  * hive ：Tb
  * mysql：处理Pb都比较费劲

对hive进行部署

分布式部署 ： cwiki.apache.org(官网)

首先把用压缩包上传到linux机器上

然后对linux进行解压

解压完成之后，我们要对他的的配置进行修改在解压之后的

`vim hive-site.xml`

编辑完成之后我们把以下内容放进去

```shell
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://bigdata2:3306/hive?createDatabaseIfNotExist=true</value>
</property>

<property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
</property>
<property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>liuzihan010616</value>
</property>
</configuration>
```

然后在环境变量中编辑我们的hive_home

`vim ~/.bashrc `

把下列加进去

```shell
#HIVE_HOME
export HIVE_HOME=/home/hadoop/app/hive
export PATH=${PATH}:${HIVE_HOME}/bin
```

然后我们要把我们的mysql链接包放在lib文件夹下

就是 `mysql-connector-java-5.1.28.jar`

要放在hive的lib文件夹下

然后对我们的hive进行初始化

`schematool -dbType mysql -initSchema`

初始化成功之后，先启动hdfs ，然后命令行输入 hive 然后输入show databases;

成功就ok了

然后再mysql的数据库里会有hive这个数据库

mysql里\G是格式化的意思

## 思考 ： 表中的字段 存在哪里？

我们先进入hive ： `create table test(name String); `

一个 hive 表 会被拆分成n个表存储再mysql里

比如　： TBl表存放的是我们的表名 `select * from tbls \G;`

```
*************************** 1. row ***************************
            TBL_ID: 1
       CREATE_TIME: 1669707861
             DB_ID: 1
  LAST_ACCESS_TIME: 0
             OWNER: hadoop
        OWNER_TYPE: USER
         RETENTION: 0
             SD_ID: 1
          TBL_NAME: test
          TBL_TYPE: MANAGED_TABLE
VIEW_EXPANDED_TEXT: NULL
VIEW_ORIGINAL_TEXT: NULL
IS_REWRITE_ENABLED:  
1 row in set (0.00 sec)

```

比如 ： columns_v2表存放的是我们的字段 `select * from columns_v2 \G;`

```
*************************** 1. row ***************************
      CD_ID: 1
    COMMENT: NULL
COLUMN_NAME: name
  TYPE_NAME: string
INTEGER_IDX: 0
1 row in set (0.00 sec)

```

比如 ： DBS是存放的我们的物理存储路径的 `select * from DBS \G;`

```
*************************** 1. row ***************************
          DB_ID: 1
           DESC: Default Hive database
DB_LOCATION_URI: hdfs://bigdata3:9000/user/hive/warehouse
           NAME: default
     OWNER_NAME: public
     OWNER_TYPE: ROLE
      CTLG_NAME: hive
1 row in set (0.00 sec)
```

## DDL

hive有个默认数据库 是default 路径 ：/user/hive/warehouse

非默认数据库 ： /user/hive/warehouse/dbname.db

### 创建数据库

CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [MANAGEDLOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

[] 可有可无
(|) 选择其中一个即可

CREATE DATABASE 名称；

CREATE DATABASE 名称 LOCATION '创建的地方'；

例子 ：

create database if not exists bigdata_hive;
create database  bigdata_hive2  LOCATION '/data/bigdata_hive2';
create database  bigdata_hive3 WITH DBPROPERTIES ('creator'='doublehappy', 'create_dt'="2099-11-29");
create database if not exists bigdata_hive4 COMMENT "这是一个数据库4";

解决此处中文乱码的问题

```
ALTER TABLE hive.dbs MODIFY COLUMN `DESC` varchar(4000) CHARACTER SET utf8 COLLATE utf8_general_ci NULL;
```

### 显示数据库

show databases;
show databases like "bigdata_hive*"
desc database  bigdata_hive3;
desc database EXTENDED bigdata_hive3;

## 思考 ： 这个数据库在hdfs的哪一个地方

可以通过查看DBS表

# hive 的注释(comment) 中文乱码的解决方法

（1）修改表字段注解和表注解

alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;

（2）修改分区字段注解

alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;
alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;

（3）修改索引注解

alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;

修改hive-site.xml配置文件

```
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&useUnicode=true&characterEncoding=UTF-8</value>
    <description>JDBC connect string for a JDBC metastore</description>
</property>
```

上述的 `& 是 &amp; `
