---
title: hive第二天
date: 11-30 8.53 
categories: 日志
comments: "true"
---
# 关于hive里的数据类型

* 整数
  * int
  * bigint ==long
* 小数 ：
  * float
  * double
  * Decimal
* 字符串：
  * String (建议统一用String)
  * varchar
  * char
* 时间：
  * 时间日期 DATE 格式：YYYY-MM-DD
  * 时间戳：TIMESTAMP YYYY-MM-DD HH:MM:SS

# 建表

CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
  [
   [ROW FORMAT row_format]
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
  ]

数据字段名字 字段类型

例如

```
create table mytest(
	id String comment '用户id',
	name string,
	age bigint
) comment '第一个表'
ROW FORMAT delimited fields terminated by ',' //指定分隔符
STORED as TEXTFILE; // 存储形式

或者

create table emp2 like emp;
复制表结构

或者
```

## 为什么要分隔符

因为我们的元数据都在hdfs上，对于hdfs上的数据可以通过分隔符进行自动导入到hive里，比如上述是，分割的，然后我hdfs上有如下数据

1，zihan,11

2,zhangsan,23

3,liu,33

就会自动按照每一行进行insert

导入数据 ： load data local inpath '本地的绝对路径' into table 表名

清空表的操作 ： truncate table 表名

# 删除库

DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];

DROP DATABASE bigdata_hive4;

DROP DATABASE bigdata_hive2 CASCADE; =>删库跑路的操作

CASCADE : 代表联合删除 ，一般删除的时候如果里面有表，会造成无法删除的问题，但是联合删除会直接删除掉

# DMl

## load ：

* 加载本地数据
* 加载hdfs上的数据

LOAD原本是追加，不是覆盖 ， 但是可以通过 加上 overwrite 关键字 进行 覆盖操作

## 覆盖例子

load data local inpath '/home/hadoop/tmp/emp.txt' OVERWRITE INTO TABLE emp;

## 上传

本地：load data inpath '本地路径' into table 表名

hdfs ： load data inpath 'hdfs上的路径' into table 表名

上述的hdfs上的相当于把其路径里的文件移动到table 表名的下面 并且改名，且关联到metastore

但是我们的hdfs mv 是不会关联到metastore的

在hive 里 update 和 delete 不要做 =》 因为效率低下

把所有的update和delete都转化成insert和overwrite

## 插入语句 ：

Inserting data into Hive Tables from queries

insert into|OVERWRITE table tablename selectQury

2.Inserting values into tables from SQL 【不推荐使用】
INSERT INTO TABLE tablename
VALUES values_row [, values_row ...]
1.每导入一条数据 就会触发一次 mapreduce job  效率太低

emp2：
	insert into table emp2
	select *  from emp;
insert overwrite table emp2
select *  from emp where deptno=10;

## 关于hive里的一些函数以及使用

### 1.where 过滤条件

```
where_condition
	 <
	 >
	 =
	 <>  !=
	 and
	 or
	 in
	 not in
	 between  and
	 is
	 is not
```

### 需求：查询表中 deptno 20 10

```
select
*
from emp
where deptno=20 or deptno =10;

select
*
from emp
where deptno in (10,20);

select
*
from emp
where deptno <> 20;
select
*
from emp
where deptno != 20;
```

### 2.order by  排序语法

```
1.默认asc 升序
2.降序 desc

select
sal
from emp
order by sal desc;
```

### 3.like 语法 模糊匹配

```
1._  占位符
2.%  模糊
rlike regexp
```

### 4.合并表

```
union  去重

union all  不去重
```

### 例子：

```
create table a(id int ,name string) row format  delimited fields terminated by ',' ;
create table b(id int ,name string) row format  delimited fields terminated by ',' ;

load data local inpath "/home/hadoop/tmp/a.txt" into table a;
load data local inpath "/home/hadoop/tmp/b.txt" into table b;

select name from a
union all
select name from b;

select name from a
union all
select name from b
union all
select "lisi" as name ;

select name,"1" as pk from a
union all
select name,"2" as pk from b
union all
select "lisi" as name,"3" as id ;
```

思考： hive建表 默认column 分割符是什么？

### 5.null 处理

    1. 过滤
		where xxx is not null
    is null 作用一样 <=>
	2. etl 转换
		ifnull  => hive里没有
		coalesce =》
		nvl  =》

### 补充：

    查看hive支持的function ：
				y=f(x)
		SHOW FUNCTIONS [LIKE "`<pattern>`"];
		show functions like nvl;  => 判断 function hive 是否存在
		desc function nvl; =》  查看某个函数具体使用

```
select
empno,
ename,
job,
mgr,
hiredate,
sal,
nvl(comm,0) as comm_alias,
deptno
from emp ;
```

## 分组 聚合函数 join

聚合函数 ：

* sum
* max
* min
* avg
* count

### group by

* 和聚合函数一起使用
* 一个或者多个colum进行分组
* 字段必须select出现 和 group by 出现要一致

### having ：

* 在group by 后面使用

```
select job,

sum(sal) as sal_num,

max(sal),

min(sal),

avg(sal),

count(1) as cnt

from emp

group by job

having sal_num > 6000
```

### join

找准关联字段

* inner join [join]
* left join
* right join
* full join

### 需求：既要显示聚合前的数据，又要显示聚合后的数据？

函数  over([partition by xxx,...] [order by xxx,....])

over: 以谁进行开窗 table、
parition by : 以谁进行分组   table columns
order by : 以谁进行排序  table columns

```
数据： 
haige,2022-11-10,1
haige,2022-11-11,5
haige,2022-11-12,7
haige,2022-11-13,3
haige,2022-11-14,2
haige,2022-11-15,4
haige,2022-11-16,4
```

需求：
	统计累计问题 ，每个用户每天累计点外卖次数

[partition by xxx,...] [order by xxx,....]

```
select 
name ,
dt ,
cnt ,
sum(cnt) over(partition by name  order by dt ) as sum_cnt
from user_mt;
```

### 命令行更改

command line
	1.hive shell
	2.jdbc => hiveServer2

    hive clinet:
		1. hive shell
		2. beeline shell jdbc   开启 hiveServer2 服务 thift

在beeline中 `!connect jdbc:hive2://localhost:10000 hadoop`

补充：
beeline => 连接 hive  =》 hdfs
对hdfs 做一个设置 代理设置：

```
core-site.xml:
<property>
	<name>hadoop.proxyuser.hadoop.hosts</name>
	<value>*</value>
</property>
<property>
	<name>hadoop.proxyuser.hadoop.groups</name>
	<value>*</value>
</property>
```
