---
title: hive第四天
date: 12-2 8.53 
categories: 日志
comments: "true"
---
维度组合分析 ：

sql 关键字 ： grouping sets

例子  ：

```
create table user_shop (
user_id String,
shop_name String,
channe String,
os String
)
row format delimited fields terminated by ','
```

一般我们进行维度计算的时候，我们可以通过group by 的方式进行

但是假如我们每次都要处理一个维度，那么我们难道要写很多个sql语句吗

这明显是不行的

那么我们如何解决呢

通过grouping sets 就可以解决了

代码如下  ：

```
SELECT empid,custid,
       sum(qty) as sumqty
FROM Orders
GROUP BY
    GROUPING SETS
    (
        (empid,custid),
        (empid),(custid),
        ()
    );
```

上面代码的意思就是 ：我要按照ｇｒｏｕｐ　ｂｙ　的方法　把empid,custid和empid和custid这几个维度都选出来，然后是上下在一起的　相当于用ｕｎｉｏｎ在一起

如果是这次选择的维度中未选择的维度，比如说　，我只选择了　维度empid　，那么custid列就会是空，但是这个比多次重复性写ｓｑｌ语句要好的多

# 数据转换

针对以下数据

```
+--------------+--------------------+
| hanglie.ame  | hanglie.teresting  |
+--------------+--------------------+
| zuan         | 王者荣耀               |
| zuan         | 吃饭                 |
| zuan         | rap                |
| zuan         | 唱歌                 |
| chaofeng     | 王者荣耀               |
| chaofeng     | 睡觉                 |
| chaofeng     | 方亚                 |
+--------------+--------------------+

```

我们可以把后面散开的数据转化成一个array存起来

通过 collect_list 函数转化成array 而且可以通过 concat_ws函数设置每个参数之间的分隔符

如下

```
select 
ame,
collect_list(teresting) as interesting,
concat_ws(':' , collect_list(teresting)) as newin
from hanglie
group by ame
```

结果如下：

```
+-----------+---------------------------+-----------------+
|    ame    |        interesting        |      newin      |
+-----------+---------------------------+-----------------+
| chaofeng  | ["王者荣耀","睡觉","方亚"]        | 王者荣耀:睡觉:方亚      |
| zuan      | ["王者荣耀","吃饭","rap","唱歌"]  | 王者荣耀:吃饭:rap:唱歌  |
+-----------+---------------------------+-----------------+

```

## concat

concat是可以更改数组分隔符的一个函数

例子 ：

拼接：

* concat  =》 字符串拼接
* select concat("zuan","|","zihang","|","chaofeng")
* 结果是 ：zuan|zihang|chaofeng
* concat_ws(string SEP, string A, string B...) =》 字符串拼接
* select concat_ws("|","zuan","zihang","chaofeng")
* 可变参数 =》 array【String】
* select concat_ws("|",split("a,a,a",',')
* select  split("a,a,a",',')  ： 这个就是切割字符串

所有类型的可以转换成字符串

字符串有好处也有坏处

因为无法排序

但是经过hive优化，字符串是可以进行四则运算的

字符串排序： 按照字典序进行排序的 a-z

## BY

四个by

### order by

全局排序 ，且reduce只有一个

order by会对输入进行全局排序，因此只有一个Reducer（多个Reducer无法保证全局有序），然而只有一个Reducer会导致计算效率非常低，使用较少。事实上，在生产环境中，order by 很容易造成OOM。

如下  ：

```
select  *  from emp order by empno;
```

执行上述语句要开启个开关才可以

hive.mapred.mode =>some risky queries are not allowed to run 【关闭】

如果用order by 推荐和 limit搭配

### sort by

分区排序 ： 不能保证 全局有序

sort by不是全局排序，它会在数据进入Reducer之前完成排序。因此如果使用sort by进行排序，并且设置mapreduce.job.reduces多于一个，则sort by只会保证每个reducer的输出有序，不能保证全局有序。但是可以对最后的结果进行归并排序实现全局排序。

假如你的reduce task 个数 是 1 则它和order by 是一样的

调制reduce task 个数 ：

* mapred.reduce.tasks
* set  mapred.reduce.tasks;

### Distribute  By

数据开发的时候会用到

distribute by的作用是控制map端如何拆分数据给reduce端。hive会根据distribute by后面的字段，对reduce的个数进行分发，默认采用的是hash算法。sort by保证每个reduce内有序，因此distribute by经常和sort by配合使用。生产环境中 distribute by + sort by用的多。

数据 如下 ：

```
2020,1w
2020,2w
2020,1w
2020,0.5w
2021,10w
2021,20w
2021,19w
2021,1.5w
2022,1.3w
2022,2w
2022,1w
2022,0.5w

+-----------------------+--------------------------+
| hive_distribute.year  | hive_distribute.earning  |
+-----------------------+--------------------------+
| 2020                  | 1w                       |
| 2020                  | 2w                       |
| 2020                  | 1w                       |
| 2020                  | 0.5w                     |
| 2021                  | 10w                      |
| 2021                  | 20w                      |
| 2021                  | 19w                      |
| 2021                  | 1.5w                     |
| 2022                  | 1.3w                     |
| 2022                  | 2w                       |
| 2022                  | 1w                       |
| 2022                  | 0.5w                     |
+-----------------------+--------------------------+

```

建表 ：

```
create table hive_distribute(
year string,
earning string
)
```

执行语句 ：

```
select  *  from hive_distribute distribute by year   sort by earning;

+-----------------------+--------------------------+
| hive_distribute.year  | hive_distribute.earning  |
+-----------------------+--------------------------+
| 2022                  | 0.5w                     |
| 2020                  | 0.5w                     |
| 2022                  | 1.3w                     |
| 2021                  | 1.5w                     |
| 2021                  | 10w                      |
| 2021                  | 19w                      |
| 2022                  | 1w                       |
| 2020                  | 1w                       |
| 2020                  | 1w                       |
| 2021                  | 20w                      |
| 2022                  | 2w                       |
| 2020                  | 2w                       |
+-----------------------+--------------------------+


```

## Cluster By

ClusterByis a short-cut for both DistributeByand Sort By.

distributeby year   sort by year  《=》 ClusterBy year 正确

当distribute by 和 sort by字段相同时，可以使用cluster by。
cluster by除了具有distribute by的功能外还兼具sort by的排序功能。但是排序只能是默认的升序，无法指定排序规则。

### 分桶表

hdfs上的文件 ，本地文件会找不到文件，一般只能识别hdfs上的

分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。 对于 hive 中每一个表、分区都可以进一步进行分桶。 由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。

要先开启分桶支持

```
set hive.enforce.bucketing=true;
```

分桶表的創建

```
 [CLUSTERED BY (col_name, col_name, ...) 
  INTO num_buckets BUCKETS]
```

数据

```
1,name1
2,name2
3,name3
4,name4
5,name5
6,name6
7,name7
8,name8
```

创建表

```
create table hive_bucket(
id int,
name string 
)
clustered by (id) into 4 buckets
row format delimited fields terminated by ",";
```

查询桶中数据

```
select id,name from hive_bucket tablesample(bucket 4 out of 4 on name); //bucket后面的数字就是我们要查看的桶的编号 out of 后面的是总数 ，on 后面的是我们分桶的属性
```

mapreduce:

hash % reducetask个数

文件存储格式

* 行式存储  ：
  * 里面的列 掺杂很多数据类型
  * 一行内容所有的列都在一个 block里面
  * 行式存储加载所 是把所有的列都查询出来 再过滤出 用户需要的列
  * 如果用户 仅仅查几个字段  =》 磁盘io 开销比较大
  * textfile 文本文件
  * SequenceFile 文本文件
* 列式存储 ：
  * 按照列进行存储
  * 前提： 企业 table 字段 几十个 到几百个
  * RCFile  =》 行 =》 列
  * ORC Files + Parquet
  * 查询几个列
  * 加载表中所有字段
* 列式存储文件 数据量 比 行式存储的数据量少 【前提 都采用压缩】

```
create table hive_distribute_col(
year string,
earning string
)
row format delimited fields terminated by ','
stored as orc; // 这个就是存储形式
```

## hive 中文件存储格式 vs 压缩

压缩格式 ：

Hive支持的压缩格式有bzip2、gzip、deflate、snappy、lzo等。Hive依赖Hadoop的压缩方法，所以Hadoop版本越高支持的压缩方法越多，可以在$HADOOP_HOME/conf/core-site.xml中进行配置：

```
<property>  
        <name>io.compression.codecs</name>  
        <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec
        </value>  
</property>  

```

常见的压缩格式 ：

| 压缩格式 | 算法实现 | 压缩比 | 效率 | 可切分 | 内置 | 扩展名       | Native | Java | 描述                                                                    |
| -------- | -------- | ------ | ---- | ------ | ---- | ------------ | ------ | ---- | ----------------------------------------------------------------------- |
| bzip2    | bzip2    | 最高   | 慢   | yes    | Y    | .bz2         | yes    | yes  | 压缩率最高，一般是源文件的30%左右 ：<br />压缩或者解压效率最慢          |
| deflate  | DEFLATE  | 高     | 慢   | no     | Y    | .deflate     | no     | yes  | 标准的压缩格式                                                          |
| gzip     | DEFLATE  | 高     | 慢   | no     | Y    | .gz          | no     | yes  | 相比deflate增加文件头，尾，<br />压缩率比较高，压缩或者解压的效率比较慢 |
| zlib     | DEFLATE  | 高     | 慢   | no     | Y    | .zl          | yes    | no   | 相比deflate增加文件头，尾                                               |
| lz4      | lz4      | 最低   | 最快 | no     | Y    | .zl4         | yes    | no   | 压缩率比较低，不过压缩和解压效率最快                                    |
| lzo      | lzo      | 较低   | 快   | yes    | N    | .lzo_deflate | yes    | no   | 压缩率比较低，不过压缩和解压效率最快                                    |
| lzop     | snappy   | 较低   | 快   | yes    | N    | .lzo         | yes    | no   | 压缩率比较低，不过压缩和解压效率最                                      |
| snappy   | snappy   | 较低   | 快   | yes    | N    | .snappy      | yes    | no   | 压缩率比较低，不过压缩和解压效率最                                      |

其中压缩比bzip2 > zlib > gzip > deflate > snappy > lzo > lz4，在不同的测试场景中，会有差异，这仅仅是一个大概的排名情况。bzip2、zlib、gzip、deflate可以保证最小的压缩，但在运算中过于消耗时间。

从压缩性能上来看：lz4 > lzo > snappy > deflate > gzip > bzip2，其中lz4、lzo、snappy压缩和解压缩速度快，压缩比低。

所以一般在生产环境中，经常会采用lz4、lzo、snappy压缩，以保证运算效率。

## Native Libraries


Hadoop由Java语言开发，所以压缩算法大多由Java实现；但有些压缩算法并不适合Java进行实现，会提供本地库Native Libraries补充支持。Native Libraries除了自带bzip2, lz4, snappy, zlib压缩方法外，还可以自定义安装需要的功能库（snappy、lzo等）进行扩展。

而且使用本地库Native Libraries提供的压缩方式，性能上会有50%左右的提升。

使用命令可以查看native libraries的加载情况：
`hadoop checknative -a`

完成对Hive表的压缩，有两种方式：配置MapReduce压缩、开启Hive表压缩功能。因为Hive会将SQL作业转换为MapReduce任务，所以直接对MapReduce进行压缩配置，可以达到压缩目的；当然为了方便起见，Hive中的特定表支持压缩属性，自动完成压缩的功能。
