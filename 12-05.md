---
title: 大数据的简单构架
date: 12-05 8.53 
categories: 日志
comments: "true"
tag: sqoop
---
# 大数据的三件事

数据采集  ：

* 采集业务数据 ： sqoop ,datax,实时采集maxwell ,flinkdoc
* 采集日志数据 ： flume ，logstush

数据存储 :

* hdfs (hadoop中的)
* hive
* hbase【大数据】
* 数据分析之后的结果数据 ：
  * mysql
  * clickhouse
  * drios

数据分析：

* map reduce 但是现在不怎么用了，但是思想最重要
* hive ： 主要是离线数仓
* hbase
* spark
* flink

数据可视化 ：

* 如果有前端开发人员，可以让他们来帮忙
* 但是如果没有 要自己做
  * superset
  * dataease
  * echarts
  * env
  * anv
* 收费 : 简历上最好不要写这个
  * quickbi
  * sugar

消息中间键 ：

* kafka
* pular

(即席查询 ： 临时查询) : presto是最好用的 clickhouse 是有bug的 对内存有要求

* sparksql , presto , druid , clickhouse ,kylin(cube)

数据种类 ：

* 业务数据【mysql ， es】app
* 日志数据 【log】linux 磁盘上 ，工作中处理的一个重点
  * 展现日志，点击日志，跳转日志
* 其他数据

架构图  ：

* 业务数据 ： mysql -》sqoop ， datax -》 hds/hive
* 日志数据 ： log文件 -》 flume -》 hdfs/hive
* hive : 构建离线数仓
  * 数据分层
  * 维度建模
  * 指标输出
* 数据可实话
  * hive -》 sqoop -》 mysql / clickhouse -》数据可视化

大数据的基础平台架构

提升 ：大数据的数据平台

大数据基础是相当于 从  0-1 搭建 -》 可以学到以上的所有框架

大数据的数据平台 是基于基础平台再提升了升级 -》 这个学不到什么东西

大数据升级平台

大数据数据开发

* 离线数仓
* 实时数仓
* 临时查询

大数据的etl工程师

* 数据清洗
* 数据抽取
* 数据转换

大数据运维工程师

* 上述的框架是它负责安装以及部署的
* 以及后续的维护
* 云原生 ， docker ，k8s

大数据算法组（数据分析师sql + 数学知识统计 。数据科学家）

* 用户画像
* 数据挖掘
  * python
  * spark , flink 自带的组件 机器学习相关的组件 ： 速度要比py快

新颖的

* 数据湖 -》主要研究方向
* 云原生 -》 docker ，k8s
  * job -》 申请资源 是再yarn上的 ，但是当yarn 做资源隔离的时候万一有三台机器 ，到时候如果所有container 都集中在一台机器上，则会造成机器得负载太大
  * 解决方法 ：
    * yarn 的底层编码重写
    * 联合云原生

# sqoop

简介 ： 可以把数据和hadoop生态圈进行数据库同步，数据传输

sqoop

* 我们可以通过sqoop这个组件 ，把mysql 里的表 同步到 hdfs，hive ，hbase
* 反之也可以
* 原理
  * sqoop 是只用map阶段 ，无reduce 阶段 （通过mapreduce 实现的）
* 指定的参数
  * url
  * username
  * password
  * 驱动
* sqoop版本
  * sqoop 1 ：1.4.7
  * sqoop 2 ：1.99.7
  * 注意这两个是没有任何联系的
  * 建议用1

# 部署sqoop

这里我使用的是1.4.7的sqoop包

首先我们上传到linux上

然后解压

解压之后我们进入到conf目录下，这个里面存的是我们的 配置文件

我们把sqoop-env-template.sh 改名字成 sqoop-env.sh

然后vim 它进行编辑

把hadoop home 的路径放上 ，以及 hive的路经

然后保存退出

我们接下来在全局变量中注册一下sqoop的bin目录

然后把mysql的connect包给到lib文件夹下

然后执行

```
1.查看可用的数据库 【mysql】
sqoop list-databases \
--connect jdbc:mysql://bigdata2:3306  \
--username root  \
--password liuzihan010616
```

1.4.7会报错 ，因为 缺少java.commons.lang包

我们把这个包上传到lib下就好了

# 导入和导出

sqoop的导入和导出

从mysql里导出数据的时候，会默认导入到/user/hadoop/*

hdfs 文件存储 默认是用 ，进行分割每个字段的

hdfs上有几个文件就是有几个map task 和reduce task

其默认的数量是4

导出

```
sqoop import \
--connect jdbc:mysql://bigdata2:3306/hive  \
--username root  \
--password liuzihan010616 \
--table TBLS
```

设置导出的列的参数

`--columns`

设置字段筛选的参数

`--where`

设置再yarn上的作业名称

`--mapreduce-job-name`

设置 map 和 reduce task 的个数

`-m,--num-mappers <n> `

用-m 或者后面的都行

设置hdfs上的文件夹

`--target-dir`

设置hdfs上存储的分隔符

`--fields-terminated-by `

删除目标文件夹

`--delete-target-dir `

总体应用

```
sqoop  import \
--connect jdbc:mysql://bigdata2:3306/hive  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
-m 1 \
--mapreduce-job-name 'mysql 的数据try' \
--where 'TBL_ID >= 10' \
--columns 'TBL_ID , OWNER' \
--table TBLS
```

有主键的表可以直接按照上述同步

但是没主键的，要转化

如果是没有主键的表，有两种转换方法

首先是可以通过  ： -m 设置为1 或者 --split-by 列的名字

如下 ：

```
sqoop  import \
--connect jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
--split-by empno \
--mapreduce-job-name 'mysql 的数据try' \
--table emp
```

空值处理 ：

`--null-non-string 0`

上面的那个是不是string的处理

`--null-string ''`

## 嵌套sql

用 --query

但是有注意的点

有--query 的时候，不能放--table

且--query 后面只能接单引号

如下

```
sqoop  import \
--connect jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
--split-by empno \
--mapreduce-job-name 'mysql 的数据try' \
--query 'select * from emp where $CONDITIONS'
```

# 简化

如上述

我们发现太繁琐了

我们可以进行封装到一起

然后直接调用文件就行

`sqoop --options-file 文件路径`

文件内容如下 ：

```
import 
--connect 
jdbc:mysql://bigdata2:3306/try  
--username 
root  
--password 
liuzihan010616 
--delete-target-dir 
--fields-terminated-by 
'\' 
--target-dir
/ghk 
--split-by 
empno 
--mapreduce-job-name 
'mysql 的数据try' 
--query 
'select * from emp where $CONDITIONS'
```

### sqoop job

* create 创建job
* list/show 查看job list 是查看列表 show 是查看详情
* exec 执行job

代码如下 ：

```
scoop job --create mysqltry -- \
import \
--connect 
jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
--split-by empno \
--mapreduce-job-name 'mysql 的数据try' \
--query 'select * from emp where $CONDITIONS'
```

## shell脚本

也可以通过shell脚本调用 sqoop

如下 ：普通表的

```



if [ $# -lt 6 ];then
 echo "$0 use sync mysql 2 hive"
 echo "USAGE:$0 mysqldb sql hivedb hivetable idautocreatetable fengefu"
 echo "Example mysql的数据库 sql语句 hive的数据库  hive的表 分隔符 是不是自动创建表"
 exit 1;
fi

#mysql parm
mysqldb=$1
sql="$2"
#hive parms 
hivedb=$3
hivetable=$4
flag =$6

try 'select * from emp' bigdata_hive3 emp6 , 1

MySQL_URL="jdbc:mysql://bigdata2:3306/${mysqldb}" 
MySQL_USER=root
MySQL_PASSWD=liuzihan010616
FIELDS_TERMINATED=$5

if [ ${flag} -eq 1 ];then
sqoop import \
--connect ${MySQL_URL}  \
--username ${MySQL_USER}  \
--password ${MySQL_PASSWD} \
--mapreduce-job-name 'mysql2hive' \
--delete-target-dir \
--target-dir /sqoop/emp_tmp \
--fields-terminated-by ${FIELDS_TERMINATED} \
-m 1 \
--query "${sql} and \$CONDITIONS " \
--hive-import \
--create-hive-table \
--hive-overwrite \
--hive-database ${hivedb} \
--hive-table ${hivetable}
else
sqoop import \
--connect ${MySQL_URL}  \
--username ${MySQL_USER}  \
--password ${MySQL_PASSWD} \
--mapreduce-job-name 'mysql2hive' \
--delete-target-dir \
--target-dir /sqoop/emp_tmp \
--fields-terminated-by ${FIELDS_TERMINATED} \
-m 1 \
--query "${sql} and \$CONDITIONS " \
--hive-import \
--hive-overwrite \
--hive-database ${hivedb} \
--hive-table ${hivetable}
exit 200;
fi
---------------------------------分区表--------------------------------------------
if [ $# -lt 8 ];then
 echo "$0 use sync mysql 2 hive"
 echo "USAGE:$0 mysqldb sql hivedb hivetable idautocreatetable fengefu hivepartition hivepartitionvalue"
 echo "Example mysql的数据库 sql语句 hive的数据库  hive的table 分隔符  hive的分区属性 分区属性的值 是不是自动创建表"
 exit 1;
fi

#mysql parm
mysqldb=$1
sql="$2"
#hive parms 
hivedb=$3
hivetable=$4
hivepartition=$6
hivepartitionvalue=$7
flag=$8
MySQL_URL="jdbc:mysql://bigdata2:3306/${mysqldb}" 
MySQL_USER=root
MySQL_PASSWD=liuzihan010616
FIELDS_TERMINATED=$5


if [ ${flag} -eq 1 ];then
sqoop import \
--connect ${MySQL_URL}  \
--username ${MySQL_USER}  \
--password ${MySQL_PASSWD} \
--mapreduce-job-name 'mysql2hive' \
--delete-target-dir \
--target-dir /sqoop/emp_tmp \
--fields-terminated-by ${FIELDS_TERMINATED} \
-m 1 \
--query "${sql} and \$CONDITIONS " \
--hive-import \
--hive-overwrite \
--create-hive-table \ 
--hive-database ${hivedb} \
--hive-table ${hivetable} \
--hive-partition-key ${hivepartition} \
--hive-partition-value ${hivepartitionvalue}
else
sqoop import \
--connect ${MySQL_URL}  \
--username ${MySQL_USER}  \
--password ${MySQL_PASSWD} \
--mapreduce-job-name 'mysql2hive' \
--delete-target-dir \
--target-dir /sqoop/emp_tmp \
--fields-terminated-by ${FIELDS_TERMINATED} \
-m 1 \
--query "${sql} and \$CONDITIONS " \
--hive-import \
--hive-overwrite \
--hive-database ${hivedb} \
--hive-table ${hivetable} \
--hive-partition-key ${hivepartition} \
--hive-partition-value ${hivepartitionvalue}
fi
-----------------------------------------------------hivetomysql的----------------------------------------------------
if [ $# -lt 4 ];then
echo "error 变量小于4个"
echo "example try , /user/hive/warehouse/bigdata_hive3.db/emp_partition/deptno=20 emp1"
echo "mysql数据库 分隔符 hdfs上的路径 mysql里的表名"
fi

mysqldb=$1
fengefu=$2
hdfslujing=$3
mysqltable=$4

hive -e "use bigdata_hive3 ; create table "

sqoop export \
--connect jdbc:mysql://bigdata2:3306/${mysqldb}  \
--username root  \
--password liuzihan010616 \
--fields-terminated-by ${fengefu} \
--export-dir ${hdfslujing} \
--null-non-string 0
--null-string ''
--table ${mysqltable} 
----------------------------------------------hivetomysql的分区表如何同步-----------------------
先把hive的分区表用create table zz as elect * from xxx(分区表的名字) 
然后把zz当成普通表传过去 ，但是在sqoop1.4.7 目前这个功能出现了些问题
 
```

# 从mysql 到 hive等工具中

大部分都和上述一样的

只不过要换个链接以及表名字

```
sqoop import \
--connect jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
--split-by empno \
--mapreduce-job-name 'mysql 的数据try' \
--query 'select * from emp where $CONDITIONS' \
--hive-import \
--hive-overwrite \
--create-hive-table \ 
--hive-database bigdata_hive3 \
--hive-table emp_hive1
```

在1.4.7的版本中其要求必须 加上 `--target dir 属性`

且要从hive的lib文件夹下，把所有jar包给sqoop的lib下

上述 create-hive-table 是自动创建表

但是因为mysql里的属性只有几个 ，没hive特有的decmical等，可能会造成丢失数据

所以提议先在hive中创建表，然后再导入

## 创建分区表

分区表如上 ；只不过要用

`--hive-partition-key xxxx（列名）`

`--hive-partition-value xxx(分区字段的数值)`

或者可以直接用--query 用代码的方式进行分区

```
sqoop import \
--connect jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--delete-target-dir \
--fields-terminated-by '\' \
--target-dir /ghk \
--split-by empno \
--mapreduce-job-name 'mysql 的数据try' \
--query 'select empno,ename,job,mgr,hiredate,sal,comm from emp where deptno=20 and $CONDITIONS ' \
--hive-import \
--hive-overwrite \
--create-hive-table \
--hive-database bigdata_hive3 \
--hive-table emp_partition \
--hive-partition-key deptno \
--hive-partition-value 20
```

## 数据导出

不管是hdfs 或者是hive 都是基于路径导出

通过export导出

代码 ：

```
sqoop export \
--connect jdbc:mysql://bigdata2:3306/try  \
--username root  \
--password liuzihan010616 \
--table xxxX \
--fields-terminated-by '分隔符' \
--export-dir hdfs上的数据的路径 \
```

这个是hdfs导入到mysql

sqoop导入hive到mysql的时候空值要先进行处理，

# 开启历史日志

开启开关 ：

```
<property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
</property>
```

上述是日志收集的开关

下面是日志在hdfs上存储的lujing

```
<property>
        <name>yarn.log.server.url</name>
        <value>http://bigdata3:19888/jobhistory/logs</value>
</property>
```

下面是设置日志收集的时间

```
<property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>259200</value>
</property>
```

接下来我们要配置mapred -site.xml

如下 ：

```
<property>
        <name>mapreduce.jobhistory.address</name>
        <value>bigdata3:10020</value>
</property>
<property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>bigdata3:19888</value>
</property>
```

然后通过命令行的方式执行命令

`yarn timelineserver   [bigdata4]`

`mapred historyserver  [bigdata3] `

提示 ： 我的namenode在bigdata3上 ，而resourcemanager 在 bigdata4上

Hive建表时，默认使用的分隔符时候一个特殊的字符，查看表决结构时候是一个'\001'

这不是真正的'\001'，其实是使用八进制编码\001表示

## hive table -> mysql 多次导入 数据结果不同

幂等性 ： 多次操作 ，数据结果是不变的

mysql - > hive -> hive-overwrite

hive -> mysql 幂等性 如何解决

方法 ：

可以通过 `mysql -u root -p xxx -e sql语句`

或者 `mysql -uroot -pliuzihan010616 < ./try.sql 这个是执行sql文件`

上述两个语句是可以在MySQL外部直接进行执行的，不用进入到mysql里

数据库唯一主键

缺点：无法使用change buffer，InnoDB为了进行唯一性检查，必须有一次磁盘IO读页

业务状态校验

业务上根据业务ID的唯一性和业务处理的结果去做判断，但是这部分判断的逻辑需要考虑原子性。否则会因为并发问题导致幂等失效。解决途径（一）加锁，根据当前的服务环境选择单机或分布式锁。（二）采用现成方案Tomato，通过滑动窗口或者固定窗口拦截控制时间内的请求

数据库乐观锁实现幂等性

缺点：操作业务前，需要先查询出当前的version版本。会增加操作

防重 Token 令牌实现幂等性

缺点：

产生过多额外请求

先删除token，如果业务处理出现异常但token已经删除掉了，再来请求会被认定为重复请求

后删除token，如果删除redis中的token失败了，再来请求不会拦截，发生了重复请求

下游传递唯一序列号实现幂等性

缺点：无法控制下游唯一序列号的生成规则，如果序列号由时间戳生成，那么无法拦截类似重复点击这种情况下的重复请求
