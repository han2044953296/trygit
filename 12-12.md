---
title: flume
date: 12-12 8.53 
categories: 日志
comments: "true"
tag: flume
---
xxl：任务调度的时候

* 设置一个xxl job 完成任务 ，解耦不好，就是代码中常规定义的高内聚 ，低偶合
* 设置多个xxl job，可以解决上述的问题 ， 但是时间不好把握 ， 就是第一个任务和第二个任务的交界处，就是如何判断第一个任务执行完了，如何开启第二个，xxl中有可以控制这个的功能
* 在任务编辑页面中，点击添加子任务id，就可以了

然后只用执行父任务就好

但是不太方便

任务调度框架 ： 推荐dolphinscheduler  官网  ：`dolphinscheduler.apache.org`

周六周日学会

首选dolphinscheduler

airflow 也是比较擅长制作dag（有向无关图）的一个框架官网 ： `airflow.apache.org`

他的dag能力非常好用，但是是要求用python使用的，同样周六周日学会

或者自己开发=》java团队

sqoop是要在yarn上申请资源，然后进行map阶段，它不走reduce阶段，它在yarn上申请资源，就是消耗时间的最大问题

# flume

主要是收集我们的日志数据的

数据采集/数据收集

数据采集：把数据采集到服务器上

数据收集：把数据移动到指定位置

上述是老师之前公司的定义

flume的架构地位 ： 一般采集日志数据，并不用我们做，是java团队要做的

日志数据 -》flume-》hdfs

业务数据 通过sqoop存到hdfs上

数据处理的两种方式 ： 离线 ，实时

上述所处的数据处理方式是离线处理，

实时处理是来一个数据，就处理一个

离线处理是把一定时间内的数据放到一起来进行处理也叫p处理

实时处理的架构线和离线处理的差不多，因为flume采集数据就是实时的

实时 ： 日志数据 -》 flume -》 kafka -》 实时处理框架

离线 ： 日志数据 -》 flume -》 hdfs -》 hive

但是现在有一种框架如下 ：

日志数据 -》 flume -》 kafka

* -》实时
* -》flume -》hdfs -》离线

并不意味着下面的架构比上面两种好，架构没有好与坏，只有合适不合适

因为下面意味着要多加一层维护，消耗人力以及物资

官网 ： `flume.apache.org`

# 简介

官方介绍 ：

收集，聚合，移动日志文件 `Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.`

flume ：采集数据是实时采集的而且支持恢复机制 ` It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.`

flume组件

* source ：采集数据
* channel ： 管道，存储采集过来的数据
* sink ： 移动数据

flume ： 使用场景

采集数据日志-》 hdfs上

# 核心概念

以后操作flume，就是编写agent里面的配置

agent ： 包括上面的那三个组件

# 部署

我们部署的化用1.9版本

第一步 ： 先解压 到app下

第二步 ： 软连接 + 环境变量

第三步 ： 配置flume，修改flume的env.sh文件，把java_home加上

第四步 ： 使用flume ： 配置agent 配置文件

flume user guide ：基本使用

flume develop guide ：二次开发

常用的 source

* avro 序列化框架的source ****
* exec 日志文件
* spooling dir 日志文件
* kafka Source
* Netcat Source 通过端口采集数据
* taildir Source 日志文件 ****
* 等 ，可以自己开发
* 其余都是两个星

常用的channel

* Memory ****
* File  ****
* JDBC *
* kafka *
* Custom ： 用户开发 *
* 等

常用的sink

* hdfs ****
* hive ****
* avro ****
* logger 控制台，打印 **
* HBase *
* kafka *
* http *
* custom *
* 等

如何配置agent

用什么查什么 ： 不用记

需求  ：从指定端口的地方获取数据并输出到控制套

分析  ：

source ：Netcat

channel ： memory

sink ： logger

然后编写一个flume的文件，文件内容如下

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

event  ：就是我们的flume的一条数据，代表数据是通过这三个阶段的一条数据

关于source的参数，下列以netcat为例

```

Property Name	Default	Description
channels	–	 
type	–	The component type name, needs to be netcat
bind	–	Host name or IP address to bind to
port	–	Port # to bind to
max-line-length	512	Max line length per event body (in bytes)
ack-every-event	true	Respond with an “OK” for every event received
selector.type	replicating	replicating or multiplexing
selector.*	 	Depends on the selector.type value
interceptors	–	Space-separated list of interceptors
interceptors.*	 	 
```

关于channel参数

```

Property Name	Default	Description
type	–	The component type name, needs to be memory
capacity	100	The maximum number of events stored in the channel
transactionCapacity	100	The maximum number of events the channel will take from a source or give to a sink per transaction
keep-alive	3	Timeout in seconds for adding or removing an event
byteCapacityBufferPercentage	20	Defines the percent of buffer between byteCapacity and the estimated total size of all events in the channel, to account for data in headers. See below.
byteCapacity	see description	Maximum total bytes of memory allowed as a sum of all events in this channel. The implementation only counts the Event body, which is the reason for providing the byteCapacityBufferPercentage configuration parameter as well. Defaults to a computed value equal to 80% of the maximum memory available to the JVM (i.e. 80% of the -Xmx value passed on the command line). Note that if you have multiple memory channels on a single JVM, and they happen to hold the same physical events (i.e. if you are using a replicating channel selector from a single source) then those event sizes may be double-counted for channel byteCapacity purposes. Setting this value to 0 will cause this value to fall back to a hard internal limit of about 200 GB.
```

关于sink

```

Property Name	Default	Description
channel	–	 
type	–	The component type name, needs to be logger
maxBytesToLog	16	Maximum number of bytes of the Event body to log
```

然后启动我们的flume

执行 `flume-ng agent --conf ${FLUME_HOME}/conf --conf-file /home/hadoop/data/flumeexample.txt -Dflume.root.logger=info,console --name a1`

然后执行telent localhost 44444

往里面发送内容，在我们启动flume的session就会发现内容

```
2022-12-12 14:11:53,172 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 31 0D                                           1. }
2022-12-12 14:11:54,427 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 32 0D                                           2. }
2022-12-12 14:11:54,877 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 33 0D                                           3. }
2022-12-12 14:11:55,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 34 0D                                           4. }
2022-12-12 14:11:55,565 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 35 0D                                           5. }
2022-12-12 14:11:55,891 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 36 0D                                           6. }
2022-12-12 14:11:56,238 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 37 0D                                           7. }
2022-12-12 14:11:56,609 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 38 0D                                           8. }
2022-12-12 14:11:57,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 39 0D                                           9. }
```

如上 ：

生产上常见的

* 采集log文件到hdfs上
* 采集log文件到hive
* 待机log文件到kafka里
* 采集kafka数据到hdfs
* 采集kafka数据到hive
* 采集数据到下一个agent里

source ：

* netcat ： 采集端口数据 ： 学习测试
* 日志文件
* kafka
* agent

采集日志文件

* exec
* spooldir
* taildir

采集数据文件到控制台

* source ： exec
* channel  ： memory
* sink ： logger

# exec

编写agent

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/hadoop/data/loger.txt

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

然后执行 `flume-ng agent --conf ${FLUME_HOME}/conf --conf-file /home/hadoop/data/flumeexample.txt -Dflume.root.logger=info,console --name a1`

结果如下 ：

```
2022-12-12 14:39:25,755 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 31                                              1 }
2022-12-12 14:39:55,761 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 61 61 61 61                                     aaaa }
2022-12-12 14:40:01,909 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 61 61 61 61                                     aaaa }
2022-12-12 14:40:10,910 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 61 61 61 61 73 73                               aaaass }
2022-12-12 14:44:40,961 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 61 61 61 61 73 73 64 73 61 61 64 73             aaaassdsaads }
```

上面的body里的东西是ascII码值的16进制

exec 的方式采集数据的时候，如果停掉flume，然后重新启动的时候，还会再次把文件里的数据再采集一次，造成数据双倍，能用，但是不建议

# spoolingdir

接下来spooldir的采集文件夹下的文件

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = 数据文件夹
a1.sources.r1.fileHeader = true

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

然后执行 `flume-ng agent --conf ${FLUME_HOME}/conf --conf-file /home/hadoop/data/flumeexample.txt -Dflume.root.logger=info,console --name a1`

被采集过的文件会被打上标记，会重命名文件命名为xxx.completed

然后就不会再次采集到这个文件，哪怕是关闭之后重新启动

而且往已经采集的文件下再次加入文件内容的时候，flume会被重新启动，且不能采集到新加的内容，而且文件名字不可以重复，如果重复，flume会挂掉，生产上不怎么使用

# taildir

接下来是taildir ：既可以采集文件夹，也可以采集单个文件，且有断点续传的作用

但是它并不能运行在windows上

编写agent

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /home/hadoop/data/try.txt
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /home/hadoop/data/flumetestdata/.*.log
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000


# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

然后运行 `flume-ng agent --conf ${FLUME_HOME}/conf --conf-file /home/hadoop/data/flumeexample.txt -Dflume.root.logger=info,console --name a1`

它可以实时性的采集数据，是生产上重点，一般都用它，在flume里模糊匹配的语法要加个点在可以

断点续传的文件，a1.sources.r1.positionFile = /var/log/flume/taildir_position.json

可以自己定义，或者是默认，默认是在~/.flume/taildir_position.json

# Sink ：hdfs

指定到hdfs上

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /home/hadoop/data/try.txt
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /home/hadoop/data/flumetestdata/.*.log
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000


# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata3:9000/data

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

flume存储的数据才hdfs山观察看不了，因为其默认的数据格式不对

更改

* hdfs.filetype DataStream
* hdfs.writeFormat : Text

如下

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /home/hadoop/data/try.txt
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /home/hadoop/data/flumetestdata/.*.log
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000


# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata3:9000/flume/events/%Y-%m-%d
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
# 控制小文件的参数
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollInterval = 21600
a1.sinks.k1.hdfs.rollCount = 1000
# 控制大文件的滚动
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.batchSize = 1200
a1.sinks.k1.hdfs.roundValue = 21
# 修改文件前缀和后缀
a1.sinks.k1.hdfs.filePrefix = events
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.useLocalTimeStamp = true


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 5000
a1.channels.c1.transactionCapacity = 2000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

执行 ：`flume-ng agent --conf ${FLUME_HOME}/conf --conf-file /home/hadoop/data/flumeexample.txt -Dflume.root.logger=info,console --name a1`

采集的时候要注意：

因为采集数据会造成小文件问题就是当flume在采集的时候，如果文件一直发生变化的时候，flume可能会造成有多个小文件

可以通过增加参数进行设置

* 按照条数进行滚动：就是生成下一个文件
* 按照时间进行滚动：就是生成下一个文件
* hdfs.round => 文件滚动的开关
* hdfs.batchSize => 按照条目数滚动，一般不会用
* hdfs.roundUnit => 按照时间滚动
* hdfs.roundValue => 时间滚动的具体值
* 文件进行滚动，是针对文件来说的，计算说当一个滚动的时候其余的文件滚动会重置

### 上面的对小文件的时候不好用，下面是一定好用的

* hdfs.rollSize => 按照大小进行滚动 默认是字节
* hdfs.rollInterval =>按照时间进行滚动，秒为单位
* hdfs.rollCount => 文件里存的数据条数进行滚动

### 更改文件前缀和后缀

* hdfs.filePrefix : 文件前缀
* hdfs.fileSuffix ：文件后缀
* 可以在path后面的问价夹地方放上时间戳
* 然后文件夹会自动生成 `a1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H%M/%S `
* `文件夹如下 ：/flume/events/2012-06-12/1150/00 `
* 进行上述更改之后，我们要再次添加上 `hdfs.useLocalTimeStamp = true`
* 这样之后相当于数据分组是用机器的时间，而不是数据本身的时间

### event

* 由两部分组成
* headers ： 描述信息 ，但是一般的时候，这个里面是空的 ， 如果path设置成上述的靠时间戳，则这里不能为空，或者用本地时间代替
* body ： 实实在在的数据 ：8进制的ASCII码值

### 业界问题

* 数据延迟的问题，对于数据假如在，23.58分采集，而flume有延迟，两分钟后才同步过来，就会出现数据本身的时间（采集时间）和本机local时间不一样，然后会影响到hdfs上文件夹的目录的存储
* 解决方法  ： log -》 flume -》 hive
* 定义udf ：函数 ： 保证正确的数据重新落盘到正确的分区 ： 数据清理
* 从flume 源头解决： 用文件创建的时间，就是在header里设置时间
  * 要二次开发才可以解决

## 同步到hive中

我们可以通过同步到hdfs中然后同步到ihive上，因为hive的数据在hdfs上

### 普通表：

对于普通表我们之间把文件放到hive的存储路径下，只要分隔符对，就可以了

### 分区表：

对于已经有的分区，之间把数据往分区文件夹里存储就好

对于没有的分区，把数据按照文件夹传输上去之后，我们还要在hive的源数据库里添加分区

通过：`alter table emp_partition add partition(deptno=10);`

然后建完分区，它就会显示数据了

# Sink:hive

以下是官方提供的hive的flume参数

```
a1.channels = c1
a1.channels.c1.type = memory
a1.sinks = k1
a1.sinks.k1.type = hive // 类型
a1.sinks.k1.channel = c1 
a1.sinks.k1.hive.metastore = thrift://bigdata3:9083 //元数据库
a1.sinks.k1.hive.database = logsdb 数据库
a1.sinks.k1.hive.table = weblogs 表
a1.sinks.k1.hive.partition = asia,%{country},%Y-%m-%d-%H-%M 分区字段
a1.sinks.k1.useLocalTimeStamp = false 是不是使用本地时间戳
a1.sinks.k1.round = true 
a1.sinks.k1.roundValue = 10
a1.sinks.k1.roundUnit = minute 
a1.sinks.k1.serializer = DELIMITED   负责解析事件中的字段并将它们映射到hive表中的列 
a1.sinks.k1.serializer.delimiter = "\t" 传入数据的分隔符（每个字段之间的）
a1.sinks.k1.serializer.serdeSeparator = '\t' 输出字段分隔符,单引号括起来，例如'\t'
a1.sinks.k1.serializer.fieldnames =id,,msg 参数名字（表的）
```

然后我们自己写一个

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /home/hadoop/data/emp_202211301118.csv
a1.sources.r1.headers.f1.headerKey1 = value1


# Describe the sink
a1.sinks.k1.type = hive
a1.sinks.k1.serializer = DELIMITED
a1.sinks.k1.hive.metastore = thrift://bigdata3:9083
a1.sinks.k1.hive.database = bigdata_hive3
a1.sinks.k1.hive.table = emp22
a1.sinks.k1.serializer.delimiter = ","
a1.sinks.k1.serializer.serdeSeparator = ','
a1.sinks.k1.serializer.fieldnames =empno,ename,job,mgr,hiredate,sal,comm,deptno


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

按照上述操作之后会报错，原因 ： 缺少依赖包

报错信息如下：

```
2022-12-13 11:19:42,806 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:150)] Failed to start agent because dependencies were not found in classpath. Error follows.
java.lang.NoClassDefFoundError: org/apache/hive/hcatalog/streaming/RecordWriter
	at org.apache.flume.sink.hive.HiveSink.createSerializer(HiveSink.java:220)
	at org.apache.flume.sink.hive.HiveSink.configure(HiveSink.java:203)
	at org.apache.flume.conf.Configurables.configure(Configurables.java:41)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:453)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.streaming.RecordWriter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 13 more

```

而且其中的metastore要进行启动

### 启动metastore

`hive --service metastore`

然后才能进去

上述错误是因为我们缺少依赖造成的是 `hive-hcatalog-streaming.jar`

我们可以通过idea的maven项目进行下载

下载之后导入到我们的flume/lib目录下，就可以运行了

然后还可能遇见以下的问题：

```
22/03/24 11:09:26 WARN hive.HiveSink: k2 : Failed connecting to EndPoint {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
org.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
	at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:99)
	at org.apache.flume.sink.hive.HiveSink.getOrCreateWriter(HiveSink.java:346)
	at org.apache.flume.sink.hive.HiveSink.drainOneBatch(HiveSink.java:297)
	at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:254)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hive.hcatalog.streaming.StreamingException: Cannot stream to table that has not been bucketed : {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.<init>(AbstractRecordWriter.java:69)
	at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.<init>(DelimitedInputWriter.java:115)
	at org.apache.flume.sink.hive.HiveDelimitedTextSerializer.createRecordWriter(HiveDelimitedTextSerializer.java:66)
	at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:89)
	... 6 more
22/03/24 11:09:26 ERROR flume.SinkRunner: Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: org.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
	at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:269)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
	at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:99)
	at org.apache.flume.sink.hive.HiveSink.getOrCreateWriter(HiveSink.java:346)
	at org.apache.flume.sink.hive.HiveSink.drainOneBatch(HiveSink.java:297)
	at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:254)
	... 3 more
Caused by: org.apache.hive.hcatalog.streaming.StreamingException: Cannot stream to table that has not been bucketed : {metaStoreUri='thrift://cdh-1:9083', database='ods', table='ods_flume_log', partitionVals=[20220324] }
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.<init>(AbstractRecordWriter.java:69)
	at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.<init>(DelimitedInputWriter.java:115)
	at org.apache.flume.sink.hive.HiveDelimitedTextSerializer.createRecordWriter(HiveDelimitedTextSerializer.java:66)
	at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:89)
	... 6 more

```

这个是因为Flume写入hive表时，需要hive表支持事务，所以hive表必须是事务表

### 开启事务表

在hive命令行运行以下命令：

```
SET hive.support.concurrency = true;
SET hive.enforce.bucketing = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.compactor.initiator.on = true;
SET hive.compactor.worker.threads = 1;

```

创建分区分桶表并开启事务

```
create table ods_flume_log(line string) 
partitioned by (dt string) 
clustered by (line) into 1 buckets 
stored as orc tblproperties ('transactional'='true');

---------------------------------------------------------------

partitioned by (dt string) - 指定分区字段
clustered by (line) – 指定分桶的字段
stored as orc - 分桶格式orc
tblproperties (‘transactional’=‘true’) - tblproperties可以添加一些hive属性，这里是开启事务

我这里只有一个字段，因为按时间分区了，所以只设置了一个桶，各位看自己情况创建。

再次运行Flume后，数据正常写入hive中
```

### 我们要不要使用双层flume ： 不要

### avro

使用场景 ： 第一个agent的输出作为第二个的输入

需求 ：读取1111端口的数据并发送到2222端口，然后把数据写入hdfs

分析 ： 两层flume

agent ：

* nc-mem-avro
* avro-mem-hdfs/logger

agent1：

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 1111

a1.channels.c1.type = memory

a1.sinks.k1.type = avro
a1.sinks.k1.hostname=bigdata32
a1.sinks.k1.port=2222

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

agent2:

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = avro
a1.sources.r1.bind = bigdata32
a1.sources.r1.port = 2222
a1.channels.c1.type = memory
a1.sinks.k1.type = logger
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

# Source : kafka

```
a1.sources = r1  
a1.channels = c1  
a1.sinks = k1  
  
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = bigdata3:9092,bigdata4:9092,bigdata5:9092
a1.sources.r1.kafka.topics = mytopic
a1.sources.r1.kafka.consumer.group.id = group1
a1.sources.r1.channels=c1  


a1.channels.c1.type=memory  
a1.channels.c1.capacity=1000  
a1.channels.c1.transactionCapacity=100  


a1.sinks.k1.type=logger  
a1.sinks.k1.channel=c1  
```

# Sink : kafka

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1


# netcat 监听端口
a1.sources.r1.type = netcat
a1.sources.r1.bind =master1
a1.sources.r1.port = 10000
a1.sources.r1.channels = c1 
# 一行的最大字节数
a1.sources.r1.max-line-length = 1024000


# channels具体配置
a1.channels.c1.type = memory 
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100


# KAFKA_sinks
a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = hello
a1.sinks.k1.brokerList = master1:9092,slave1:9092,slave2:9092
a1.sinks.k1.requiredAcks = 1
a1.sinks.k1.batchSize = 20
```

# 编写脚本：

关于上述的文件每次更改都要进行一次的编写，为避免有些太麻烦了

于是我们可以通过编写shell脚本的方式进行编写

如下

```shell
if [ $# -lt 3 ];then
echo "使用教程，把taildir的position路径改成自己的，还有hdfs的机器名改成自己的，以及hive的机器名，以及kafka的集群地址"
echo "error more fouth was need"
echo "Source channel Slink"
echo  "第一个变量以及第二个变量分别是有几个channel和几个Sink flagSink=1 flagChannel=2 第三个参数是Source然后是其内部配置，然后是channel，以此类推，下面分别介绍各种配置参数"
echo "source : exec sqoo avro taildir netcat"
echo "taildir 选择模式:1是文件模式，2是文件夹模式，3是一起的 本地文件或者文件夹路径，最多支持一个文件以及一个文件夹，用，分割  具体的文件夹/文件路径 是不是开启阻断1是开启，其他是不开启 选择channel"
echo "exec/spoo 文件/文件夹 是不是开启阻断 选择channel"
echo "netcat/avro 主机ip 端口 是不是开启阻断 选择channel"
echo "kafka topic名字 group.id batchSize batchDurationMillis"
echo "阻断： type key value"
echo "Channel : mem file"
echo "mem 容量 事务容量 选择Sink"
echo "file point文件夹 data文件夹 选择Sink"
echo "Sink : hdfs hive avro logger"
echo "hdfs hdfs上的路经 文件前缀 文件后缀 要不要打开压缩1是开启，其他是不开启 设置压缩格式（开启之后）"
echo "hive hive数据库 hive表 差分文件的分隔符 输入表的分隔符 字段的映射"
echo "avro 主机ip 端口"
echo "kafka topic名字 ack模式： acks=0表明producer完全不管发送结果；acks=all或-1表明producer会等待ISR所有节点均写入后的响应结果；acks=1，表明producer会等待leader写入后的响应结果 batchSize：配置多少条消息可以在 一个处理批次 被发送到 Kafka Broker。较大的批可以提高吞吐量，同时增加延迟。"
echo "logger"
echo "文件命名采用随机数的方式+变量组合的方式"
echo  "多通道例子   2 2 failover replicating 15000 10 6 taildir 1 '/home/hadoop/data/1.log' 1 static 1-boy boy mem 10000 10000 file /home/hadoop/data/flumepostion/ /home/hadoop/data/flumedata/ logger avro localhost 1111 0"
echo "单通道 1 1 avro localhost 1111 0 file /home/hadoop/data/flumepostion/ /home/hadoop/data/flumedata/ logger 0"
echo "最后一个参数是控制是不是开启http监控的监控端口是5555 1是开启监控 其他是不开启"
exit 1;
fi

filename=$(($RANDOM%9999+6))-$1-$2-$3

echo "a1.sources = r1 ">> ./${filename}

Sinkhead="a1.sinks = "
Sinklist=""
flagSink=$1





function Sinkgt1(){
if [ ${flagSink} -ne 1 ];then
for ((i=1;i<=${flagSink};i++))
do
    Sinklist=${Sinklist}k$i" "
done
echo ${Sinkhead}${Sinklist} >> ./${filename}
echo "a1.sinkgroups = g1" >> ./${filename}
echo "a1.sinkgroups.g1.sinks = "${Sinklist} >> ./${filename}
echo "a1.sinkgroups.g1.processor.type = $1">> ./${filename}
Channelgt1 ${@:2}
else 
echo "a1.sinks = k1" >> ./${filename}
Channelgt1 ${@:1}
fi 
}



Channelhead="a1.channels = "
Channelist=""
flagChannel=$2
SinkChannelHead="a1.sources.r1.channels = "

function Channelgt1(){
if [ ${flagChannel} -ne 1 ];then
for ((i=1;i<=${flagChannel};i++))
do
    Channelist=${Channelist}c$i" "
done
echo ${Channelhead}${Channelist} >> ./${filename}
echo ${SinkChannelHead}${Channelist} >> ./${filename}
echo "a1.sources.r1.selector.type = $1" >> ./${filename}
panduan ${@:2}
else 
echo "a1.channels = c1" >> ./${filename}
echo "a1.sources.r1.channels = c1">> ./${filename}
panduan ${@:1}
fi
}



function Fileordir()
{

    case $1 in
    1)
        echo "a1.sources.r1.filegroups = f1" >> ./${filename}
        echo "a1.sources.r1.filegroups.f1 = $2" >> ./${filename}
        echo "a1.sources.r1.headers.f1.headerKey1 = value1" >> ./${filename}
        echo "a1.sources.r1.fileHeader = true" >> ./${filename}
        echo "a1.sources.ri.maxBatchCount = 1000" >> ./${filename}
        case $3 in
        1)
        interceptors ${@:4}
        ;;
        *)
        selectchinnal ${@:4}
        ;;
        esac
    ;;
    2)
        echo "a1.sources.r1.filegroups = f2" >> ./${filename}
        echo "a1.sources.r1.filegroups.f2 = $2" >> ./${filename}
        echo "a1.sources.r1.headers.f2.headerKey1 = value2" >> ./${filename}
        echo "a1.sources.r1.headers.f2.headerKey2 = value2-2" >> ./${filename}
        echo "a1.sources.r1.fileHeader = true" >> ./${filename}
        echo "a1.sources.ri.maxBatchCount = 1000" >> ./${filename}
        case $3 in
        1)
        interceptors ${@:4}
        ;;
        *)
        selectchinnal ${@:4}
        ;;
        esac
    ;;
    3)
        echo "a1.sources.r1.filegroups = f1" >> ./${filename}
        echo "a1.sources.r1.filegroups.f1 = $2" >> ./${filename}
        echo "a1.sources.r1.headers.f1.headerKey1 = value1" >> ./${filename}
        echo "a1.sources.r1.filegroups = f2" >> ./${filename}
        echo "a1.sources.r1.filegroups.f2 = $3" >> ./${filename}
        echo "a1.sources.r1.headers.f2.headerKey1 = value2" >> ./${filename}
        echo "a1.sources.r1.headers.f2.headerKey2 = value2-2" >> ./${filename}
        echo "a1.sources.r1.fileHeader = true" >> ./${filename}
        echo "a1.sources.ri.maxBatchCount = 1000" >> ./${filename}
        case $4 in
        1)
        interceptors ${@:5}
        ;;
        *)
        selectchinnal ${@:5}
        ;;
        esac
    ;; 
    esac
}


function interceptors(){

echo "a1.sources.r1.interceptors = example" >> ./${filename}
echo "a1.sources.r1.interceptors.example.type = $1" >> ./${filename}
echo "a1.sources.r1.interceptors.example.key = $2">> ./${filename}
echo "a1.sources.r1.interceptors.example.value = $3">> ./${filename}
selectchinnal ${@:4}
}



flagChannellins=${flagChannel}
function selectchinnal() 
{
case $1 in
    "mem")
        echo "a1.channels.c${flagChannellins}.type = memory">> ./${filename}
        echo "a1.channels.c${flagChannellins}.capacity = $2">> ./${filename}
        echo "a1.channels.c${flagChannellins}.transactionCapacity = $3" >> ./${filename}
        flagChannellins=$((${flagChannellins}-1))
        if [ ${flagChannellins} == 0 ];then
        selectSinks ${@:4}
        else
        selectchinnal ${@:4}
        fi
    ;;
    "file")
        echo "a1.channels.c${flagChannellins}.type = file" >> ./${filename}
        echo "a1.channels.c${flagChannellins}.checkpointDir = $2" >> ./${filename}
        echo "a1.channels.c${flagChannellins}.dataDirs = $3">> ./${filename}
        flagChannellins=$((${flagChannellins}-1))
        if [ ${flagChannellins} == 0 ];then
        selectSinks ${@:4}
        else
        selectchinnal ${@:4}
        fi
    ;;
    *)
        echo "无匹配的channel"
        exit 1
    ;;
esac
}



flagSinklins=${flagSink}


function selectSinks()
{
        case $1 in
            "logger")
                echo "a1.sinks.k${flagSinklins}.type = logger" >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins
                else
                selectSinks ${@:2}
                fi

            ;;
            "hdfs")
                echo "a1.sinks.k${flagSinklins}.type = hdfs" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.path = hdfs://bigdata3:9000$2" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.filePrefix = $3" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.fileSuffix = $4" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.writeFormat = Text" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.rollSize = 134217728" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.rollInterval = 21600" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.rollCount = 1000" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.round = true" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.roundUnit = minute" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.batchSize = 1200" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.roundValue = 21" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.useLocalTimeStamp = true" >> ./${filename}
                case $5 in 
                1)
                echo "a1.sinks.k${flagSinklins}.hdfs.codeC = $6" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hdfs.fileType = CompressedStream" >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins ${@:7}
                else
                selectSinks ${@:7}
                fi
                ;;
                *)
                echo "a1.sinks.k${flagSinklins}.hdfs.fileType = DataStream" >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins ${@:6}
                else
                selectSinks ${@:6}
                fi
                ;;
                esac
            ;;
            "avro")
                echo "a1.sinks.k${flagSinklins}.type = avro " >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hostname=$2" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.port=$3"  >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins ${@:4}
                else
                selectSinks ${@:4}
                fi
            ;;
            "hive")
                echo "a1.sinks.k${flagSinklins}.type = hive" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.serializer = DELIMITED" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hive.metastore = thrift://bigdata3:9083" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hive.database =$2" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.hive.table = $3"  >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.serializer.delimiter = $4"  >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.serializer.serdeSeparator = $5"  >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.serializer.fieldnames =  $6"  >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlinshive ${@:7}
                else
                selectSinks ${@:7}
                fi
            ;;
            "kafka")
                echo "a1.sinks.k${flagSinklins}.type = org.apache.flume.sink.kafka.KafkaSink">> ./${filename}
                echo "a1.sinks.k${flagSinklins}.topic = $2">> ./${filename}
                echo "a1.sinks.k${flagSinklins}.brokerList = bigdata3:9092,bigdata4:9092,bigdata5:9092">> ./${filename}
                echo "a1.sinks.k${flagSinklins}.requiredAcks = $3" >> ./${filename}
                echo "a1.sinks.k${flagSinklins}.batchSize = $4" >> ./${filename}
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlinshive ${@:5}
                else
                selectSinks ${@:5}
                fi
            ;;
            *)
                echo "无匹配的Sink"
                exit 1
            ;;
        esac
}


function codeC(){
    case $5 in 
                1)
                echo "a1.sinks.k${flagSinklins}.hdfs.codeC = $6"
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins
                else
                selectSinks ${@:7}
                fi
                ;;
                *)
                flagSinklins=$((${flagSinklins}-1))
                if [ ${flagSinklins} == 0 ];then
                    cleanlins
                else
                selectSinks ${@:6}
                fi
                ;;
                esac
}


function cleanlins()
{
cat ./${filename}
case $1 in
1)
flume-ng agent --conf ${FLUME_HOME}/conf --conf-file ./${filename} -Dflume.root.logger=info,console --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=5555
;;
*)
flume-ng agent --conf ${FLUME_HOME}/conf --conf-file ./${filename} -Dflume.root.logger=info,console --name a1
;;
esac
rm -rf ./${filename}
exit 99
}

function cleanlinshive(){
cat ./${filename}
nohup hive --service metastore > ~/log/metastore.log 1>&2 &
case $1 in
1)
flume-ng agent --conf ${FLUME_HOME}/conf --conf-file ./${filename} -Dflume.root.logger=info,console --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=5555
;;
*)
flume-ng agent --conf ${FLUME_HOME}/conf --conf-file ./${filename} -Dflume.root.logger=info,console --name a1
;;
esac
rm -rf ./${filename}
exit 100
}







function choicein(){
        case $1 in
        1)
        interceptors ${@:2}
        ;;
        *)
        selectchinnal ${@:2}
        ;;
        esac
}





function sourcechoice(){
    case $1 in
    "taildir")
        echo "a1.sources.r1.type = TAILDIR" >> ./${filename}
        echo "a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json" >> ./${filename}
        Fileordir ${@:2}
    ;;
    "exec")
        echo "a1.sources.r1.type = exec" >> ./${filename}
        echo "a1.sources.r1.command = tail -F $2" >> ./${filename}
        choicein ${@:3}
    ;;
    "spoo")
        echo "a1.sources.r1.type = spooldir" >> ./${filename}
        echo "a1.sources.r1.spoolDir = $2" >> ./${filename}
        echo "a1.sources.r1.fileHeader = true" >> ./${filename}
        choicein ${@:3}
    ;;
    "avro")
        echo "a1.sources.r1.type = avro" >> ./${filename}
        echo "a1.sources.r1.bind = $2" >> ./${filename}
        echo "a1.sources.r1.port = $3" >> ./${filename}
        choicein ${@:4}
    ;;
    "netcat")
        echo "a1.sources.r1.type = netcat" >> ./${filename}
        echo "a1.sources.r1.bind = $2" >> ./${filename}
        echo "a1.sources.r1.port = $3">> ./${filename}
        choicein ${@:4}
    ;;
    "kafka")
    echo "a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource" >>./${filename}
    echo "a1.sources.r1.kafka.bootstrap.servers = bigdata3:9092,bigdata4:9092,bigdata5:9092" >>./${filename}
    echo "a1.sources.r1.kafka.topics = $2" >>./${filename}
    echo "a1.sources.r1.kafka.consumer.group.id = $3" >>./${filename}
    echo "a1.sources.r1.batchSize = $4"  >>./${filename}
    echo "a1.sources.r1.batchDurationMillis = $5" >>./${filename}
    choicein ${@:6}
    ;;
    *)
        echo "无匹配的source"
        exit 1
    ;;
esac

}




function panduan(){
if [ ${flagSink} -ne 1 ] && [ ${flagChannel} == 1 ];then
for ((i=1;i<=${flagSink};i++))
do
echo "a1.sinks.k${i}.channel = c1" >> ./${filename}
done
echo "a1.sinkgroups.g1.processor.maxpenalty = $1" >> ./${filename}
case ${flagSink} in
2)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    sourcechoice ${@:4}
;;
3)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
     sourcechoice ${@:5}
;;
4)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
     sourcechoice ${@:6}
;;
5)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
     sourcechoice ${@:7}
;;
6)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
     sourcechoice ${@:8}
;;
7)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k7 = $8">> ./${filename}
     sourcechoice ${@:9}
;;
8)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k7 = $8">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k8 = $9">> ./${filename}
     sourcechoice ${@:10}
;;
*)
echo "超出限制8个"
exit 10000
;;
esac
fi


if [ ${flagSink} == 1 ] && [ ${flagChannel} -ne 1 ];then
for ((i=1;i<=${flagChannel};i++))
do
echo "a1.sinks.k1.channel = c${i}" >> ./${filename}
done
sourcechoice ${@:1}
fi


if [ ${flagSink} -ne 1 ] && [ ${flagChannel} -ne 1 ];then
for ((i=1;i<=${flagSink};i++))
do
echo "a1.sinks.k${i}.channel = c${i}" >> ./${filename}
done
echo "a1.sinkgroups.g1.processor.maxpenalty = $1" >> ./${filename}
case ${flagSink} in
2)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    sourcechoice ${@:4}
;;
3)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
     sourcechoice ${@:5}
;;
4)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
     sourcechoice ${@:6}
;;
5)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
     sourcechoice ${@:7}
;;
6)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
     sourcechoice ${@:8}
;;
7)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k7 = $8">> ./${filename}
     sourcechoice ${@:9}
;;
8)
    echo "a1.sinkgroups.g1.processor.priority.k1 = $2">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k2 = $3">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k3 = $4">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k4 = $5">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k5 = $6">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k6 = $7">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k7 = $8">> ./${filename}
    echo "a1.sinkgroups.g1.processor.priority.k8 = $9">> ./${filename}
     sourcechoice ${@:10}
;;
*)
echo "超出限制8个"
exit 10000
;;
esac
fi


if [ ${flagSink} == 1 ] && [ ${flagChannel} == 1 ];then
echo "a1.sinks.k1.channel = c1" >> ./${filename}
sourcechoice ${@:1}
fi
}

Sinkgt1 ${@:3}


```

# channel:file

关于Filechannel的例子官方是如下介绍的

```shell
a1.channels = c1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /mnt/flume/checkpoint
a1.channels.c1.dataDirs = /mnt/flume/data
```

于是我们可以写个简单的agent：taidir-file-logger

```shell
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/hadoop/data/flumepostion/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /home/hadoop/data/try.txt
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.fileHeader = true
a1.sources.ri.maxBatchCount = 1000


# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /home/hadoop/data/flumefilepostion
a1.channels.c1.dataDirs = /home/hadoop/data/flumedata

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

就可以了

关于flume的负载问题

我们可以设置Sink的负载就是当是多个数据输出的时候

可以设置负载策略，可以让它变成随机发送，或者是轮询发送等

均衡 ： load_balance ： 将数据分开，提供并行度的功能 减轻sink 的压力 如果突然输出的agent挂掉，数据都会发送到没有挂的Sink的agent上，但是会有个超时时间，才会进行上述所说

需要设置两个参数

容灾 ： sink出现问题的

```
	processor.backoff  true 
	processor.selector.maxTimeOut
```

设置多个Sink的agent

```shell
agent1：
agent1.sources = r1
agent1.sinks = k1 k2
agent1.channels = c1

agent1.sources.r1.type = netcat
agent1.sources.r1.bind = bigdata32
agent1.sources.r1.port = 1111

agent1.channels.c1.type = memory

#定义sink 2222
agent1.sinks.k1.type = avro
agent1.sinks.k1.hostname = bigdata32
agent1.sinks.k1.port = 2222

#定义sink 3333
agent1.sinks.k2.type = avro
agent1.sinks.k2.hostname = bigdata32
agent1.sinks.k2.port = 3333

#定义sink processers
agent1.sinkgroups = g1
agent1.sinkgroups.g1.sinks = k1 k2
agent1.sinkgroups.g1.processor.type = load_balance
agent1.sinkgroups.g1.processor.backoff = true
agent1.sinkgroups.g1.processor.selector = round_robin

agent1.sources.r1.channels = c1
agent1.sinks.k1.channel = c1
agent1.sinks.k2.channel = c1

agent2：2222端口
agent2.sources = r1
agent2.sinks = k1
agent2.channels = c1

agent2.sources.r1.type = avro
agent2.sources.r1.bind = bigdata32
agent2.sources.r1.port = 2222

agent2.channels.c1.type = memory
agent2.sinks.k1.type = logger

agent2.sources.r1.channels = c1
agent2.sinks.k1.channel = c1


agent3: 3333端口
agent3.sources = r1
agent3.sinks = k1
agent3.channels = c1

agent3.sources.r1.type = avro
agent3.sources.r1.bind = bigdata32
agent3.sources.r1.port = 3333

agent3.channels.c1.type = memory
agent3.sinks.k1.type = logger

agent3.sources.r1.channels = c1
agent3.sinks.k1.channel = c1

```

启动我们的agent

```shell
启动agent： 
	从后往前 启动 

flume-ng agent \
--name agent3 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/sink/agent3.conf \
-Dflume.root.logger=info,console


flume-ng agent \
--name agent2 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/sink/agent2.conf \
-Dflume.root.logger=info,console

flume-ng agent \
--name agent1 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/sink/agent1.conf \
-Dflume.root.logger=info,console

telnet bigdata32 1111

```

然后可以设置策略

`agent1.sinkgroups.g1.processor.selector = round_robin ： 这个是轮询策略，就是一个换一个的`

`agent1.sinkgroups.g1.processor.selector = random ： 这个是随机策略的`

负载，相当于是多几个备用通道，通过不同优先级进行设置通道

# Source的组件

拦截器 ： 数据转换 或者数据清洗的

channel选择器 ：把采集过来的数据发送到那一个channel里面

# SInk组件

Sink组件就是上述的Sink processers ：把采集的数据发送到哪一个sink上

# 需求

定义一个agent 端口 1111 采集数据

一个发送到hdfs上

另外一个发送到logger上

架构图就出来了 ：

source ->

* channel -> sink ->logger
* channel -> sink ->hdfs

## 思考如何配置channel选择器

官网如下

```shell
a1.sources = r1
a1.channels = c1 c2 c3
a1.sources.r1.selector.type = replicating
a1.sources.r1.channels = c1 c2 c3
a1.sources.r1.selector.optional = c3
```

我们自己写的如下：

```shell
agent1.sources = r1
agent1.sinks = k1 k2
agent1.channels = c1 c2

agent1.sources.r1.type = netcat
agent1.sources.r1.bind = bigdata3
agent1.sources.r1.port = 1111

#0 配置source channle
agent1.sources.r1.selector.type = replicating
agent1.sources.r1.channels = c1 c2

#1.配置两个channel
agent1.channels.c1.type = memory
agent1.channels.c2.type = memory

#定义sink hdfs
agent1.sinks.k1.type = hdfs
agent1.sinks.k1.hdfs.path = hdfs://bigdata3:9000/flume/channel_selector/
agent1.sinks.k1.hdfs.fileType=DataStream
agent1.sinks.k1.hdfs.writeFormat=Text
#文件前后缀
agent1.sinks.k1.hdfs.filePrefix=events
agent1.sinks.k1.hdfs.fileSuffix=.log
agent1.sinks.k1.hdfs.useLocalTimeStamp=true
#文件滚动
agent1.sinks.k1.hdfs.rollInterval=60
agent1.sinks.k1.hdfs.rollSize=134217728
agent1.sinks.k1.hdfs.rollCount=1000

#定义sink logger
agent1.sinks.k2.type = logger

#定义 连接
agent1.sources.r1.channels = c1 c2
agent1.sinks.k1.channel = c1
agent1.sinks.k2.channel = c2


```

作业 ：用三个agent 完成上面的事情

代码如下 ：

```shell
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

a1.sources.r1.type = netcat
a1.sources.r1.bind = bigdata3
a1.sources.r1.port = 1111

#0 配置source channle
a1.sources.r1.selector.type = replicating
a1.sources.r1.channels = c1 c2

#1.配置两个channel
a1.channels.c1.type = memory
a1.channels.c2.type = memory

#定义sink 2222
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = bigdata3
a1.sinks.k1.port = 2222

#定义sink 3333
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = bigdata3
a1.sinks.k2.port = 3333

#定义 连接
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2
```

然后我们书写端口的agent

2222端口

```shell
a1.sources = r1
a1.sinks = k1
a1.channels = c1
a1.sources.r1.type = avro
a1.sources.r1.bind = bigdata3 
a1.sources.r1.port = 2222 
a1.channels.c1.type = memory
a1.channels.c1.capacity = 15000
a1.channels.c1.transactionCapacity = 15000
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata3:9000/data
a1.sinks.k1.hdfs.filePrefix = com
a1.sinks.k1.hdfs.fileSuffix = .test
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollInterval = 21600
a1.sinks.k1.hdfs.rollCount = 1000
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.batchSize = 1200
a1.sinks.k1.hdfs.roundValue = 21
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sources.r1.channels = c1 
a1.sinks.k1.channel = c1


```

3333端口

```shell
a1.sources = r1
a1.sinks = k1
a1.channels = c1
a1.sources.r1.type = avro
a1.sources.r1.bind = bigdata3 
a1.sources.r1.port = 3333 
a1.channels.c1.type = memory
a1.channels.c1.capacity = 15000
a1.channels.c1.transactionCapacity = 15000
a1.sinks.k1.type = logger
a1.sources.r1.channels = c1 
a1.sinks.k1.channel = c1


```

flume里官方提供的拦截器很多，不过我们一般都不用，只用自己研发的

拦截器需求 ：

现在有三个数据源 ： 分别是1111端口，1112端口，1113端口，通过拦截器把数据分别发送过去

agent如下：

```shell
agent1:
agent1.sources = r1
agent1.sinks = k1
agent1.channels = c1

agent1.sources.r1.type = netcat
agent1.sources.r1.bind = bigdata32
agent1.sources.r1.port = 1111

#添加一个拦截器 =》 数据清洗 + event打标签
agent1.sources.r1.interceptors = i1
agent1.sources.r1.interceptors.i1.type = static
agent1.sources.r1.interceptors.i1.key = dl2262
agent1.sources.r1.interceptors.i1.value = boy
#0 配置source channle
agent1.sources.r1.channels = c1
#1.配置两个channel
agent1.channels.c1.type = memory
#定义sink 2222
agent1.sinks.k1.type = avro
agent1.sinks.k1.hostname = bigdata32
agent1.sinks.k1.port = 2222
#定义 连接
agent1.sources.r1.channels = c1
agent1.sinks.k1.channel = c1


agent2:
agent2.sources = r1
agent2.sinks = k1
agent2.channels = c1

agent2.sources.r1.type = netcat
agent2.sources.r1.bind = bigdata32
agent2.sources.r1.port = 1112

#添加一个拦截器 =》 数据清洗 + event打标签
agent2.sources.r1.interceptors = i1
agent2.sources.r1.interceptors.i1.type = static
agent2.sources.r1.interceptors.i1.key = dl2262
agent2.sources.r1.interceptors.i1.value = girl
#0 配置source channle
agent2.sources.r1.channels = c1
#1.配置两个channel
agent2.channels.c1.type = memory
#定义sink 2222
agent2.sinks.k1.type = avro
agent2.sinks.k1.hostname = bigdata32
agent2.sinks.k1.port = 2222
#定义 连接
agent2.sources.r1.channels = c1
agent2.sinks.k1.channel = c1

agent3:
agent3.sources = r1
agent3.sinks = k1
agent3.channels = c1

agent3.sources.r1.type = netcat
agent3.sources.r1.bind = bigdata32
agent3.sources.r1.port = 1113

#添加一个拦截器 =》 数据清洗 + event打标签
agent3.sources.r1.interceptors = i1
agent3.sources.r1.interceptors.i1.type = static
agent3.sources.r1.interceptors.i1.key = dl2262
agent3.sources.r1.interceptors.i1.value = tea
#0 配置source channle
agent3.sources.r1.channels = c1
#1.配置两个channel
agent3.channels.c1.type = memory
#定义sink 2222
agent3.sinks.k1.type = avro
agent3.sinks.k1.hostname = bigdata32
agent3.sinks.k1.port = 2222
#定义 连接
agent3.sources.r1.channels = c1
agent3.sinks.k1.channel = c1

agent4:

agent4.sources = r1
agent4.sinks = k1 k2 k3
agent4.channels = c1 c2 c3

agent4.sources.r1.type = avro
agent4.sources.r1.bind = bigdata32
agent4.sources.r1.port = 2222


#0 配置source channle
agent4.sources.r1.selector.type = multiplexing
agent4.sources.r1.selector.header = dl2262
agent4.sources.r1.selector.mapping.boy = c1
agent4.sources.r1.selector.mapping.girl = c2
agent4.sources.r1.selector.default = c3
agent4.sources.r1.channels = c1 c2 c3
#1.配置两个channel
agent4.channels.c1.type = memory
agent4.channels.c2.type = memory
agent4.channels.c3.type = memory
#定义sink logger
agent4.sinks.k1.type =logger
agent4.sinks.k2.type =logger
agent4.sinks.k3.type =logger
#定义 连接
agent4.sources.r1.channels = c1 c2 c3
agent4.sinks.k1.channel = c1
agent4.sinks.k2.channel = c2
agent4.sinks.k3.channel = c3

启动：
flume-ng agent \
--name agent4 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/many2one/agent4.conf \
-Dflume.root.logger=info,console

flume-ng agent \
--name agent3 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/many2one/agent3.conf \
-Dflume.root.logger=info,console

flume-ng agent \
--name agent2 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/many2one/agent2.conf \
-Dflume.root.logger=info,console

flume-ng agent \
--name agent1 \
--conf ${FLUME_HOME}/conf \
--conf-file /home/hadoop/project/flume/many2one/agent1.conf \
-Dflume.root.logger=info,console

telnet bigdata32 1111
telnet bigdata32 1112
telnet bigdata32 1113


```

# 细节

## channel

默认的容量 ： 就是存储的容量capacity

事务容量 ： 就是发生错误的时候可以回撤的条数，包括写进去的时候的事务 transactionCapacity

## 监控

source ：

channel ：

sink ：当sink突然采集的数据变少，可能是上述两个组件出问题了

### 监控手段

flume的ganglia框架，监控

agent启动一些参数获取这三个组件的相关指标

建议用第二个：因为简单，因为第一个要安装ganglia

对于第二种是获取json数据进而获取的，通过采集http的接口数据，如何通过dataease或者superset进行可视化，或者给前端人员

### 配置

在命令行加上命令就可以啦

```
flume-ng agent -c . -f conf/exec-tail.conf -n a1 -Dflume.root.logger=INFO,console -Dflume.monitoring.type=http -Dflume.monitoring.port=1234
```
