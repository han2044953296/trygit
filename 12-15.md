---
title: zookeeper
date: 12-15 8.20 
categories: 日志
comments: "true"
tag: zookeeper
---
# hive的抓取策略

什么是hive的抓取策略呢，举个简单的例子,当我们用 `select * from xxx`的时候它不会走mr阶段，会直接出来，可是有时候会走mr这个是由什么控制的？就是由我们的hive的抓取策略决定的

我们还可以在配置文件中进行更改，把不要mr的代码变多

`hive/conf/hive-default.xml.template -》 2637行，修改hive.fetch.task.conversion为more；`

就相当于把全局查找，字段查找，filter查找，limit查找等都不走MR，直接Fetch

```
  <property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
    <description>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have
      any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
    </description>
  </property>
```

然后还可以设置 ：

 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。

set hive.exec.mode.local.auto=true; //开启本地mr

设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M

set hive.exec.mode.local.auto.inputbytes.max=51234560;

设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4

set hive.exec.mode.local.auto.input.files.max=10;

调整hive里的切片大小 ：

mapreduce.input.fileinputformat.split.maxsize

mapreduce.input.fileinputformat.split.minsize

maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。
minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。

# sql调优（speed）

## 小表join大表

当小表Join大表时，如果不指定MapJoin，那么hive解析器会将join操作转换为Common Join操作，在Reduce端完成join，容易发生[数据倾斜](https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C&spm=1001.2101.3001.7020)。开启MapJoin后可以将小表全部加载到内存中，在map端进行join，避免reducer处理。

优点 ：

```
（1）没有reducer处理，就不会产生数据倾斜；

（2）没有Map -> Reduce中间的shuffle操作，避免了IO

1. 开启MapJoin设置
（1）设置自动选择MapJoin，默认是true
set hive.auto.convert.join = true;
 
（2）设置小表阈值，默认是25M
set hive.mapjoin.smalltable.filesize=25000000;
 
2. 再大表join小表，与小表join大表，执行速率几乎相等

Hive中mapjoin的原理：

map-side join：

小表数据映射成一张hashtable，再上传到分布式节点的内存中；
大表进行分片，每个节点一部分数据，大表数据文件作为map端输入，对map()函数每一对输入的kv都与已加载到内存中的小表数据连接，
把连接结果按key输出，有多少个map task，产生多少个结果文件；
由于join操作在map task中完成，所以无需启动reduce task，没有shuffle操作和reduce端，避免io和数据倾斜

reduce-side join：

map端把结果按key输出，并在value中标记出数据来源于table1还是table2
因为在shuffle阶段已经按key分组，reduce阶段会判断每个value来自哪张表，然后两表相同key的记录连接
join操作在reduce task中完成

缺点1：在map阶段没有对数据瘦身，shuffle的网络传输和排序性能很低
缺点2：reduce对2个集合做城际计算，很耗内存，容易造成oom
```

## 大表join大表

有时join超时是因为某些key对应的数据太多了。由于相同的key对应的数据都会发送到相同的reducer上，如果出现数据倾斜可能导致内存不足。

常见对于key对应字段为空，可以采取的优化手段包括空值key过滤和空值key转换

首先我们要进行配置，开启历史服务器

```
1. 首先配置mapred-site.xml
<property>
<name>mapreduce.jobhistory.address</name>
<value>hadoop100:10020</value>
</property>
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop100:19888</value>
</property>
 
2. 在shell端启动历史服务器
sbin/mr-jobhistory-daemon.sh start historyserver
 
3. 查看jobhistory端口
http://hadoop100:19888/jobhistory
```

空key过滤

```
不过滤空id列
insert overwrite table A 
select b.* from B b left join C c on b.id = c.id;
 
过滤掉空id列
insert overwrite table A 
select b.* from (select * from B where id is not null) b left join C c on b.id = c.id;
过滤掉空id列后，耗费时间会降低很多。
```

**或者把它变成其他的有值的**

有时虽然某个key对应的null很多，但null并不是异常数据，不能过滤掉，必须包含在join的结果中，这样就可以考虑把表中key为null的字段赋一个随机值，使得数据随机均匀分到不同的reducer上。

```
首先设置reduce个数
set mapreduce.job.reduces = 5;
 
然后join两张表，随机设置null值
insert overwrite table A
select b.* from B b full join C c on
case when b.id is null then concat ('hive',rand()) else b.id end = c.id;
```

原理：当表b的字段id为null时，如果null过多所有null对应同一个key即id，都挤到一个reduce上，通过优化将表b的key=id换成key=hive随机数，这样null分配到不同的key上，避免数据倾斜。

case when A then B else C end语法：

当b表的字段id为null时，对id取值为拼接字符串（hive+随机数），否则依然取b.id；然后on条件为：hive随机数=c.id或者b.id=c.id。

## group by

对于group by聚合，默认情况下，Map阶段同一Key的数据发给一个reduce，若某个key的数据量太大，可能会造成数据倾斜。

如果在Map端就直接完成部分聚合，最后在Reduce端得出最终结果，就可以避免数据倾斜。

需要在Map端进行聚合参数设置：

```
1. 是否在Map端进行聚合，默认是true
set hive.map.aggr = true
 
2. 在Map端进行聚合操作的条目数，默认10w条
set hive.groupby.mapaggr.checkinterval = 100000
 
3. 有数据倾斜时进行负载均衡，默认false
set hive.groupby.skewindata = true
```

原理是当set hive.groupby.skewindata = true后，会生成两个MR Job，启两个任务。

job1将group by的key，相同的key可能随机分发到不同的Reduce中，然后Reduce依据key对数据进行聚合，此时每一个Reduce中每个数据的key不一定相同，但是经过这一步聚合后，大大减少了数据量。

job2是真正意义上MR的底层实现，将相同的key分配到同一个reduce中，进行key的聚合操作。

第一步job1实现负载均衡，第二步job2实现聚合需求。

如果skewindata参数=false，也就是默认情况下，只会进行job2操作，进行一次MapReduce。

## count

用 count(distinct key)处理数据

当数据量大的时候，由于count（distinct key）去重聚合是全聚合操作，即便是设定了reduce tasks的个数，例如set mapred.reduce.tasks=100；hive也只会启动一个reducer（order by也是这个情况），这就造成一个reducer处理的数据量太大，导致整个Job完成的很慢。

可以将count（distinct key）的方式，改为先group by 再count的方式，也就是将distinct换成group by。

这种优化可以增加reducer的个数，虽然会用多个Job完成，但是适合处理数据量大的情况。

```
原始：  select count(distinct id) from table;
 
优化后：select count(id) from (select id from table group by id) a;
```

## 行列过滤（分区过滤，先where再join嵌套子查询）——谓词下推

列处理：在select中，只拿需要的列，如果有分区，尽量使用分区字段查询（分区过滤），避免使用select *全表扫描select key from tablename where 分区字段 = '~'

行处理：两表连接时，对一个表的数据先where过滤，再join（如果先join再过滤，过滤的数据量会很大），即嵌套子查询

在Hive中，可以通过将参数hive.optimize.ppd设置为true，启用谓词下推。与它对应的逻辑优化器是PredicatePushDown。该优化器就是将OperatorTree中的FilterOperator向上提

Hive中与列裁剪和分区裁剪优化相关的配置参数分别为：hive.optimize.cp和hive.optimize.pruner，默认都是true。

```
eg：join两张表，A表joinB表，要求是把B表中id<10的过滤掉后，只查询联结表的id列
原始： select a.id from A a join B b on a.id=b.id where b.id<10
 
优化后：select a.id from A a join (select id from B where id<10) b on a.id=b.id
```

## 动态分区

分区列是表的一个伪列，它对应HDFS的一个分区文件夹，并且分区列存在于表的最后。

如果不设定动态分区，往分区表中导入数据的方式如下：

```
1. 创建分区表
create table stu_par(id int,name string)
partitioned by (month string)
row format delimited fields terminated by '\t';
 
2. 往分区中导入数据
load data local inpath '/opt/module/datas/student.txt' into table stu_par partition(month='10');
```

如果设定动态分区，导入数据就不再需要指定分区字段了

```
1. 设置为非严格模式（默认strict，表示必须指定至少一个分区为静态分区；nonstrict表示允许所有分区可使用动态分区）

hive.exec.dynamic.partition.mode=nonstrict

2. 默认配置

（1）开启动态分区功能（默认true，开启）

hive.exec.dynamic.partition=true

（2）在所有执行MR节点上，最大一共可以创建多少个动态分区，默认1000个

hive.exec.max.dynamic.partitions=1000

（3）在每个执行MR的节点上，最大可以创建多少个分区，默认值100

eg：若源数据包含一年的数据，按照天数分区，day字段应该有365个值，这里的默认值100就应该修改为大于365的数。

hive.exec.max.dynamic.partitions.pernode=100

（4）整个MR job中，最大可以创建多少个HDFS文件，默认100000

hive.exec.max.created.files=100000
```

操作 ：

```
1. 创建分区表
create table dept_par(id string,name string) partitioned by (location int)
row format delimited fields terminated by '\t';
 
2. 设置动态分区非严格模式
set hive.exec.dynamic.partition.mode = nonstrict;
 
3. 查看dept表
+--------------+-------------+-----------+--+
| dept.deptno  | dept.dname  | dept.loc  |
+--------------+-------------+-----------+--+
| 10           | ACCOUNTING  | 1700      |
| 20           | RESEARCH    | 1800      |
| 30           | SALES       | 1900      |
| 40           | OPERATIONS  | 1700      |
+--------------+-------------+-----------+--+
 
4. 分区表动态导入数据（并未指定分区字段的值）
insert into table dept_par partition(location) select deptno,dname,loc from dept;
...
	Loading partition {location=1900}
	Loading partition {location=1800}
	Loading partition {location=1700}
...
 
5. 查看分区表的分区情况
hive (hive_db1)> show partitions dept_par;
OK
partition
location=1700
location=1800
location=1900
 
6. 查询分区表
select * from dept_par where location='1700';
+--------------+----------------+--------------------+--+
| dept_par.id  | dept_par.name  | dept_par.location  |
+--------------+----------------+--------------------+--+
| 10           | ACCOUNTING     | 1700               |
| 40           | OPERATIONS     | 1700               |
+--------------+----------------+--------------------+--+
 
select * from dept_par where location='1800';
+--------------+----------------+--------------------+--+
| dept_par.id  | dept_par.name  | dept_par.location  |
+--------------+----------------+--------------------+--+
| 20           | RESEARCH       | 1800               |
+--------------+----------------+--------------------+--+
 
select * from dept_par where location='1900';
+--------------+----------------+--------------------+--+
| dept_par.id  | dept_par.name  | dept_par.location  |
+--------------+----------------+--------------------+--+
| 30           | SALES          | 1900               |
+--------------+----------------+--------------------+--+
```

**原理：**

将select的最后一列，认为是分区列（因为分区列在表的最后），将最后一列字段值相同的行，导入同一个分区中；

好处是避免了指定分区字段的值，直接动态的将值相同的行导入同一个分区中，加大效率。

ps：如果把deptno放在select最后一列，那么会生成四个分区

## 分桶

适合对数据进行抽样查询的情况，clustered by 字段 into x buckets，将表数据以（字段hash值%分桶数）按照取模结果，对数据进行分桶，也就是随机分布成几块。

ps：分区列是伪列，需要指明字段类型；分桶列是实际列，不需要指明字段类型。

分桶前，需要设置属性set hive.enforce.bucketing = true

## 分区表

主要目的是提高hive查询效率，避免全表查询 select * from tablename where 分区字段 = '~'

# zookeeper

生产上namenode是两个是没有snn的，同样yarn的老大也是两个，是忘了防止一个老大突然挂掉，然后让下一个上位

控制整个老大的更换的就是zookeeper

hadoop ： 请求服务是不用关注所有节点的ip的，我们只用关注namenode的，但是当是两个namenode的时候（生产上），这个时候就会把两个namenode放在一起，形成一个namespace-》zookeeper实现

部署 ：

* 单点
* 分布式
* 要求 ：部署的机器数量有要求 ，2n + 1 台机器

主从架构 ：如果老大挂了，后面的老二就会上位

zookeeper版本有很多：但是对于我们现在能用就行 ： 3.8.0

部署 ：

有前置要求：具体查看官网 ：对系统有要求 ： jdk 1.8orlater

先把包解压 -》 软连接 -》 zookeeper -》 环境变量 -》 *cmd的是windows上运行的 -》 进入 conf目录 zoo.cfg（改名之后）

* 单点：直接dataDir:改成自己的 - 》 启动
* 分布 ：dataDir ,三台机器的id，以及机器id的端口号

```
bigdata3
dataDir=xxxxx
server.1=bigdata3:2888:3888
server.2=bigdata4:2888:3888
server.3=bigdata5:2888:3888

myid : 指定机器号 ： 在zookeeper的数据文件夹下创建一个myid，id号就是server点的数字
```

然后启动zk ： 通过zkServer.sh start

分布式要每一台机器上都整

分布式：查看status之后会出现 follower 和leader 就是副手和老大

然后创建一个群起脚本

```
if [ $# -lt 1 ];then
echo "start|stop|status"
exit 1
fi



function startzk(){
 nohup zkServer.sh start  > ~/log/zk.log 1>&2 &
 ssh bigdata4 "zkServer.sh start"
 ssh bigdata5 "zkServer.sh start"
}

function stopzk(){
 nohup zkServer.sh stop  > ~/log/zk.log 1>&2 &
 ssh bigdata4 "zkServer.sh stop"
 ssh bigdata5 "zkServer.sh stop"
}

function statuszk(){
echo "-----------------bigdata3------------------"
jps | grep QuorumPeerMain
echo "-----------------bigdata4------------------"
ssh bigdata4 "jps | grep QuorumPeerMain"
echo "-----------------bigdata5------------------"
ssh bigdata5 "jps | grep QuorumPeerMain"
}

case $1 in
    "start")
    startzk
    ;;
    "stop")
    stopzk
    ;;
    "status")
    statuszk
    ;;
    *)
    echo "error input"
    ;;
esac


```

## zk是干什么的？

zk可以做监控，可以检查到机器是不是还活着，

管理配置信息的

## zk的数据模型

层级式的结构，一个树形的结构，和linux差不多

节点/目录

* 节点就是目录
* 节点保存数据的内容
* zk里的所有的目录都可以叫节点

通过命令 ：zkCli.sh 访问zk

然后我们可以对其数据进行操作 ：

* 查看节点 ：ls name
* 查看节点内容 ： get name
* 节点

  * 临时节点 ：只在当前session有效 ，关闭当前会话，则就失效，不可以存放子节点
  * 永久节点 ：永久存在的，可以存放子节点
* zk 每个节点都有自己的id，节点号，不会重复 ， 数据是存放在节点上的
* 数据不是很大的数据
* 仅仅是比较小的数据
* 如果存放的数据发生变更，数据版本号也会发生变更
* 创建节点 ： create : `create /dl2262 zihang`
* stat path :获取当前节点状态 ： `stat /dl2262`
* ```
  cZxid = 0x300000002 id
  ctime = Thu Dec 15 14:52:29 CST 2022
  mZxid = 0x300000002
  mtime = Thu Dec 15 14:52:29 CST 2022
  pZxid = 0x300000002
  cversion = 0 
  dataVersion = 0 数据版本
  aclVersion = 0 权限的版本
  ephemeralOwner = 0x0 是不是临时节点 0x0 不是临时节点 其余就是临时节点
  dataLength = 6 数据的长度：zihang就是6个字节
  numChildren = 0 下面的子节点
  ```
* create -e ：创建临时节点 ：`create -e /dl2262 zuan` 临时节点也有自己的过期时间，时间一过就会自动删除（重新启动客户端的时候）
* 顺序节点 ： create -s  默认会给节点后面加一个自增的序列： `create -s /dl2262/ziyuan`
* 不可以创建深层文件夹的方式命令，如果使用zk的api则可以，或者java代码的方式
* 修改数据内容的 ： set :   `set [-s] [-v version] path data->set /dl2262 xxxx  `

  * 数据版本 ： 如果数据版本不对应，就会报错的
* 删除节点 ：delete ： `delete [-v version] path` 或者deleteall 删除其及以下所有数据
* 如果版本不对还是要出错的
* zk自带的监控 ， 监听器
* 命令中有-w的就是可以监听的
* 针对每一个节点都可以执行的，每个节点都有一个监听器，当每个节点发生变化，就会触发watch事件
* 但是zk的shell命令每次只能监听一次 ， zk的原生的api不行，但是curator 的可以一直监听
* 命令 stat -w nodenpath 就可以监听，或者get -w nodepath等
* 但是命令行的方式只能触发一次

zk的四字命令

* zk要对外进行服务的
* 通过器对外的端口对其进行监控这个是对zk进行监控的
* 使用前提 ： 要编辑配置文件 ：在zoo.cng这个配置文件里添加 `4lw.commands.whitelist=*`
* stat ：可以列出自己的服务端和客户端的一些详细信息
* 使用方式 ： 直接在linux命令行操作 ， `echo stat | telnet localhost 2181`
* ruok : are you ok ? 就是检测服务的功能启动了吗
* dump ：列出最近的会话和临时节点的详细信息
* conf ： 打印机器的配置

或者可以使用Prometheus框架监控
