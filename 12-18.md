---
title: datax
date: 12-18 8.40
categories: 杂货技术栈
comments: "true"
tag: datax
---
# datax

## 简介

[DataX](https://so.csdn.net/so/search?q=DataX&spm=1001.2101.3001.7020) 是阿里开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。

## 理念

DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通

## 框架设计

DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。

Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。

Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端

Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。

DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下，详情请点击：[DataX数据源参考指南](https://github.com/alibaba/DataX/wiki/DataX-all-data-channels)

| 类型               | 数据源                          | Reader(读) | Writer(写) |                                                                                                                    文档                                                                                                                    |
| ------------------ | ------------------------------- | :--------: | :--------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| RDBMS 关系型数据库 | MySQL                           |     √     |     √     |                                       [读](https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md)                                       |
|                    | Oracle                          |     √     |     √     |                                     [读](https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md)                                     |
|                    | OceanBase                       |     √     |     √     | [读](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) 、[写](https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase) |
|                    | SQLServer                       |     √     |     √     |                               [读](https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md)                               |
|                    | PostgreSQL                      |     √     |     √     |                             [读](https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md)                             |
|                    | DRDS                            |     √     |     √     |                                         [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md)                                         |
|                    | Kingbase                        |     √     |     √     |                                         [读](https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md)                                         |
|                    | 通用RDBMS(支持所有关系型数据库) |     √     |     √     |                                       [读](https://github.com/alibaba/DataX/blob/master/rdbmsreader/doc/rdbmsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/rdbmswriter/doc/rdbmswriter.md)                                       |
| 阿里云数仓数据存储 | ODPS                            |     √     |     √     |                                         [读](https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/odpswriter/doc/odpswriter.md)                                         |
|                    | ADS                             |            |     √     |                                                                                 [写](https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md)                                                                                 |
|                    | OSS                             |     √     |     √     |                                           [读](https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md)                                           |
|                    | OCS                             |            |     √     |                                                                                 [写](https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md)                                                                                 |
|                    | Hologres                        |            |     √     |                                                                        [写](https://github.com/alibaba/DataX/blob/master/hologresjdbcwriter/doc/hologresjdbcwriter.md)                                                                        |
|                    | AnalyticDB For PostgreSQL       |            |     √     |                                                                                                                     写                                                                                                                     |
| 阿里云中间件       | datahub                         |     √     |     √     |                                                                                                                   读 、写                                                                                                                   |
|                    | SLS                             |     √     |     √     |                                                                                                                   读 、写                                                                                                                   |
| 阿里云图数据库     | GDB                             |     √     |     √     |                                           [读](https://github.com/alibaba/DataX/blob/master/gdbreader/doc/gdbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/gdbwriter/doc/gdbwriter.md)                                           |
| NoSQL数据存储      | OTS                             |     √     |     √     |                                           [读](https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md)                                           |
|                    | Hbase0.94                       |     √     |     √     |                               [读](https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md)                               |
|                    | Hbase1.1                        |     √     |     √     |                                 [读](https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md)                                 |
|                    | Phoenix4.x                      |     √     |     √     |                           [读](https://github.com/alibaba/DataX/blob/master/hbase11xsqlreader/doc/hbase11xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase11xsqlwriter/doc/hbase11xsqlwriter.md)                           |
|                    | Phoenix5.x                      |     √     |     √     |                           [读](https://github.com/alibaba/DataX/blob/master/hbase20xsqlreader/doc/hbase20xsqlreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hbase20xsqlwriter/doc/hbase20xsqlwriter.md)                           |
|                    | MongoDB                         |     √     |     √     |                                   [读](https://github.com/alibaba/DataX/blob/master/mongodbreader/doc/mongodbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/mongodbwriter/doc/mongodbwriter.md)                                   |
|                    | Cassandra                       |     √     |     √     |                               [读](https://github.com/alibaba/DataX/blob/master/cassandrareader/doc/cassandrareader.md) 、[写](https://github.com/alibaba/DataX/blob/master/cassandrawriter/doc/cassandrawriter.md)                               |
| 数仓数据存储       | StarRocks                       |     √     |     √     |                                                                         读 、[写](https://github.com/alibaba/DataX/blob/master/starrockswriter/doc/starrockswriter.md)                                                                         |
|                    | ApacheDoris                     |            |     √     |                                                                               [写](https://github.com/alibaba/DataX/blob/master/doriswriter/doc/doriswriter.md)                                                                               |
|                    | ClickHouse                      |            |     √     |                                                                                                                     写                                                                                                                     |
|                    | Hive                            |     √     |     √     |                                         [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                         |
|                    | kudu                            |            |     √     |                                                                                [写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                                                                |
| 无结构化数据存储   | TxtFile                         |     √     |     √     |                                   [读](https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md) 、[写](https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md)                                   |
|                    | FTP                             |     √     |     √     |                                           [读](https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md)                                           |
|                    | HDFS                            |     √     |     √     |                                         [读](https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md)                                         |
|                    | Elasticsearch                   |            |     √     |                                                                       [写](https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md)                                                                       |
| 时间序列数据库     | OpenTSDB                        |     √     |            |                                                                            [读](https://github.com/alibaba/DataX/blob/master/opentsdbreader/doc/opentsdbreader.md)                                                                            |
|                    | TSDB                            |     √     |     √     |                                       [读](https://github.com/alibaba/DataX/blob/master/tsdbreader/doc/tsdbreader.md) 、[写](https://github.com/alibaba/DataX/blob/master/tsdbwriter/doc/tsdbhttpwriter.md)                                       |
|                    | TDengine                        |     √     |     √     |                              [读](https://github.com/alibaba/DataX/blob/master/tdenginereader/doc/tdenginereader-CN.md) 、[写](https://github.com/alibaba/DataX/blob/master/tdenginewriter/doc/tdenginewriter-CN.md)                              |

## 部署需求

* Linux
* [JDK(1.8以上，推荐1.8)](http://www.oracle.com/technetwork/cn/java/javase/downloads/index.html)
* [Python(2或3都可以)](https://www.python.org/downloads/)
* [Apache Maven 3.x](https://maven.apache.org/download.cgi) (Compile DataX)

## 部署步骤以及问题

解压datax的安装包 -》 制作软连接 -》 修改环境变量并source -》 执行自检脚本 `datax.py ${DATAX_HOME}/job/job.json`

如果出现

```

DataX (DATAX-OPENSOURCE-3.0), From Alibaba !
Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.


2022-12-18 10:01:32.806 [main] INFO  VMInfo - VMInfo# operatingSystem class => sun.management.OperatingSystemImpl
2022-12-18 10:01:32.825 [main] INFO  Engine - the machine info  => 

	osInfo:	Oracle Corporation 1.8 25.45-b02
	jvmInfo:	Linux amd64 3.10.0-1127.el7.x86_64
	cpu num:	2

	totalPhysicalMemory:	-0.00G
	freePhysicalMemory:	-0.00G
	maxFileDescriptorCount:	-1
	currentOpenFileDescriptorCount:	-1

	GC Names	[PS MarkSweep, PS Scavenge]

	MEMORY_NAME                    | allocation_size                | init_size                
	PS Eden Space                  | 256.00MB                       | 256.00MB                 
	Code Cache                     | 240.00MB                       | 2.44MB                   
	Compressed Class Space         | 1,024.00MB                     | 0.00MB                   
	PS Survivor Space              | 42.50MB                        | 42.50MB                  
	PS Old Gen                     | 683.00MB                       | 683.00MB                 
	Metaspace                      | -0.00MB                        | 0.00MB                   


2022-12-18 10:01:32.848 [main] INFO  Engine - 
{
	"content":[
		{
			"reader":{
				"name":"streamreader",
				"parameter":{
					"column":[
						{
							"type":"string",
							"value":"DataX"
						},
						{
							"type":"long",
							"value":19890604
						},
						{
							"type":"date",
							"value":"1989-06-04 00:00:00"
						},
						{
							"type":"bool",
							"value":true
						},
						{
							"type":"bytes",
							"value":"test"
						}
					],
					"sliceRecordCount":100000
				}
			},
			"writer":{
				"name":"streamwriter",
				"parameter":{
					"encoding":"UTF-8",
					"print":false
				}
			}
		}
	],
	"setting":{
		"errorLimit":{
			"percentage":0.02,
			"record":0
		},
		"speed":{
			"byte":10485760
		}
	}
}

2022-12-18 10:01:32.889 [main] WARN  Engine - prioriy set to 0, because NumberFormatException, the value is: null
2022-12-18 10:01:32.890 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0
2022-12-18 10:01:32.890 [main] INFO  JobContainer - DataX jobContainer starts job.
2022-12-18 10:01:32.891 [main] INFO  JobContainer - Set jobId = 0
2022-12-18 10:01:32.916 [job-0] INFO  JobContainer - jobContainer starts to do prepare ...
2022-12-18 10:01:32.917 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] do prepare work .
2022-12-18 10:01:32.917 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] do prepare work .
2022-12-18 10:01:32.917 [job-0] INFO  JobContainer - jobContainer starts to do split ...
2022-12-18 10:01:32.918 [job-0] INFO  JobContainer - Job set Max-Byte-Speed to 10485760 bytes.
2022-12-18 10:01:32.919 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] splits to [1] tasks.
2022-12-18 10:01:32.919 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] splits to [1] tasks.
2022-12-18 10:01:32.941 [job-0] INFO  JobContainer - jobContainer starts to do schedule ...
2022-12-18 10:01:32.945 [job-0] INFO  JobContainer - Scheduler starts [1] taskGroups.
2022-12-18 10:01:32.947 [job-0] INFO  JobContainer - Running by standalone Mode.
2022-12-18 10:01:32.967 [taskGroup-0] INFO  TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks.
2022-12-18 10:01:32.978 [taskGroup-0] INFO  Channel - Channel set byte_speed_limit to -1, No bps activated.
2022-12-18 10:01:32.978 [taskGroup-0] INFO  Channel - Channel set record_speed_limit to -1, No tps activated.
2022-12-18 10:01:32.999 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started
2022-12-18 10:01:33.100 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[116]ms
2022-12-18 10:01:33.101 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] completed it's tasks.
2022-12-18 10:01:43.004 [job-0] INFO  StandAloneJobContainerCommunicator - Total 100000 records, 2600000 bytes | Speed 253.91KB/s, 10000 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.029s |  All Task WaitReaderTime 0.029s | Percentage 100.00%
2022-12-18 10:01:43.005 [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.
2022-12-18 10:01:43.005 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] do post work.
2022-12-18 10:01:43.005 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] do post work.
2022-12-18 10:01:43.005 [job-0] INFO  JobContainer - DataX jobId [0] completed successfully.
2022-12-18 10:01:43.006 [job-0] INFO  HookInvoker - No hook invoked, because base dir not exists or is a file: /home/hadoop/app/datax/hook
2022-12-18 10:01:43.007 [job-0] INFO  JobContainer - 
	 [total cpu info] => 
		averageCpu                     | maxDeltaCpu                    | minDeltaCpu              
		-1.00%                         | -1.00%                         | -1.00%
                  

	 [total gc info] => 
		 NAME                 | totalGCCount       | maxDeltaGCCount    | minDeltaGCCount    | totalGCTime        | maxDeltaGCTime     | minDeltaGCTime   
		 PS MarkSweep         | 0                  | 0                  | 0                  | 0.000s             | 0.000s             | 0.000s       
		 PS Scavenge          | 0                  | 0                  | 0                  | 0.000s             | 0.000s             | 0.000s       

2022-12-18 10:01:43.007 [job-0] INFO  JobContainer - PerfTrace not enable!
2022-12-18 10:01:43.007 [job-0] INFO  StandAloneJobContainerCommunicator - Total 100000 records, 2600000 bytes | Speed 253.91KB/s, 10000 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.029s |  All Task WaitReaderTime 0.029s | Percentage 100.00%
2022-12-18 10:01:43.011 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2022-12-18 10:01:32
任务结束时刻                    : 2022-12-18 10:01:43
任务总计耗时                    :                 10s
任务平均流量                    :          253.91KB/s
记录写入速度                    :          10000rec/s
读出记录总数                    :              100000
读写失败总数                    :                   0

```

则部署成功

## 遇见的问题

自建的时候出现

```
Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.


2022-12-18 09:56:52.120 [main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/app/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件. 
2022-12-18 09:56:53.124 [main] ERROR Engine - 

经DataX智能分析,该任务最可能的错误原因是:
com.alibaba.datax.common.exception.DataXException: Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/app/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件.
	at com.alibaba.datax.common.exception.DataXException.asDataXException(DataXException.java:26)
	at com.alibaba.datax.common.util.Configuration.from(Configuration.java:95)
	at com.alibaba.datax.core.util.ConfigParser.parseOnePluginConfig(ConfigParser.java:153)
	at com.alibaba.datax.core.util.ConfigParser.parsePluginConfig(ConfigParser.java:125)
	at com.alibaba.datax.core.util.ConfigParser.parse(ConfigParser.java:63)
	at com.alibaba.datax.core.Engine.entry(Engine.java:137)
	at com.alibaba.datax.core.Engine.main(Engine.java:204)

```

### 解决方法

执行

```
find ${DATAX_HOME}/plugin/reader/ -type f -name "._*er" | xargs rm -rf
find ${DATAX_HOME}/plugin/writer/ -type f -name "._*er" | xargs rm -rf
```

然后重新自检就成功了

# 安装datax-web工具

简介 ： DataX Web是在DataX之上开发的分布式数据同步工具，提供简单易用的操作界面。

相当于多了一个图形化界面

## 部署

解压压缩包-> 制作软连接 -> 编辑并导入环境变量

* -> 修改modules下的datax-admin下的conf下的bootstrap.properties
* DB_HOST ： mysql所在的ip
* DB_PORT ： mysql的端口
* DB_USERNAME ： mysql的用户名
* DB_PASSWORD ： mysql的用户名的密码
* DB_DATABASE ： datax的配置文件所在的mysql的数据库
* -> 修改datax-executor下的bin下的env.properties的PYTOHON_PATH的配置地址，需与我们安装的datax目录一致
* 比如 ： `PYTHON_PATH=/home/hadoop/app/datax/bin/datax.py`
* -> 修改/datax-executor/conf下的application.yml，把其中的admin下的addresses: http://ip:${datax.admin.port} ：ip改成自己机器的
* 然后进入到我们的最外层的bin目录下，执行 `start-all.sh`ps : 这里的start-all.sh和hadoop的启动脚本冲突，我们可以把它改名就好了
* 执行 `mv ./start-all.sh ./startdatax_web-all.sh停止脚本同理`
* 然后启动脚本执行 ： ` startdatax_web-all.sh`
* 然后我们访问 `http://服务器IP:9527/index.html`
* 初始账号密码 admin  123456

使用方法和xxl非常类似

# Prometheus

简介 ： Prometheus(由go语言(golang)开发)是一套开源的监控&报警&时间序列数据库的组合。适合监控容器平台。因为kubernetes(俗称k8s)的流行带动了prometheus的发展

Prometheus的主要特征有:

1. 多维度数据模型
2. 灵活的查询语言
3. 不依赖分布式存储，单个服务器节点是自主的
4. 以HTTP方式，通过pull模型拉去时间序列数据
5. 也可以通过中间网关支持push模型
6. 通过服务发现或者静态配置, 来发现目标服务对象
7. 支持多种多样的图表和界面展示

| 主机名   | ip             | 身份             |
| -------- | -------------- | ---------------- |
| bigdata4 | 192.168.41.133 | prometheus服务器 |
| bigdata3 | 192.168.41.132 | 被监控服务器     |
| bigdata5 | 192.168.41.134 | granfana服务器   |

时间同步

```
# systemctl restart ntpd
# systemctl enable ntpd


```

关闭防火墙和seLInux

```
# systemctl stop firewalld
# systemctl enable firewalld

# setenforce 0
# vim /etc/selinux/config
# iptables -F

```

## 部署安装

去官网下载二进制的安装包这里要下载两个东西 ，一个是prometheus-2.41.0-rc.0.linux-amd64.tar 一个是 node_exporter-1.5.0.linux-amd64.tar.gz

简单来说 prometheus-2.41.0-rc.0.linux-amd64.ta是服务器 而 node_exporter-1.5.0.linux-amd64.tar.gz是监控器

监控器除了服务器以外的机器上都要有，服务器只要在你想部署的服务器上就好

解压 -》 软连接 -》 配置环境变量

* -》 修改prometheus文件夹下的prometheus.yml文件
* job_name ： 代表监控器的名字
* targets: ["localhost:9090"] 代表监控器的IP地址以及端口号
* 监控器的默认端口是9100
* 我们对其修改，增加几个

然后在服务器上执行 `prometheus --config.file="/home/hadoop/app/prometheus/prometheus.yml" &`

在另外两个机器上执行 ` node_exporter`

然后访问IP:9090点击Status然后点击Target就可以看见我们的三台机器以及其名字

### 监控Zookeeper

监控Zookeeper和上述监控机器一样，只不过因为github上有开源的[zookeeper_exporter](https://github.com/jiankunking/zookeeper_exporter)然后直接上传到linux上，给他赋权限（执行权限）

然后执行 `zookeeper_exporter ip：ZK的端口：监控日志的对接端口`就启动了，然后我们在我们的prometheus服务器上修改配置文件，添加上即可

### 监控kafka

监控kafka同上

### 监控flume

关于监控flume同上步骤，不过，flume的监控器是go语言进行编译的，所以要先安装go语言的环境

# granfana

其是对普罗米修斯收集的数据进行处理以及显示的一个软件

## 部署

下载rpm包 执行以下命令 ` wget https://dl.grafana.com/enterprise/release/grafana-enterprise-9.1.5-1.x86_64.rpm`

在完成之后直接执行安装命令要root用户 ` rpm -ivh grafana-enterprise-9.1.5-1.x86_64.rpm`

然后执行 ` systemctl start grafana-server`

然后用 `lsof -i:3000` 查看一下3000端口出现如下

```
COMMAND    PID    USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
grafana-s 6962 grafana    8u  IPv6  90355      0t0  TCP *:hbci (LISTEN)

```

然后浏览器访问ip:3000就可以了

进去之后选择数据源，选择普罗米修斯的，然后添加上他的链接就ok了（服务器的链接以及端口）
