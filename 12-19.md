---
title: clickHouse
date: 12-19 8.40
categories: 杂货技术栈
comments: "true"
tag: clickhouse
---
# 为什么选择CK

1. **目前企业用户行为日志每天百亿量级，虽然经过数仓的分层以及数据汇总层通用维度指标的预计算，但有些个性化的分析场景还是需要直接编写程序或sql查询，这种情况下hive sql和spark sql的查询性能已无法满足用户需求，我们迫切的需要一个OLAP引擎来支持快速的即席查询。**
2. **BI存储库主要采用的是Infobright，在千万量级能很快的响应BI的查询请求，但随着时间推移和业务的发展，Infobright的并发量与查询瓶颈日益凸显，我们尝试将大数据量级的表导入TiDB、Hbase、ES等存储库，虽然对查询有一定的提速，但是也存在着相应的问题（后续章节会详细介绍），这时我们考虑到Clickhouse。**
3. **Clickhouse社区活跃度高、版本迭代非常快,几乎几天到十几天更新一个小版本，我们非常看好它以后的发展。**

# ck特性

**Clickhouse是俄罗斯yandex公司于2016年开源的一个列式数据库管理系统，在OLAP领域像一匹黑马一样，以其超高的性能受到业界的青睐。**特性：

* **基于shard+replica实现的线性扩展和高可靠**
* **采用列式存储，数据类型一致，压缩性能更高**
* **硬件利用率高，连续IO，提高了磁盘驱动器的效率**
* **向量化引擎与SIMD提高了CPU利用率，多核多节点并行化大查询**

**不足：**

* **不支持事务、异步删除与更新**
* **不适用高并发场景**

# 数据接入层

**提供了数据导入相关的服务及功能，按照数据的量级和特性我们抽象出三种Clickhouse导入数据的方式。**

* **方式一：数仓应用层小表导入**这类数据量级相对较小，且分布在不同的数据源如hdfs、es、hbase等，这时我们提供基于DataX自研的TaskPlus数据流转+调度平台导入数据，单分区数据无并发写入，多分区数据小并发写入，且能和线上任务形成依赖关系，确保导入程序的可靠性。
* **方式二：离线多维明细宽表导入**这类数据一般是汇总层的明细数据或者是用户基于Hadoop生产的大量级数据，我们基于Spark开发了一个导入工具包，用户可以根据配置直接拉取hdfs或者hive上的数据到clickhouse，同时还能基于配置sql对数据进行ETL处理，工具包会根据配置集群的节点数以及Clickhouse集群负载情况(merges、processes)对local表进行高并发的写入，达到快速导数的目的。
* **方式三：实时多维明细宽表导入**实时数据接入场景比较固定，我们封装了通用的ClickhouseSink，将app、pc、m三端每日百亿级的数据通过Flink接入clickhouse，ClickhouseSink也提供了batchSize(单次导入数据量)及batchTime(单次导入时间间隔)供用户选择。

# 数据存储层

**数据存储层这里我们采用双副本机制来保证数据的高可靠，同时用nginx代理clickhouse集群，通过域名的方式进行读写操作，实现了数据均衡及高可靠写入，且对于域名的响应时间及流量有对应的实时监控，一旦响应速度出现波动或异常我们能在第一时间收到报警通知。**

* **nginx_one_replication：代理集群一半节点即一个完整副本，常用于写操作，在每次提交数据时由nginx均衡路由到对应的shard表，当某一个节点出现异常导致写入失败时，nginx会暂时剔除异常节点并报警，然后另选一台节点重新写入。**
* **nginx_two_replication：代理集群所有节点，一般用作查询和无副本表数据写入，同时也会有对于异常节点的剔除和报警机制。**

# 数据服务层

* **对外：将集群查询统一封装为scf服务(RPC)，供外部调用。**
* **对内：提供了客户端工具直接供分析师及开发人员使用。**

# 数据应用层

* **埋点系统：对接实时clickhouse集群，提供秒级别的OLAP查询功能。**
* **用户分析平台：通过标签筛选的方式，从用户访问总集合中根据特定的用户行为捕获所需用户集。**
* **BI：提供数据应用层的可视化展示，对接单分片多副本Clickhouse集群，可横向扩展。**

# 部署

进行集群部署clickhouse的步骤很简单，不过有很多坑

首先执行

```
sudo yum install yum-utils
sudo rpm --import https://repo.clickhouse.com/CLICKHOUSE-KEY.GPG
sudo yum-config-manager --add-repo https://repo.clickhouse.com/rpm/stable/x86_64
```

然后用命令安装

```
sudo yum install clickhouse-server clickhouse-client


```

然后执行

```
sudo /etc/init.d/clickhouse-server start

```

启动服务后，可以使用命令行客户端连接到它:

```
#默认没有密码：
clickhouse-client
#有密码：
clickhouse-client -u default --password [-m --port 你自定义的端口]
```

```
:) SELECT 1
 
SELECT 1
 
┌─1─┐
│ 1 │
└───┘
 
1 rows in set. Elapsed: 0.003 sec.

```

则基本完成，接下来我们修改配置文件进行集群部署

注意：接下来的操作要换成root用户

vi /etc/clickhouse-server/users.xml修改我们的password

vi /etc/clickhouse-server/config.xml 修改如下

```
#配置3分片2副本集群
 
<remote_servers>
   <ck_cluster>  <!--集群的名字，随意起 -->
            <!-- 集群第一个分片 -->
            <shard>
                <weight>1</weight>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>ip1</host>   
                    <port>9000</port>
                </replica>
                <replica>
                    <host>ip2</host>  
                    <port>9000</port>
                </replica>
            </shard>
            <!-- 集群第二个分片 -->
            <shard>
                <weight>1</weight>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>ip3</host>
                    <port>9000</port> 
                </replica>
                <replica>
                    <host>ip4</host>
                    <port>9000</port>
                </replica>
            </shard>
            <!-- 集群第三个分片 -->
            <shard>
                <weight>1</weight>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>ip5</host>
                    <port>9000</port>
                </replica>
                <replica>
                    <host>ip6</host>
                    <port>9000</port>
                </replica>
            </shard>
    </ck_cluster>
</remote_servers>
 
<zookeeper> 
        <node>
            <host>ip1</host>
            <port>2181</port>
        </node>
        <node>
            <host>ip2</host>
            <port>2181</port>
        </node>
        <node>
            <host>ip3</host>
            <port>2181</port>
        </node>
    </zookeeper>
 
<!-- 定义该clickhouse实例存放什么 -->
    <macros>
              <!-- 几号分片 -->
        <shard>01</shard>
              <!-- 1分片的2副本 -->
        <replica>集群名-01-2</replica>
    </macros>
```

还可以进行修改我们的数据文件以及日志存放的位置，不过有问题，因为clickhouse自己创建的文件默认分组和用户是clickhouse的所以要把权限赋值给我们的文件夹，chmod 777 ，可是如果直接把整个用户文件夹设置777则会造成ssh链接不上，最后选择不修改ck的数据文件夹就ok了

然后我们进行重新启动我们的clickhouse，通过 `sudo systemctl restart clickhouse-server.service`

然后通过命令登录clickhouse `clickhouse-client -u default --password liuzihan010616`

然后我们输入select * from system.clusters; 查看一下自己的集群出现如下结果就成功了

```
Query id: a8bda66b-0e9b-4e52-a2ef-bf4450796d91

┌─cluster────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address───┬─port─┬─is_local─┬─user────┬─default_database─┬─errors_count─┬─slowdowns_count─┬─estimated_recovery_time─┐
│ ck_cluster │         1 │            1 │           1 │ bigdata5  │ 192.168.41.134 │ 9000 │        1 │ default │                  │            0 │               0 │                       0 │
│ ck_cluster │         2 │            1 │           1 │ bigdata4  │ 192.168.41.133 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
│ ck_cluster │         3 │            1 │           1 │ bigdata3  │ 192.168.41.132 │ 9000 │        0 │ default │                  │            0 │               0 │                       0 │
└────────────┴───────────┴──────────────┴─────────────┴───────────┴────────────────┴──────┴──────────┴─────────┴──────────────────┴──────────────┴─────────────────┴─────────────────────────┘

3 rows in set. Elapsed: 0.001 sec. 


```

如果启动不成功，就输入 `journalctl -xe`

查看系统日志，然后看报错信息解决

然后如果想远程ip访问

通过如下：

```
 vim /etc/clickhouse-server/config.xml

```

修改旗下的 `<listen_host>::</listen_host>  `打开就好

因为clickhouse集群之间通信的端口是9000和hadoop的主节点端口冲突

更改的方法：在conf文件更改 `<tcp_port>9001</tcp_port>` 同时上面的集群的端口也要进行更改

创建用户的权限。在user.xml文件里对其进行更改把 `<!-- <access_management>1</access_management> -->`

解开标签就代表这个用户有创建其他用户的权限

# mongo

mongo是一个非关系型的数据库，进而mysql不同，他的值都是key - > value的

非关系型数据库(nosql ),属于文档型数据库。先解释一下文档的数据库，即可以存放xml、json、bson类型系那个的数据。这些数据具备自述性，呈现分层的树状数据结构。数据结构由键值(key=>value)对组成。

1、存储方式：虚拟内存+持久化。

2、查询语句：是独特的MongoDB的查询方式。

3、适合场景：事件的记录，内容管理或者博客平台等等。

4、架构特点：可以通过副本集，以及分片来实现高可用。

5、数据处理：数据是存储在硬盘上的，只不过需要经常读取的数据会被加载到内存中，将数据存储在物理内存中，从而达到高速读写。

6、成熟度与广泛度：新兴数据库，成熟度较低，Nosql数据库中最为接近关系型数据库，比较完善的DB之一，适用人群不断在增长。

## MongoDB优势与劣势

优势：

1、在适量级的内存的MongoDB的性能是非常迅速的，它将热数据存储在物理内存中，使得热数据的读写变得十分快。

2、MongoDB的高可用和集群架构拥有十分高的扩展性。

3、在副本集中，当主库遇到问题，无法继续提供服务的时候，副本集将选举一个新的主库继续提供服务。

4、MongoDB的Bson和JSon格式的数据十分适合文档格式的存储与查询。

劣势：

1、 不支持事务操作。MongoDB本身没有自带事务机制，若需要在MongoDB中实现事务机制，需通过一个额外的表，从逻辑上自行实现事务。 ACID

2、 应用经验少，由于NoSQL兴起时间短，应用经验相比关系型数据库较少。

3、MongoDB占用空间过大。

4、无join

其和mysql的对比如下
![](https://pic.imgdb.cn/item/63a01e84b1fccdcd3663566e.jpg)

## 应用场景

1）表结构不明确且数据不断变大

MongoDB是非结构化文档数据库，扩展字段很容易且不会影响原有数据。内容管理或者博客平台等，例如圈子系统，存储用户评论之类的。

2）更高的写入负载

MongoDB侧重高数据写入的性能，而非事务安全，适合业务系统中有大量“低价值”数据的场景。本身存的就是json格式数据。例如做日志系统。

3）数据量很大或者将来会变得很大

Mysql单表数据量达到5-10G时会出现明显的性能降级，需要做数据的水平和垂直拆分、库的拆分完成扩展，MongoDB内建了sharding、很多数据分片的特性，容易水平扩展，比较好的适应大数据量增长的需求。

4）高可用性

自带高可用，自动主从切换（副本集）

不适用的场景

1）MongoDB不支持事务操作，需要用到事务的应用建议不用MongoDB。

2）MongoDB目前不支持join操作，需要复杂查询的应用也不建议使用MongoDB。

## 关系型数据库和非关系型数据库的应用场景对比、

关系型数据库适合存储结构化数据，如用户的帐号、地址：

1）这些数据通常需要做结构化查询，比如join，这时候，关系型数据库就要胜出一筹

2）这些数据的规模、增长的速度通常是可以预期的

3）事务性、一致性

　　

NoSQL适合存储非结构化数据，如文章、评论：

1）这些数据通常用于模糊处理，如全文搜索、机器学习

2）这些数据是海量的，而且增长的速度是难以预期的，

3）根据数据的特点，NoSQL数据库通常具有无限（至少接近）伸缩性

4）按key获取数据效率很高，但是对join或其他结构化查询的支持就比较差

相比较MySQL，MongoDB数据库更适合那些#读作业较重的任务模型。MongoDB能充分利用机器的内存资源。如果机器的内存资源丰富的话，MongoDB的查询效率会快很多。

在带”_id”插入数据的时候，MongoDB的插入效率其实并不高。如果想充分利用MongoDB性能的话，推荐采取不带”_id”的插入方式，然后对相关字段作索引来查询。
