---
title: hive调优
date: 12-24 8.40
categories: 日志
comments: "true"
tag: hive
---
# hive的内置function

通过 `show functions`查看，命令总数以及名称

通过 `show functions like xxx`代表模糊查询

通过 `desc function extended xxxx方法名字` 查看方法如何使用

## 时间函数

current_date:它打印的是当前时间

```
hive (default)> select current_date;
OK
_c0
2022-12-28
Time taken: 5.424 seconds, Fetched: 1 row(s)

```

current_timestamp:打印当前时间戳

```
hive (default)> select current_timestamp;
OK
_c0
2022-12-28 09:56:56.204
Time taken: 0.23 seconds, Fetched: 1 row(s)

```

unix_timestamp:把肉眼可见的时间格式转换成秒值，1970年到指定日子的秒值

```
hive (default)> desc function extended unix_timestamp;
OK
tab_name
unix_timestamp(date[, pattern]) - Converts the time to a number
Converts the specified time to number of seconds since 1970-01-01. The unix_timestamp(void) overload is deprecated, use current_timestamp.
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp
Function type:BUILTIN
Time taken: 0.015 seconds, Fetched: 4 row(s)

---------------------------------------------------------------使用
hive (default)> select unix_timestamp()
              > ;
unix_timestamp(void) is deprecated. Use current_timestamp instead.
unix_timestamp(void) is deprecated. Use current_timestamp instead.
OK
_c0
1672192863
Time taken: 0.145 seconds, Fetched: 1 row(s)
------------------------------------------------------------------加参数
hive (default)> select unix_timestamp('2022-12-28');
OK
_c0
NULL
Time taken: 0.146 seconds, Fetched: 1 row(s)
hive (default)> select unix_timestamp('2022-12-28 12:00:00');
OK
_c0
1672228800
Time taken: 0.141 seconds, Fetched: 1 row(s)
hive (default)> 
默认加参数的时候要YYYY-MM-dd HH:mm:ss 但是可以加格式 ，通过下述方法添加自定义格式
hive (default)> select unix_timestamp('2022-12-28','yyyy-MM-dd');
OK
_c0
1672185600
Time taken: 0.149 seconds, Fetched: 1 row(s)
其最小的时间是8点，并不是0点

```

from_unixtime：把毫秒数转换成日期

```
hive (default)> desc function extended from_unixtime;
OK
tab_name
from_unixtime(unix_time, format) - returns unix_time in the specified format
Example:
  > SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss') FROM src LIMIT 1;
  '1970-01-01 00:00:00'
Function class:org.apache.hadoop.hive.ql.udf.UDFFromUnixTime
Function type:BUILTIN
Time taken: 0.016 seconds, Fetched: 6 row(s)
--------------------------------------------------------使用
hive (default)> select from_unixtime(1672193662);
OK
_c0
2022-12-28 02:14:22
Time taken: 0.122 seconds, Fetched: 1 row(s)

```

to_date：

```
hive (default)> desc function extended to_date;
OK
tab_name
to_date(expr) - Extracts the date part of the date or datetime expression expr
Example:
   > SELECT to_date('2009-07-30 04:17:52') FROM src LIMIT 1;
  '2009-07-30'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate
Function type:BUILTIN
Time taken: 0.015 seconds, Fetched: 6 row(s)

```

year:month:day:hour:minute等等 ，是取值日期中的一部分

quarter ：季度 =》 q1,q2,q3,q4 => 第几季度

默认是三个月一个季度

month_between : 两个日期差几个月

```
hive (default)> desc function extended months_between;
OK
tab_name
months_between(date1, date2, roundOff) - returns number of months between dates date1 and date2
If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2.
date1 and date2 type can be date, timestamp or string in the format 'yyyy-MM-dd' or 'yyyy-MM-dd HH:mm:ss'. The result is rounded to 8 decimal places by default. Set roundOff=false otherwise.
 Example:
  > SELECT months_between('1997-02-28 10:30:00', '1996-10-30');
 3.94959677
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFMonthsBetween
Function type:BUILTIN
Time taken: 0.016 seconds, Fetched: 8 row(s)

```

add_months:当前日期往后加几个月，可以加正数，也可以加负数

```
hive (default)> desc function extended add_months;
OK
tab_name
add_months(start_date, num_months, output_date_format) - Returns the date that is num_months after start_date.
start_date is a string or timestamp indicating a valid date. num_months is a number. output_date_format is an optional String which specifies the format for output.
The default output format is 'YYYY-MM-dd'.
Example:
  > SELECT add_months('2009-08-31', 1) FROM src LIMIT 1;
 '2009-09-30'.
  > SELECT add_months('2017-12-31 14:15:16', 2, 'YYYY-MM-dd HH:mm:ss') LIMIT 1;
'2018-02-28 14:15:16'.

Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFAddMonths
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 11 row(s)
```

datediff:两个日期相差几天：

```
hive (default)> desc function extended datediff;
OK
tab_name
datediff(date1, date2) - Returns the number of days between date1 and date2
date1 and date2 are strings in the format 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. The time parts are ignored.If date1 is earlier than date2, the result is negative.
Example:
   > SELECT datediff('2009-07-30', '2009-07-31') FROM src LIMIT 1;
  1
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff
Function type:BUILTIN
Time taken: 0.025 seconds, Fetched: 7 row(s)
hive (default)> 

```

date_add:数字可以变成-1的

```
hive (default)> desc function extended date_add;
OK
tab_name
date_add(start_date, num_days) - Returns the date that is num_days after start_date.
start_date is a string in the format 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. num_days is a number. The time part of start_date is ignored.
Example:
   > SELECT date_add('2009-07-30', 1) FROM src LIMIT 1;
  '2009-07-31'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 7 row(s)

```

last_day:有类型限制

```
hive (default)> desc function extended last_day;
OK
tab_name
last_day(date) - Returns the last day of the month which the date belongs to.
date is a string in the format 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. The time part of date is ignored.
Example:
  > SELECT last_day('2009-01-12') FROM src LIMIT 1;
 '2009-01-31'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFLastDay
Function type:BUILTIN
Time taken: 0.015 seconds, Fetched: 7 row(s)

```

日期格式 ： date_foermat =>重要

```
hive (default)> desc function extended date_format;
OK
tab_name
date_format(date/timestamp/string, fmt) - converts a date/timestamp/string to a value of string in the format specified by the date format fmt.
Supported formats are SimpleDateFormat formats - https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html. Second argument fmt should be constant.
Example: > SELECT date_format('2015-04-08', 'y');
 '2015'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateFormat
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 6 row(s)

```

内置的时间函数大体over

## 算数相关的函数

round：小数点后面几位

```
hive (default)> desc function extended round;
OK
tab_name
round(x[, d]) - round x to d decimal places
Example:
  > SELECT round(12.3456, 1) FROM src LIMIT 1;
  12.3'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound
Function type:BUILTIN
Time taken: 0.021 seconds, Fetched: 6 row(s)
```

ceil ： 取整 ，就是返回一个最小的整数，向上取整

```
hive (default)> desc function extended ceil;
OK
tab_name
ceil(x) - Find the smallest integer not smaller than x
Synonyms: ceiling
Example:
  > SELECT ceil(-0.1) FROM src LIMIT 1;
  0
  > SELECT ceil(5) FROM src LIMIT 1;
  5
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil
Function type:BUILTIN

```

floor：向下取整

```
hive (default)> desc function extended floor;
OK
tab_name
floor(x) - Find the largest integer not greater than x
Example:
  > SELECT floor(-0.1) FROM src LIMIT 1;
  -1
  > SELECT floor(5) FROM src LIMIT 1;
  5
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor
Function type:BUILTIN
Time taken: 0.016 seconds, Fetched: 8 row(s)
```

rand：随机数

```
hive (default)> desc function extended rand;
OK
tab_name
rand([seed]) - Returns a pseudorandom number between 0 and 1
Function class:org.apache.hadoop.hive.ql.udf.UDFRand
Function type:BUILTIN
Time taken: 0.016 seconds, Fetched: 3 row(s)
```

## 字符串相关的

upper，lower,length

length:

```
hive (default)> desc function extended length;
OK
tab_name
length(str | binary) - Returns the length of str or number of bytes in binary data
Example:
  > SELECT length('Facebook') FROM src LIMIT 1;
  8
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFLength
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 6 row(s)

```

trim:去除前后空格

```
hive (default)> desc function extended trim;
OK
tab_name
trim(str) - Removes the leading and trailing space characters from str 
Example:
  > SELECT trim('   facebook  ') FROM src LIMIT 1;
  'facebook'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 6 row(s)

```

upper:

```
hive (default)> desc function extended upper;
OK
tab_name
upper(str) - Returns str with all characters changed to uppercase
Synonyms: ucase
Example:
  > SELECT upper('Facebook') FROM src LIMIT 1;
  'FACEBOOK'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper
Function type:BUILTIN
Time taken: 0.018 seconds, Fetched: 7 row(s)
```

lower:

```
hive (default)> desc function extended lower;
OK
tab_name
lower(str) - Returns str with all characters changed to lowercase
Synonyms: lcase
Example:
  > SELECT lower('Facebook') FROM src LIMIT 1;
  'facebook'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower
Function type:BUILTIN
Time taken: 0.013 seconds, Fetched: 7 row(s)

```

lpad:左边补齐

```
hive (default)> desc function extended lpad;
OK
tab_name
lpad(str, len, pad) - Returns str, left-padded with pad to a length of len
If str is longer than len, the return value is shortened to len characters.
In case of empty pad string, the return value is null.
Example:
  > SELECT lpad('hi', 5, '??') FROM src LIMIT 1;
  '???hi'
  > SELECT lpad('hi', 1, '??') FROM src LIMIT 1;
  'h'
  > SELECT lpad('hi', 5, '') FROM src LIMIT 1;
  null
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad
Function type:BUILTIN
Time taken: 0.012 seconds, Fetched: 12 row(s)

```

rpad:右边补齐

```
hive (default)> desc function extended rpad;
OK
tab_name
rpad(str, len, pad) - Returns str, right-padded with pad to a length of len
If str is longer than len, the return value is shortened to len characters.
In case of empty pad string, the return value is null.
Example:
  > SELECT rpad('hi', 5, '??') FROM src LIMIT 1;
  'hi???'
  > SELECT rpad('hi', 1, '??') FROM src LIMIT 1;
  'h'
  > SELECT rpad('hi', 5, '') FROM src LIMIT 1;
  null
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad
Function type:BUILTIN
Time taken: 0.013 seconds, Fetched: 12 row(s)

```

replace:

```
hive (default)> desc function extended replace;
OK
tab_name
replace(str, search, rep) - replace all substrings of 'str' that match 'search' with 'rep'
Example:
  > SELECT replace('Hack and Hue', 'H', 'BL') FROM src LIMIT 1;
  'BLack and BLue'
Function class:org.apache.hadoop.hive.ql.udf.UDFReplace
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 6 row(s)

```

regexp_replace:

```
hive (default)> desc function extended regexp_replace;
OK
tab_name
regexp_replace(str, regexp, rep) - replace all substrings of str that match regexp with rep
Example:
  > SELECT regexp_replace('100-200', '(\d+)', 'num') FROM src LIMIT 1;
  'num-num'
Function class:org.apache.hadoop.hive.ql.udf.UDFRegExpReplace
Function type:BUILTIN
Time taken: 0.013 seconds, Fetched: 6 row(s)

```

substr:字符串截取

```
hive (default)> desc function extended substr;
OK
tab_name
substr(str, pos[, len]) - returns the substring of str that starts at pos and is of length len orsubstr(bin, pos[, len]) - returns the slice of byte array that starts at pos and is of length len
Synonyms: substring
pos is a 1-based index. If pos<0 the starting position is determined by counting backwards from the end of str.
Example:
   > SELECT substr('Facebook', 5) FROM src LIMIT 1;
  'book'
  > SELECT substr('Facebook', -5) FROM src LIMIT 1;
  'ebook'
  > SELECT substr('Facebook', 5, 1) FROM src LIMIT 1;
  'b'
Function class:org.apache.hadoop.hive.ql.udf.UDFSubstr
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 12 row(s)
```

concat：字符串拼接

```
hive (default)> desc function extended concat;
OK
tab_name
concat(str1, str2, ... strN) - returns the concatenation of str1, str2, ... strN or concat(bin1, bin2, ... binN) - returns the concatenation of bytes in binary data  bin1, bin2, ... binN
Returns NULL if any argument is NULL.
Example:
  > SELECT concat('abc', 'def') FROM src LIMIT 1;
  'abcdef'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat
Function type:BUILTIN
Time taken: 0.013 seconds, Fetched: 7 row(s)

```

concat_ws:上个的高级版

```
hive (default)> desc function extended concat_ws;
OK
tab_name
concat_ws(separator, [string | array(string)]+) - returns the concatenation of the strings separated by the separator.
Example:
  > SELECT concat_ws('.', 'www', array('facebook', 'com')) FROM src LIMIT 1;
  'www.facebook.com'
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS
Function type:BUILTIN
Time taken: 0.013 seconds, Fetched: 6 row(s)
hive (default)> 

```

split：字符串分割

```
hive (default)> desc function extended split;
OK
tab_name
split(str, regex) - Splits str around occurances that match regex
Example:
  > SELECT split('oneAtwoBthreeC', '[ABC]') FROM src LIMIT 1;
  ["one", "two", "three"]
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit
Function type:BUILTIN
Time taken: 0.015 seconds, Fetched: 6 row(s)

```

## json相关的数据处理

get_json_object

```
hive (default)> desc function extended get_json_object;
OK
tab_name
get_json_object(json_txt, path) - Extract a json object from path 
Extract json object from a json string based on json path specified, and return json string of the extracted json object. It will return null if the input json string is invalid.
A limited version of JSONPath supported:
  $   : Root object
  .   : Child operator
  []  : Subscript operator for array
  *   : Wildcard for []
Syntax not supported that's worth noticing:
  ''  : Zero length string as key
  ..  : Recursive descent
  @   : Current object/element
  ()  : Script expression
  ?() : Filter (script) expression.
  [,] : Union operator
  [start:end:step] : array slice operator

Function class:org.apache.hadoop.hive.ql.udf.UDFJson
Function type:BUILTIN
Time taken: 0.016 seconds, Fetched: 18 row(s)

```

json_tuple:

```
hive (default)> desc function extended json_tuple;
OK
tab_name
json_tuple(jsonStr, p1, p2, ..., pn) - like get_json_object, but it takes multiple names and return a tuple. All the input parameters and output column types are string.
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple
Function type:BUILTIN
Time taken: 0.014 seconds, Fetched: 3 row(s)
----------------------------------------

```

例子：现在我的数据库里有如下数据：

```
hive (bigdata_hive3)> select * from skill limit 10;
OK
skill.json
[
    null,
    { "id": 1, "animationId": 1, "damage": { "critical": true, "elementId": -1, "formula": "a.atk * 4 - b.def * 2", "type": 1, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 0, "value1": 1, "value2": 0 }], "hitType": 1, "iconIndex": 880, "message1": "%1的攻击！", "message2": "", "mpCost": 0, "name": "攻击", "note": "1 号技能会在选择“攻击”指令时使用。\n\n<setup action>\ndisplay action\nimmortal: targets, true\n</setup action>\n\n<target action>\nif user.attackMotion() !== 'missile'\nmove user: targets, front, 20\nelse\nperform start\nend\nwait for movement\nmotion attack: user\n\nattack animation: target\nwait for animation\naction effect\n</target action>", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 1, "speed": 0, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 10, "messageType": 1 },
    { "id": 2, "animationId": 0, "damage": { "critical": false, "elementId": 0, "formula": "0", "type": 0, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 2, "value1": 1, "value2": 0 }], "hitType": 0, "iconIndex": 688, "message1": "%1正在保护自己。", "message2": "", "mpCost": 0, "name": "防御", "note": "1 号技能会在选择“防御”指令时使用。", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 11, "speed": 10, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 10, "messageType": 1 },
    { "id": 3, "animationId": -1, "damage": { "critical": true, "elementId": -1, "formula": "a.atk * 4 - b.def * 2", "type": 1, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 0, "value1": 1, "value2": 0 }], "hitType": 1, "iconIndex": 880, "message1": "%1的攻击！", "message2": "", "mpCost": 0, "name": "连续攻击", "note": "", "occasion": 1, "repeats": 2, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 1, "speed": 0, "stypeId": 2, "successRate": 100, "tpCost": 0, "tpGain": 5, "messageType": 1 },
    { "id": 4, "animationId": -1, "damage": { "critical": true, "elementId": -1, "formula": "a.atk * 4 - b.def * 2", "type": 1, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 0, "value1": 1, "value2": 0 }], "hitType": 1, "iconIndex": 880, "message1": "%1的攻击！", "message2": "", "mpCost": 0, "name": "两次攻击", "note": "", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 4, "speed": 0, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 5, "messageType": 1 },
    { "id": 5, "animationId": -1, "damage": { "critical": true, "elementId": -1, "formula": "a.atk * 4 - b.def * 2", "type": 1, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 0, "value1": 1, "value2": 0 }], "hitType": 1, "iconIndex": 849, "message1": "%1的攻击！", "message2": "", "mpCost": 0, "name": "三次攻击", "note": "", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 5, "speed": 0, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 4, "messageType": 1 },
    { "id": 6, "animationId": 0, "damage": { "critical": false, "elementId": 0, "formula": "0", "type": 0, "variance": 20 }, "description": "", "effects": [{ "code": 41, "dataId": 0, "value1": 0, "value2": 0 }], "hitType": 0, "iconIndex": 883, "message1": "%1逃跑了。", "message2": "", "mpCost": 0, "name": "逃跑", "note": "", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 11, "speed": 0, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 0, "messageType": 1 },
    { "id": 7, "animationId": 0, "damage": { "critical": false, "elementId": 0, "formula": "0", "type": 0, "variance": 20 }, "description": "", "effects": [], "hitType": 0, "iconIndex": 979, "message1": "%1正在观望。", "message2": "", "mpCost": 0, "name": "观望", "note": "", "occasion": 1, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 0, "speed": 0, "stypeId": 0, "successRate": 100, "tpCost": 0, "tpGain": 10, "messageType": 1 },
    { "id": 8, "animationId": 41, "damage": { "critical": false, "elementId": 0, "formula": "200 + a.mat", "type": 3, "variance": 20 }, "description": "", "effects": [{ "code": 21, "dataId": 4, "value1": 1, "value2": 0 }, { "code": 21, "dataId": 7, "value1": 1, "value2": 0 }, { "code": 21, "dataId": 8, "value1": 1, "value2": 0 }, { "code": 21, "dataId": 8, "value1": 1, "value2": 0 }, { "code": 21, "dataId": 6, "value1": 1, "value2": 0 }], "hitType": 0, "iconIndex": 72, "message1": "%1吟唱了%2！", "message2": "", "mpCost": 5, "name": "治愈", "note": "", "occasion": 0, "repeats": 1, "requiredWtypeId1": 0, "requiredWtypeId2": 0, "scope": 1, "speed": 0, "stypeId": 1, "successRate": 100, "tpCost": 0, "tpGain": 10, "messageType": 1 },
Time taken: 0.103 seconds, Fetched: 10 row(s)

```

然后我们通过json_tuple来解析它

```
hive (bigdata_hive3)> select json_tuple(json , 'id' , 'animationId' , 'damage','description','effects','hitType','iconIndex','message1','message2','mpCost','name','note','occasion','repeats','requiredWtypeId1','requiredWtypeId2','scope','speed','stypeId','successRate','tpCost','tpGain','messageType') from skill limit 10 ;
OK
c0	c1	c2	c3	c4	c5	c6	c7	c8	c9	c10	c11	c12	c13	c14	c15	c16	c17	c18	c19	c20	c21	c22
NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL	NULL
1	1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}		[{"code":21,"dataId":0,"value1":1,"value2":0}]	1	880	%1的攻击！		0攻击	1 号技能会在选择“攻击”指令时使用。

<setup action>
display action
immortal: targets, true
</setup action>

<target action>
if user.attackMotion() !== 'missile'
move user: targets, front, 20
else
perform start
end
wait for movement
motion attack: user

attack animation: target
wait for animation
action effect
</target action>	1	1	0	0	1	0	0	100	0	10	1
2	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}		[{"code":21,"dataId":2,"value1":1,"value2":0}]	0	688	%1正在保护自己。		0	防御	1 号技能会在选择“防御”指令时使用。	1	1	0	0	11	10	0	100	0	10	1
3	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}		[{"code":21,"dataId":0,"value1":1,"value2":0}]	1	880	%1的攻击！		0连续攻击		1	2	0	0	1	0	2	100	0	5	1
4	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}		[{"code":21,"dataId":0,"value1":1,"value2":0}]	1	880	%1的攻击！		0两次攻击		1	1	0	0	4	0	0	100	0	5	1
5	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}		[{"code":21,"dataId":0,"value1":1,"value2":0}]	1	849	%1的攻击！		0三次攻击		1	1	0	0	5	0	0	100	0	4	1
6	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}		[{"code":41,"dataId":0,"value1":0,"value2":0}]	0	883	%1逃跑了。		0	逃跑		11	0	0	100	0	0	1
7	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}		[]	0	979	%1正在观望。		0	观望		1	1	0	0	0	100	0	10	1
8	41	{"critical":false,"elementId":0,"formula":"200 + a.mat","type":3,"variance":20}		[{"code":21,"dataId":4,"value1":1,"value2":0},{"code":21,"dataId":7,"value1":1,"value2":0},{"code":21,"dataId":8,"value1":1,"value2":0},{"code":21,"dataId":8,"value1":1,"value2":0},{"code":21,"dataId":6,"value1":1,"value2":0}]	0	72	%1吟唱了%2！		5	治愈		0	1	100	0	10	1
Time taken: 0.129 seconds, Fetched: 10 row(s)
----------------------------------------------------------------这样是因为数据太多了，我们简单筛选出来几条数据如下
hive (bigdata_hive3)> select json_tuple(json , 'id' , 'animationId' , 'damage') from skill limit 10 ;
OK
c0	c1	c2
NULL	NULL	NULL
NULL	NULL	NULL
1	1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}
2	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}
3	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}
4	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}
5	-1	{"critical":true,"elementId":-1,"formula":"a.atk * 4 - b.def * 2","type":1,"variance":20}
6	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}
7	0	{"critical":false,"elementId":0,"formula":"0","type":0,"variance":20}
8	41	{"critical":false,"elementId":0,"formula":"200 + a.mat","type":3,"variance":20}
Time taken: 0.11 seconds, Fetched: 10 row(s)
----------------------------------------------------------------------对于嵌套的json，就是上面的[{"code":21,"dataId":4,"value1":1,"value2":0},{"code":21,"dataId":7,"value1":1,"value2":0},{"code":21,"dataId":8,"value1":1,"value2":0},{"code":21,"dataId":8,"value1":1,"value2":0},{"code":21,"dataId":6,"value1":1,"value2":0}]这种数据用下面的方法 通过get_json_object

hive (bigdata_hive3)> select get_json_object(json , '$.effects[0]') from skill limit 10;
OK
_c0
NULL
NULL
{"code":21,"dataId":0,"value1":1,"value2":0}
{"code":21,"dataId":2,"value1":1,"value2":0}
{"code":21,"dataId":0,"value1":1,"value2":0}
{"code":21,"dataId":0,"value1":1,"value2":0}
{"code":21,"dataId":0,"value1":1,"value2":0}
{"code":41,"dataId":0,"value1":0,"value2":0}
NULL
{"code":21,"dataId":4,"value1":1,"value2":0}
Time taken: 0.103 seconds, Fetched: 10 row(s)




```

parse_url_tuple:解析url可能会用到=》这个是单纯对url解析

```
hive (bigdata_hive3)> desc function extended parse_url_tuple;
OK
tab_name
parse_url_tuple(url, partname1, partname2, ..., partnameN) - extracts N (N>=1) parts from a URL.
It takes a URL and one or multiple partnames, and returns a tuple. All the input parameters and output column types are string.
Partname: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<KEY_NAME>
Note: Partnames are case-sensitive, and should not contain unnecessary white spaces.
Example:
  > SELECT b.* FROM src LATERAL VIEW parse_url_tuple(fullurl, 'HOST', 'PATH', 'QUERY', 'QUERY:id') b as host, path, query, query_id LIMIT 1;
  > SELECT parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE',  'AUTHORITY', 'USERINFO', 'QUERY:k1') as (ho, pa, qu, re, pr, fi, au, us, qk1) from src a;
Function class:org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple
Function type:BUILTIN
Time taken: 0.017 seconds, Fetched: 9 row(s)

```

## 判断类的

case when / if

```
首先随便创建表
create table emp_info(
id int,
name string
)
row formatted fields terminated by ','
------------------------------数据如下
1,zihang
2,zuan
3,zihan
4,shuangxi
4,mengke
-----------------------------数据筛选
select name , sum(case when name='zihan' then 1 else 0 end) from emp_info
----------------------------if
select sum(if(name='sz',1 ,0))from emp_info
```

udf:

spark ,flink...都有

使用场景：内置函数不可以帮助我们解决问题了

udf的种类：

* udf ：一进一出

  * 继承udf类=》实现evaLuate=》打包上传服务器=》
    * 使用临时函数 =>`add jar 路径`=> 创建临时函数 = >` create function  方法名字 as  ' 包名'`=> 就可以直接使用了，但是临时函数添加的只对当前的session有用，包括add jar  也是临时的
    * 另外一种使用方式  =》 把jar包上传到 hive的auxlib下，然后就不用add jar 了=》直接创建临时函数就ok=》因为hive在启动的时候会有脚本自动add jar
    * 永久函数：一般不建议 =》 除非业务稳定udf函数才会不懂，但是会经常改动就没必要了
      * 永久函数在元数据里是可以找到的
      * `create temporary function 方法名字 as '包名' 文件类型 文件的地址（可以在uri上）`本地路径会出问题=》最好在hdfs上
        * 文件类型
        * using jar
        * File
        * archive
      * 在mysql中的元数据里面，可以查看Funcs表，里面存放着我们的永久函数
      * 删除 `drop function functionname`但是元数据可能会有不同步的问题
  * 临时函数用的比较多但是最推荐=》 `hive -e/f `
  * `hive -i xxxx.sql`初始化hive的时候添加的一些依赖还可以和-f联用，在spark里也是
    例子如下

  ```
  hive (bigdata_hive3)> add jar /home/hadoop/data/bigdatajava-1.0-SNAPSHOT.jar;
  Added [/home/hadoop/data/bigdatajava-1.0-SNAPSHOT.jar] to class path
  Added resources: [/home/hadoop/data/bigdatajava-1.0-SNAPSHOT.jar] 
  hive (bigdata_hive3)> create temporary function hello_udf as 'udf.udftest';
  OK
  hive (bigdata_hive3)> select hello_udf('jjjj');
  OK
  _c0
  jjjjhello
  Time taken: 0.108 seconds, Fetched: 1 row(s)
  -----------------------------------------------------------------------------永久函数和这个差不多
  ```
* udaf：多进一出
* udtf：一进多出

## 调优：

模式限制 ：`hive.mapred.mode =stricted/unstricted`严格模式=》一些风险操作不被允许=》order by 等 =》 原因 order by 做全局排序的时候是一个reduce task的当数据量特别多的时候 可能会卡死

推测式执行：

* 假设 node1 ,node2,node3
* 机器上运行我们的task
* 假设node3机器负载不较高=》cpu利用率高=》mem被别的作业占用

思考=》task 1 ， task2 很快运行完了=》但是因为node3负载高比较慢 =》木桶效应 =》 短板效应 ：桶中能装多少水是由最短的短板决定的=》大数据里task完成的时间是由最慢的task时间决定的

所以很多多大数据框架都有一个机制=》推测式执行机制 =>解决这个问题=>比如node3节点上跑了一定时间，超过了这个一定时间，就会启动推测式执行，就是会在另外一个节点上再启动个一样的task继续执行=》启动参数（默认打开的）

* mapreduce =》
  * map `mapreduce.map.speculative`
  * reduce `mapreduce.reduce.speculative`
* hive =>
  * `hive.mapred.reduce.tasks.speculative.execution`

新的问题：

map端：正常

reduce端： reduce task key 不够分散[skew] => 数据倾斜如何解决？

### 裁剪：了解

参数：`hive.optimize.cp`是不是开启列裁剪默认就是开启=》进行列式存储的时候：行式存储则没有用，因为无论如何都全部加载=》就是选则列的时候选几列就加载几列，不会像以前一样全加载，在进行筛选自己想要的列

参数 ：`hive.optimize.ppd`是不是开启了谓词下压=》和上面一样节省网络io和磁盘io=》减少首次加载的数据量=》如果开启，hive的底层会进行优化=》针对列式存储的时候如果开启，然后用where会直接定位到那个数据，而行式存储则是会对sql进行优化，让其首次加载的数据量变小

* 谓词：where等条件
* 下压：数据源头

mapreduce和hive进行整合的时候可以进行设置maptask的数量以及reduce task数量

map task 个数 取决于 切片的个数 =》文件个数，文件大小，blocksize ， 是不是可以被切分

hive参数 `mapreduce.input.fileinputformat.split.minsize`和 `mapreduce.input.fileinputformat.split.maxsize`

map task越多越好吗？=》并不是=》jvm开启耗费资源=》所以并部署越多越好

设置reduce task 个数 =》 决定了最终的文件的个数 =》

* 用户指定 `set mapreduce.job.reduce=number`
* 用户自己设置参数hive自己算 `set hive.exec.reducers.bytes.per.reduce`=>每个reduce处理的数据量 `set hive.exec.reduces.max` =>reduce task最多有多少个

reduce数量设置按照作业复杂度进行判断

hive 小文件合并问题？
为什么说hdfs不适合处理/存储小文件？

存储角度=》nn压力

处理角度：一个小文件一个reduce task

如何处理小文件？

hdfs api 处理 => 行式存储 =》

hive 里 合并 DDL =》rc 或者orc文件形式存储 =》 Alter table/Partition Concatenate =》走mr

hive可以mr1-》mr2-》mr3 explain可以查看

hive ：sql =》mr sql的复杂度不一样 =》 翻译成 mr的个数以及依赖都不一样

执行计划：

查看sql计划

* sql的语法树：现在没了
* sql的依赖stage
* 以及mr的数量

就是在要执行的sql语句前面加上explain

并行执行即sql可以转换成多个mr 的job执行这些job可能是串行执行，也可能是并行执行

假如sql =》 4个job

1，2，3无依赖关系，那么他可以并行跑（资源足够的情况下） =》 减少时间=》参数 ：`hive.exec.parallel`=》`hive.exec.parallel.thread.number`允许并行的数量

比如 ： emp e leftjoin depton on xxx

就关联之前是可以并行跑的，可以减少时间

## 数据倾斜

wc为例：

map：输出（word ，1）

shuffle ： 按照分区规则，对map端进行数据的重新分发

reduce ： 对shuffle的数据进行处理

但是=》当reduce的数据量严重倾斜的时候 会导致=》

* 能跑完，但是会特别慢
* 跑不完，会卡死

由于某一个或者某几个key对应的数据量特别大，从而导致对应的task处理非常慢，或者卡死=》导致的原因shuffle再分配导致的

走shuffle的

* join
* group by
* count
* distinct

解决 ： 思想-》先打散再聚合

group by 导致的 => map 输出的一些key数据量过大从而导致数据倾斜

通过参数=》

`hive.groupby.skewindata`当出现数据倾斜的时候我们hive会自动对我们的group by 进行优化

`hive.map.aggr`map端开启了一个聚合

不通过参数=》udf =》 相当于先把我们的数据打散之后，对数据进行更改，原来经过shuffle都到一个reduce上，现在，你把数据的匹配规则改一下，让原本10亿的a变成2亿的1_a,2亿的2_a等等，通过上述，把数量，或者操作做完了，然后再次执行一个mr进行去前缀，然后再输出结果上输出a...

=》udf:

add_shuffix

pre_shuffix

例子：

```
select 
pre_suffix(n_deptno) as pre_deptno,
sum(cnt) as cnt_sum
from 
(
		select 
	add_suffix(deptno) as n_deptno,
	count(1) as cnt 
	from emp 
	group by  add_suffix(deptno)
) as as 
 group by pre_suffix(n_deptno);
```

count（distinct）导致的数据倾斜 =》

distinct=》一个reduce task 来完成 =》 必然会导致数据倾斜=》设置reduce 的个数是没有用的=》建议未来不用distinct去重=》用group by 进行

join

shuffle join /普通的join =》会导致

map join =》 不会导致数据倾斜

可以使用map join 但是有个前提 =》 有一个表是小表 =》 小表是条件

```
（1）没有reducer处理，就不会产生数据倾斜；

（2）没有Map -> Reduce中间的shuffle操作，避免了IO

1. 开启MapJoin设置
（1）设置自动选择MapJoin，默认是true
set hive.auto.convert.join = true;
 
（2）设置小表阈值，默认是25M
set hive.mapjoin.smalltable.filesize=25000000;
 
2. 再大表join小表，与小表join大表，执行速率几乎相等

Hive中mapjoin的原理：

map-side join：

小表数据映射成一张hashtable，再上传到分布式节点的内存中；
大表进行分片，每个节点一部分数据，大表数据文件作为map端输入，对map()函数每一对输入的kv都与已加载到内存中的小表数据连接，
把连接结果按key输出，有多少个map task，产生多少个结果文件；
由于join操作在map task中完成，所以无需启动reduce task，没有shuffle操作和reduce端，避免io和数据倾斜

reduce-side join：

map端把结果按key输出，并在value中标记出数据来源于table1还是table2
因为在shuffle阶段已经按key分组，reduce阶段会判断每个value来自哪张表，然后两表相同key的记录连接
join操作在reduce task中完成

缺点1：在map阶段没有对数据瘦身，shuffle的网络传输和排序性能很低
缺点2：reduce对2个集合做城际计算，很耗内存，容易造成oom

```

参数设置（上述）

hints写法

select /* + MAPJOIN(product_info) */

a.*,

b.*

from user_click as a left join product_info as b

on a.product_id=b.product_id

思路 =》 美团的关于数据倾斜的解决方法

a表 =》 有可能会导致数据倾斜的key 和不会导致数据倾斜的

b表 =》 有可能会导致数据倾斜的key 和不会导致数据倾斜的

然后让他们join的时候无数据倾斜的时候，让他们正常关联，有数据倾斜的要加前缀-》关联join-》减前缀-》聚合

或者直接抽取skew数据 -》 一个sql专门关联skew的-》一个是无数据倾斜的key

分桶解决 -》 一般不太纠结这个

通过idea进行加前缀以及减少前缀

```
加前缀--------------------------------------------------------------
package udf;

import org.apache.hadoop.hive.ql.exec.UDF;

import java.util.Random;

public class addqianzhui extends UDF {
    public static void main(String[] args) {

        addqianzhui addqianzhui = new addqianzhui();
        System.out.println(addqianzhui.evaluate(args[0], args[1],args[2]));
    }
    public String evaluate(String input,String num,String biaoshifu){ // object 类型 肯定不会出问题
        Random random = new Random();
        int suijinum = random.nextInt(Integer.parseInt(num));
        return num+biaoshifu+input;
    }
}

----------------------------------------------------------------减前缀
package udf;

import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.kafka.common.protocol.types.Field;

import java.util.Random;

public class cutqianzhui extends UDF {
    public static void main(String[] args) {

        cutqianzhui cutqianzhui = new cutqianzhui();
        System.out.println(cutqianzhui.evaluate(args[0], args[1]));
    }

    public String evaluate(String input,String biaoshifu){ // object 类型 肯定不会出问题
        String data = input.split(biaoshifu)[1];
        return data;
    }
}

```

或者我们还可以对hive底层源码进行二次编译，然后打包上传进行函数的添加

美团文章：

```
没找到
```
