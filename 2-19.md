---
title: hbase
date: 2-19 8.40
categories: 日志
comments: "true"
tag: hbase
---
# hbase

在讲HBase之前先说nosql

## noql

不支持sql语法进行查询数据,查询数据比较快

redis,hbase等

## RDBMS

把数据相当于excel表，有表头什么的

mysql，clickhouse等

## hbase的概述

为了解决读取数据慢的场景

官网地址：[hbase](hbase.apache.org)

通过官网我们进行学习：

* 随机的实时读写数据
* 大体量 => 10亿多个表
* 分布式存储系统 => 构建在hadoop之上 => 高扩展
* nosql数据库

做实时数仓一般可以用hbase来当数仓的存储地方，用phoenix来使用

生产上为什么不用hbase做数仓？

* 不支持sql
* api和一些shell不好用
* 不支持二级索引
* 不好维护

版本：

* cdh : 5.x => hbase 1.x
* cdh: 6.x => hbase 2.x

好用的版本1.2.x

## 部署hbase

hbase也是个主从架构

老大：hmaster

小弟：hregionserver

hbase 是构建在hadoop基础之上的框架

* 数据存储
* 数据资源

环境：

* jdk
* ZK
* hadoop

[下载地址](http://archive.apache.org/dist/)我们下载的是2.0.5的

然后解压设置软连接，设置环境变量

配置参数：

进入conf目录下

编辑hbase-env.sh => 更改 `export HBASE_MANAGES_ZK=false 相当于用外部的zk，不用hbase自己提供的zk`

然后打开hbase-site.xml其余配置如下：

```
<property>
  <name>hbase.rootdir</name>
  <value>hdfs://bigdata3:9000/hbase</value>
</property>
<!--hbase集群开关-->
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>
<!--hbase要在本地存储的临时文件夹-->
<property>
  <name>hbase.tmp.dir</name>
  <value>/home/hadoop/data/hbase</value>
</property>
<!--如果就设置一个hmaster下面是主机ip:端口号-->
<!--如果是HA的hmaster，则下面是直接放上端口号，因为Zk会自动帮我们选择ip-->
<property>
  <name>hbase.master</name>
  <value>bigdata5:60000</value>
</property>
<!--设置Zk的快照的存储位置，默认位置是/tmp.在重启时会清空，因为zk是独立安装的-->
<property>
  <name>hbase.zookeeper.property.dataDir</name>
  <value>/home/hadoop/app/zookeeper/data</value>
</property>

<property>
  <name>hbase.zookeeper.quorum</name>
  <value>bigdata3,bigdata4,bigdata5</value>
</property>
<!--客户端连接Zk的端口-->
<property>
  <name>hbase.zookeeper.property.clientPort</name>
  <value>2181</value>
</property>
<!--zk会话超时的时候hbase把这个值传递给它，向他推荐一个最大的超时时间-->
<property>
  <name>zookeeper.session.timeout</name>
  <value>120000</value>
</property>
<!--当hbase的节点挂了的时候选择重启而不是终止-->
<property>
  <name>hbase.regionserver.restart.on.zk.expire</name>
  <value>true</value>
</property>

</configuration>

```

而如果是HA ：则要把 conf/backup-masters 配上hostname 如果是HA则这个名字多加几个就行

然后进行小弟的配置：conf/regionservers 里ip和名称的映射

启动：`start-hbase.sh`

停止：`stop-hbase.sh`

如果启动的时候报错如下：

```
2022-05-16 21:15:56,450 WARN  [RS-EventLoopGroup-1-2] concurrent.DefaultPromise: An exception was thrown by org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$4.opera
tionComplete()
java.lang.IllegalArgumentException: object is not an instance of declaring class
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hbase.io.asyncfs.ProtobufDecoder.<init>(ProtobufDecoder.java:69)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.processWriteBlockResponse(FanOutOneBlockAsyncDFSOutputHelper.java:343)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.access$100(FanOutOneBlockAsyncDFSOutputHelper.java:112)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$4.operationComplete(FanOutOneBlockAsyncDFSOutputHelper.java:425)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:184)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.initialize(FanOutOneBlockAsyncDFSOutputHelper.java:419)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.access$300(FanOutOneBlockAsyncDFSOutputHelper.java:112)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$5.operationComplete(FanOutOneBlockAsyncDFSOutputHelper.java:477)
        at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$5.operationComplete(FanOutOneBlockAsyncDFSOutputHelper.java:472)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
        at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
        at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.fulfillConnectPromise(AbstractEpollChannel.java:653)
        at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:691)
        at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:567)
        at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:470)
        at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at org.apache.hbase.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)


```

则是hbase的jar包和hadoop的jar包冲突了

执行如下操作：

* 首先进入 `cd app/hbase/lib/client-facing-thirdparty/`
* 然后执行备份：`mkdir before` `mv ./slf4j-* ./before/`
* 然后把hadoop里的jar包拿过来就行了：`cp ~/app/hadoop/share/hadoop/common/lib/slf4j-* ./`
* 然后再配置文件里 `hbase-env.sh`把 `export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP="true"打开就行`
* 就成功解决了

## hbase存储

rowkey：相当于mysql的主键

column family: 列族 简称CF => 一个表里多个column组成一个CF，生产要求尽量不超过三个

column：列

version number： 记录数据插入的时间戳

Region：存储数据的最小单元，按照rowkey划分

如果建表的时候不指定region个数默认是一个region。但是当达到一定阈值会自动切分region。他是放在regionserver上的

### 逻辑概念：

整个数据是按照rowkey进行字典排序的

按照rowkey的范围划分不同的region

region是按照不同的列族划分不同的store

store包含memstore和多个storefile => hfile => hdfs : 当内存存不下，则会存hdfs上，而且storefile最终也是落盘到hdfs上的叫hfile

### 物理层面

实际上hbase以key - value 存储数据的和redis类似

key是rowkey value是CF：column：TS：value

一个regionserver管理一个或者多个region

一个region管理一个或者多个CF

TS=> 管理相同数据的版本

注意：hbase中使用客户端

* put： 插入数据
* delete： 删除数据

其实本质上删除也是插入数据，是插入一条带有删除标记的数据

所以里面没有真正删除的操作，对于有删除标记的数据它不会读取

第一次使用put的时候会产生一个时间戳，就是代表他的版本

只读取最新的时间戳的数据

hbase设计理念=> 重写轻读

hbase加列特别方便，随时加列等

## hbase基本架构

了解

## hbase shell

hbase api几乎不用因为太难写

生产很少用

hbase shell 如下：

```
Usage: hbase [<options>] <command> [<args>]
Options:
  --config DIR         Configuration direction to use. Default: ./conf
  --hosts HOSTS        Override the list in 'regionservers' file
  --auth-as-server     Authenticate to ZooKeeper using servers configuration
  --internal-classpath Skip attempting to use client facing jars (WARNING: unstable results between versions)
  --help or -h         Print this help message

Commands:
Some commands take arguments. Pass no args or -h for usage.
  shell            Run the HBase shell
  hbck             Run the HBase 'fsck' tool. Defaults read-only hbck1.
                   Pass '-j /path/to/HBCK2.jar' to run hbase-2.x HBCK2.
  snapshot         Tool for managing snapshots
  wal              Write-ahead-log analyzer
  hfile            Store file analyzer
  zkcli            Run the ZooKeeper shell
  master           Run an HBase HMaster node
  regionserver     Run an HBase HRegionServer node
  zookeeper        Run a ZooKeeper server
  rest             Run an HBase REST server
  thrift           Run the HBase Thrift server
  thrift2          Run the HBase Thrift2 server
  clean            Run the HBase clean up script
  classpath        Dump hbase CLASSPATH
  mapredcp         Dump CLASSPATH entries required by mapreduce
  pe               Run PerformanceEvaluation
  ltt              Run LoadTestTool
  canary           Run the Canary tool
  version          Print the version
  completebulkload Run BulkLoadHFiles tool
  regionsplitter   Run RegionSplitter tool
  rowcounter       Run RowCounter tool
  cellcounter      Run CellCounter tool
  pre-upgrade      Run Pre-Upgrade validator tool
  hbtop            Run HBTop tool
  CLASSNAME        Run the class named CLASSNAME

```

输入 `hbase shell`就会进入shell界面

如下：

```
[hadoop@bigdata5 ~]$ hbase shell
2023-02-19 12:19:59,274 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
HBase Shell
Use "help" to get list of supported commands.
Use "exit" to quit this interactive shell.
For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell
Version 2.4.11, r7e672a0da0586e6b7449310815182695bc6ae193, Tue Mar 15 10:31:00 PDT 2022
Took 0.0014 seconds                                                                                                                                                                                    
hbase:001:0> help
HBase Shell, version 2.4.11, r7e672a0da0586e6b7449310815182695bc6ae193, Tue Mar 15 10:31:00 PDT 2022
Type 'help "COMMAND"', (e.g. 'help "get"' -- the quotes are necessary) for help on a specific command.
Commands are grouped. Type 'help "COMMAND_GROUP"', (e.g. 'help "general"') for help on a command group.

COMMAND GROUPS:
  Group name: general
  Commands: processlist, status, table_help, version, whoami

  Group name: ddl
  Commands: alter, alter_async, alter_status, clone_table_schema, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, list_regions, locate_region, show_filters

  Group name: namespace
  Commands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables

  Group name: dml
  Commands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve

  Group name: tools
  Commands: assign, balance_switch, balancer, balancer_enabled, catalogjanitor_enabled, catalogjanitor_run, catalogjanitor_switch, cleaner_chore_enabled, cleaner_chore_run, cleaner_chore_switch, clear_block_cache, clear_compaction_queues, clear_deadservers, clear_slowlog_responses, close_region, compact, compact_rs, compaction_state, compaction_switch, decommission_regionservers, flush, get_balancer_decisions, get_balancer_rejections, get_largelog_responses, get_slowlog_responses, hbck_chore_run, is_in_maintenance_mode, list_deadservers, list_decommissioned_regionservers, major_compact, merge_region, move, normalize, normalizer_enabled, normalizer_switch, recommission_regionserver, regioninfo, rit, snapshot_cleanup_enabled, snapshot_cleanup_switch, split, splitormerge_enabled, splitormerge_switch, stop_master, stop_regionserver, trace, unassign, wal_roll, zk_dump

  Group name: replication
  Commands: add_peer, append_peer_exclude_namespaces, append_peer_exclude_tableCFs, append_peer_namespaces, append_peer_tableCFs, disable_peer, disable_table_replication, enable_peer, enable_table_replication, get_peer_config, list_peer_configs, list_peers, list_replicated_tables, remove_peer, remove_peer_exclude_namespaces, remove_peer_exclude_tableCFs, remove_peer_namespaces, remove_peer_tableCFs, set_peer_bandwidth, set_peer_exclude_namespaces, set_peer_exclude_tableCFs, set_peer_namespaces, set_peer_replicate_all, set_peer_serial, set_peer_tableCFs, show_peer_tableCFs, update_peer_config

  Group name: snapshots
  Commands: clone_snapshot, delete_all_snapshot, delete_snapshot, delete_table_snapshots, list_snapshots, list_table_snapshots, restore_snapshot, snapshot

  Group name: configuration
  Commands: update_all_config, update_config

  Group name: quotas
  Commands: disable_exceed_throttle_quota, disable_rpc_throttle, enable_exceed_throttle_quota, enable_rpc_throttle, list_quota_snapshots, list_quota_table_sizes, list_quotas, list_snapshot_sizes, set_quota

  Group name: security
  Commands: grant, list_security_capabilities, revoke, user_permission

  Group name: procedures
  Commands: list_locks, list_procedures

  Group name: visibility labels
  Commands: add_labels, clear_auths, get_auths, list_labels, set_auths, set_visibility

  Group name: rsgroup
  Commands: add_rsgroup, alter_rsgroup_config, balance_rsgroup, get_namespace_rsgroup, get_rsgroup, get_server_rsgroup, get_table_rsgroup, list_rsgroups, move_namespaces_rsgroup, move_servers_namespaces_rsgroup, move_servers_rsgroup, move_servers_tables_rsgroup, move_tables_rsgroup, remove_rsgroup, remove_servers_rsgroup, rename_rsgroup, show_rsgroup_config

SHELL USAGE:
Quote all names in HBase Shell such as table and column names.  Commas delimit
command parameters.  Type <RETURN> after entering a command to run it.
Dictionaries of configuration used in the creation and alteration of tables are
Ruby Hashes. They look like this:

  {'key1' => 'value1', 'key2' => 'value2', ...}

and are opened and closed with curley-braces.  Key/values are delimited by the
'=>' character combination.  Usually keys are predefined constants such as
NAME, VERSIONS, COMPRESSION, etc.  Constants do not need to be quoted.  Type
'Object.constants' to see a (messy) list of all constants in the environment.

If you are using binary keys or values and need to enter them in the shell, use
double-quote'd hexadecimal representation. For example:

  hbase> get 't1', "key\x03\x3f\xcd"
  hbase> get 't1', "key\003\023\011"
  hbase> put 't1', "test\xef\xff", 'f1:', "\x01\x33\x40"

The HBase shell is the (J)Ruby IRB with the above HBase-specific commands added.
For more on the HBase Shell, see http://hbase.apache.org/book.html

```

ps：简单介绍：命名空间

对于我们的数据库，无论是hive还是hbase创建DATabase就是创建一个namespace

### 创建库

```
create_namespace 'warehouse'
```

### 创建表

```
create 'warehouse:test','sku','order'
```

### 查看表

```
hbase:004:0> list
TABLE                                                                                                                                                                                                  
warehouse:test                                                                                                                                                                                         
1 row(s)
Took 0.0230 seconds                                                                                                                                                                                    
=> ["warehouse:test"]
```

### 插入数据

```
put 'warehouse:test', '1', 'sku:skuname', 'aa'
```

### 查看数据

```
hbase:007:0> scan 'warehouse:test'
ROW                                                   COLUMN+CELL                                                                                                                                      
 1                                                    column=sku:skuname, timestamp=2023-02-19T12:34:08.225, value=aa                                                                                  
1 row(s)
Took 0.0529 seconds  
```

### 查数据（精准）

```
hbase:011:0> get 'warehouse:test','1'
COLUMN                                                CELL                                                                                                                                             
 sku:skuname                                          timestamp=2023-02-19T12:34:08.225, value=aa                                                                                                      
1 row(s)
Took 0.0222 seconds 
```

### 删除数据

```
delete 'warehouse:test','1','sku:skuname' // 加时间戳也可以的默认是当前版本，加时间戳默认的话就是时间戳的那个版本
```

### 删除所有数据

因为上面delete要删除多个列的话，太麻烦，用deleteall是直接删除rowkey对应的所有列

```
deleteall 'warehouse:test','2'
```

### 删除表

有前提：table本身是不可用的

就是相当于开关

先用describe查看表

然后用disable给予权限

```
describe 'warehouse:test'
disable 'warehouse:test'
drop 'warehouse:test'
```

### hbase多版本

就是一个表，可以设置它显示多个版本的数据

通过

```
alter 'warehouse:test',{NAME=>'列名'，VERSIONS=>要显示的版本数}
```

scan 'table' => 只是查看最新的数据

如果要看所有的则

```
scan 'table' , {RAW => true , VERSIONS => 要显示的版本数}
```

这个查看和scan的关系不大，它走的是自己的versions，但是如果不加这个参数，则走表的

## hbase的核心架构

hmaster:负责hbase里的表的table region管理以及regionserver里的region的负载均衡问题，负责region的分裂以及分裂后region的迁移，regionserver如果挂了，则由它进行迁移

regionserver：负责数据的读写，和数据持久化，部署的时候尽量和datanode再同节点

zk：负责hmaster和rs之间的通信，存储meta表 所在的rs节点，master可以感知小弟的状态

meta表： => hive元数据 => hbase元数据

可以用scan ' hbase:meta'来查看

```
hbase:010:0> scan 'hbase:meta'
ROW                                                   COLUMN+CELL                                                                                                                                      
 hbase:namespace                                      column=table:state, timestamp=2023-02-19T11:38:17.073, value=\x08\x00                                                                            
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:regioninfo, timestamp=2023-02-19T12:19:55.180, value={ENCODED => ae92042c68118b0558fa2ffc7409a50b, NAME => 'hbase:namespace,,1676777271034.ae92
 ffc7409a50b.                                         042c68118b0558fa2ffc7409a50b.', STARTKEY => '', ENDKEY => ''}                                                                                    
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:seqnumDuringOpen, timestamp=2023-02-19T12:19:55.180, value=\x00\x00\x00\x00\x00\x00\x00\x0A                                          
 ffc7409a50b.                                                                                                                                                                                          
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:server, timestamp=2023-02-19T12:19:55.180, value=bigdata3:16020                                                                      
 ffc7409a50b.                                                                                                                                                                                          
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:serverstartcode, timestamp=2023-02-19T12:19:55.180, value=1676780385227                                                              
 ffc7409a50b.                                                                                                                                                                                          
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:sn, timestamp=2023-02-19T12:19:54.865, value=bigdata3,16020,1676780385227                                                            
 ffc7409a50b.                                                                                                                                                                                          
 hbase:namespace,,1676777271034.ae92042c68118b0558fa2 column=info:state, timestamp=2023-02-19T12:19:55.180, value=OPEN                                                                                 
 ffc7409a50b.                                                                                                                                                                                          
2 row(s)
Took 0.0528 seconds 
```

meta表：

* hbase table:
  * regioninfo: region的范围
  * region所在的机器

Hlog：预写日志

* 每次写数据的时候首先会先写数据到Hlog文件和mysql的binlog类似

blockCache:

* 一个rs只有一个blockcache 用来读缓存，提高读的性能

region:一个或者多个store [cf]

store:

* memstore
  * 作为写缓存，读的时候不一定用
  * 当超过一定阈值flush，落盘到hdfs上
* storefile => hfile
  * 以hfile的形式写入到hdfs上，当hfile数量增长到一定个数之后，系统会自动进行合并
    * 大合并
    * 小合并

hbase是没有小文件问题的

一个regionserver包含多个region

一个region根据CF列族划分成多个store

一个store包含一个memstore和0/多个storefile

一个region只能属于一个表，在一个rs节点上

## 读写流程

写流程：

* client 向zk /hbase/meta-region-server 获取hbase:meta所在的rs节点
* client 向rs节点获取hbase:meta表
  * 根据rowkey对比，找到对应的RS节点和region
* client将请求发送给节点，写数据
* 先写Hlog 然后再写region 列族对应的store里的memstore
* 当memstore达到阈值，就落盘，异步的，将内存中的数据写入文件

读流程：

* client向zk发送请求，获取meta表
* client 会发送请求封装成一个scan，发送给节点进行处理
* 节点会先去memstore里查如果没有，则去blockcache里查，如果还查不到，就去storefile里查
* 节点处理之后就把结果返回给client

## 写问题

正常来说写数据的时候是Hlog一份

然后就写到memstore等等

**memstore flush 的时候hbase的这个region 会暂停读写**

我们必须通过调优解决这个问题，要不然机会出现夯住的时候

**memstore flush 最小级别是region而不是memstore**

调优：

* 当region任意一个memstore的size达到阈值，就会flush

  * 参数：`hbase.hregion.memstore.flush.size`
  * 可以再hbase的配置里更改他的大小，它默认是134217728kb = 128M
  * 一般调整的时候最好按照他的倍数来，一般是给4倍
* 当region所有的memstore加和达到阈值就会触发memstore flush

  * 参数 `hbase.hregion.memstore.block.multiplier`
  * 默认大小是4
  * 当region里的memstore达到这个参数x上面的单个最大阈值的时候
  * 才会开始落盘
  * 所以我们可以通过调大这个参数解决这个问题
* 当regionserver里的所有memstore的和超过 低水位线 则这个regionserver 会强制 进行flush

  * 会导致多个表不能进行读写
  * 先从最大的memstore最大的region开始flush，依次flush
  * 如果低于水位线就不会flush
  * 参数
    * `hbase.regionserver.global.memstore.size ` 这个就是你给hbase的整体容量
    * `hbase.regionserver.global.memstore.size.lower.limit` 这个就是我们计算低水位线的计算条件
    * `hbase.regionserver.global.memstore.size.upper.limit ` 这个就是我们memstore能用的内存能的多少的条件
  * 一般默认情况下upper是最大容量的40%，lower是这个40%的95%
  * 要进行调整就好
* 不太重要的级别
* hlog也有要求 => 当Hlog的数量达到一个阈值的时候就会落盘Hlog 参数 `hbase.regionserver.max.logs`
* 定期级别 => 检查系统的周期是一小时检查一次，确保memstore及时flush 参数把值给成-1就可以关闭它
* 手动级别 => 用户可以提前对他进行flush 通过shell命令的flush 'table/region'

上述对业务影响比较小的：

* 定期级别
* 手动级别
* hlog
* region

影响大的：

* regionserver：几乎是灾难性的 ，暂停时间几乎是能达到分钟级别

## 合并

hfile进行合并参数名称：compaction

把多个小的hfile主动合并成一个大的hfile => minor(小合并)

major(大合并) ：

除了上述的我合并文件之外，他还会把之前打上删除标记的数据删除还有TTL数据，以及超过设定的版本号的数据

就是我们建表的时候设定的版本号

大合并消耗的时间可能会比较长的，资源消耗比较多 => 一般生产上要把这两个自动合并给关闭

然后我们手动合并，控制他的时间

minor的触发条件：

和hfile的个数有关系

参数：`hbase.hstore.compactionThreshold`他的默认值是3

如果超过3个小文件，则会触发这个合并操作

就是再memstore flush之后检查这个文件个数

major触发条件：

自动触发 => 关闭

自动触发的操作：

周期性检查的时间的参数：`hbase.hregion.majorcompaction` 默认是7天

然后还有浮动值，就是这个周期的时间有浮动：

参数是：`hbase.hregion.majorcompaction.jitter`默认是0.5

所以他的时间是[7,7-7x0.5/7+7x0.5]

手动触发的参数：`major_compact`

然后一般就是通过xxl每半年，或者多久触发一次

## rowkey设计

为什么要设计RK？

* 写性能
* 读性能
* 读写数据均衡

hbase本身就是key - value的

如果设计不好,则会导致数据写入不均衡,就容易造成数据倾斜之类的问题

数据写入: 是不是均匀的写入region

数据读取:是按照rowkey进行读取的

预分区:

* 默认建表的时候就一个region,当这个region越来越大,所在机器上的压力也就越来越大
* 然后预分区,就是限定这个region的范围,他只能在一定区间
* 就相当于分出来很多个region
* `create 'lpl','user',SPLITS=>['a','b','c','d']`
* 相当于在default数据库下创建了一个表,叫lpl,然后有一个user列族,后面则是控制每个region的范围,
* 上述的region是均匀的分布在各个机器上的
* 但是这么指定也要注意rowkey的命名,让他均匀的在每个region,要不然如果一个region特别多就和之前一样了
* 针对于写入是如上 : 并不一定是均衡
* 针对于读取数据 : 也不一定精准(get)

生产上:rk设计 + 二级索引 => 精准get => 这种是hbase本身不支持,要你自己来写的

rowkey设计原则:听听就行

* 别太长 => 占用存储空间
* 唯一性
* 散列原则
  * rowkey均匀分布到节点上
* 排序原则

rowkey设计:

* 加盐:
  * 在rowkey前面加上一个固定长度的随机数
  * 缺点 : 第一个region必然是空的 ,不能精准读取(打破之前的数据排序规则)
* hash
  * rk进行hash => 作为前缀加上去,就形成一个新的rowkey => 不会有重复数据
  * 写入保证数据均衡
  * get的性能可以保证
  * scan会变低
  * 但是已经算是最好的方法了
  * 通过hash建表 `create 'tale' , '列族',{NUMREGIONS => region的数量 , SPLITALGO => 'HexStringSplit'}`
  * 就会自动的按照hash进行分区
* 反转

# phoenix

sql on hbase

支持jdbc链接

建表的时候默认有盐表,可以保证数据写入均衡

读数据的时候加索引就好了

[官网](phoenix.apache.org)

他是一个OLTP的框架 : 联机事务处理

优点:

* 可以用sql的方式链接hbase
* hive 和 phoenix整合之后 很方便.表可以互通
* spark

注意它仅仅是个客户端,提交sql的和hive一样

部署:解压->软连接->环境变量->把phoenix-server-hbase-2.4-5.1.3.jar -> cp到hbase的各个节点lib文件夹下

然后修改hbase配置hbase-site.xml

```
  <property>
    <name>hbase.table.sanity.checks</name>
    <value>false</value>
  </property>

  <property>
    <name>hbase.regionserver.wal.codec</name>
    <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
  </property>

  <property>
    <name>phoenix.schema.isNamespaceMappingEnabled</name>
    <value>true</value>
  </property>

  <property>
    <name>phoenix.schema.mapSystemTablesToNamespace</name>
    <value>true</value>
  </property>

  <property>
    <name>hbase.coprocessor.abortonerror</name>
    <value>false</value>
  </property>

```

然后还要再phoenix里的bin中的hbase-site.xml和这个同步

我们直接做软连接了

如果上述报错，则可以试试把三个都cp到hbase的lib下，以及使用命令 `hbase-cleanup.sh --cleanAll`然后重启hbase和phoenix

[phoenix学习使用](https://phoenix.apache.org/language/index.html)

## 简单的应用

### 查看表

```
0: jdbc:phoenix:> !tables
+-----------+-------------+------------+--------------+---------+-----------+---------------------------+----------------+-------------+----------------+--------------+--------------+----------------+--------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME |  TABLE_TYPE  | REMARKS | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION | INDEX_STATE | IMMUTABLE_ROWS | SALT_BUCKETS | MULTI_TENANT | VIEW_STATEMENT | VIEW_T |
+-----------+-------------+------------+--------------+---------+-----------+---------------------------+----------------+-------------+----------------+--------------+--------------+----------------+--------+
|           | SYSTEM      | CATALOG    | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
|           | SYSTEM      | CHILD_LINK | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
|           | SYSTEM      | FUNCTION   | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
|           | SYSTEM      | LOG        | SYSTEM TABLE |         |           |                           |                |             | true           | 32           | false        |                |        |
|           | SYSTEM      | MUTEX      | SYSTEM TABLE |         |           |                           |                |             | true           | null         | false        |                |        |
|           | SYSTEM      | SEQUENCE   | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
|           | SYSTEM      | STATS      | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
|           | SYSTEM      | TASK       | SYSTEM TABLE |         |           |                           |                |             | false          | null         | false        |                |        |
+-----------+-------------+------------+--------------+---------+-----------+---------------------------+----------------+-------------+----------------+--------------+--------------+----------------+--------
```

### 创建数据库

```
0: jdbc:phoenix:> CREATE SCHEMA test
. . . .semicolon> ;
No rows affected (0.322 seconds)

```

### 创建表

```
0: jdbc:phoenix:> CREATE TABLE test.my_test ( id BIGINT not null primary key, date Date);
No rows affected (2.661 seconds)

```

然后去hbase shell 查看

```
hbase:002:0> list
TABLE                                                                                                                                                                                                    
SYSTEM:CATALOG                                                                                                                                                                                           
SYSTEM:CHILD_LINK                                                                                                                                                                                        
SYSTEM:FUNCTION                                                                                                                                                                                          
SYSTEM:LOG                                                                                                                                                                                               
SYSTEM:MUTEX                                                                                                                                                                                             
SYSTEM:SEQUENCE                                                                                                                                                                                          
SYSTEM:STATS                                                                                                                                                                                             
SYSTEM:TASK                                                                                                                                                                                              
TEST:MY_TEST                                                                                                                                                                                             
9 row(s)
Took 0.0139 seconds                                                                                                                                                                                      
=> ["SYSTEM:CATALOG", "SYSTEM:CHILD_LINK", "SYSTEM:FUNCTION", "SYSTEM:LOG", "SYSTEM:MUTEX", "SYSTEM:SEQUENCE", "SYSTEM:STATS", "SYSTEM:TASK", "TEST:MY_TEST"]

```

出现我们创建的表

注意在hbase里表以及数据库的名字会大写,以及表名

### 插入数据

```
0: jdbc:phoenix:> UPSERT INTO MY_TEST(id,date) VALUES(1,'2020-12-01');
1 row affected (0.498 seconds)

```

在hbase里查看数据

```
hbase:004:0> scan 'TEST:MY_TEST'
ROW                                                   COLUMN+CELL                                                                                                                                        
 \x80\x00\x00\x00\x00\x00\x00\x01                     column=0:\x00\x00\x00\x00, timestamp=2023-02-20T12:29:36.819, value=x                                                                              
 \x80\x00\x00\x00\x00\x00\x00\x01                     column=0:\x80\x0B, timestamp=2023-02-20T12:29:36.819, value=\x80\x00\x01v\x1B\x99L\x00                                                             
1 row(s)
Took 0.0540 seconds  
```

这个upsert => insert + update

### 查询数据

```
0: jdbc:phoenix:> select * from test.my_test;
+----+------------+
| ID |    DATE    |
+----+------------+
| 1  | 2020-12-01 |
+----+------------+
1 row selected (0.482 seconds)

```

### 查看字段

```
!describe test.my_test;
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM_PREC_RADIX | NULLABLE | REMARKS | COLUMN_DEF | SQL_DATA_TYPE | SQL_DATETIME_S |
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
|           | TEST        | MY_TEST    | ID          | -5        | BIGINT    | null        | null          | null           | null           | 0        |         |            | null          | null           |
|           | TEST        | MY_TEST    | DATE        | 91        | DATE      | null        | null          | null           | null           | 1        |         |            | null          | null           |
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
0: jdbc:phoenix:> 
```

## 数据结构

数值类型

* 整数：bigint，interger，smallint，tinyint
* 小数： double，float，decimal
* 无符号：unsigned_int，unsigned_date，unsigned_timestamp，unsigned_long等

字符串

* varchar

日期

* date,time,timestamp

复杂类型

* array

其他还有boolean等类型

具体数据类型在[官网](https://phoenix.apache.org/language/datatypes.html)详细请在官网查看

## sql语法

### ddl

非查询的，在phoenix里创建数据库叫创建schema

之前配置的

```
  <property>
    <name>phoenix.schema.isNamespaceMappingEnabled</name>
    <value>true</value>
  </property>

  <property>
    <name>phoenix.schema.mapSystemTablesToNamespace</name>
    <value>true</value>
  </property>

```

就是要让phoenix和hbase里的库一一对应，他的官网给的参数不全，所以搜不到很正常，我这个是5.1.2的正常使用

配置完之后就相当于是你在phoenix里创建一个，同时也映射到hbase里

在phoenix里我们主要创建的表是盐表，

创建盐表 `create table test.test_salt(id bigint primary key,name varchar,age integer,address varchar) SALT_BUCKETS = 3;`

`SALT_BUCKETS`的数值设置要按照hbase有几台regionserver来进行设置，就和hive里的桶表类似

```
0: jdbc:phoenix:> create table test.test_salt(id bigint primary key,name varchar,age integer,address varchar) SALT_BUCKETS = 3;
No rows affected (3.269 seconds)
0: jdbc:phoenix:> !describe test.test_salt;
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME | DATA_TYPE | TYPE_NAME | COLUMN_SIZE | BUFFER_LENGTH | DECIMAL_DIGITS | NUM_PREC_RADIX | NULLABLE | REMARKS | COLUMN_DEF | SQL_DATA_TYPE | SQL_DATETIME_S |
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
|           | TEST        | TEST_SALT  | ID          | -5        | BIGINT    | null        | null          | null           | null           | 0        |         |            | null          | null           |
|           | TEST        | TEST_SALT  | NAME        | 12        | VARCHAR   | null        | null          | null           | null           | 1        |         |            | null          | null           |
|           | TEST        | TEST_SALT  | AGE         | 4         | INTEGER   | null        | null          | null           | null           | 1        |         |            | null          | null           |
|           | TEST        | TEST_SALT  | ADDRESS     | 12        | VARCHAR   | null        | null          | null           | null           | 1        |         |            | null          | null           |
+-----------+-------------+------------+-------------+-----------+-----------+-------------+---------------+----------------+----------------+----------+---------+------------+---------------+----------------+
0: jdbc:phoenix:> 

```

在hbase的web界面查看如下：
![](https://pic.imgdb.cn/item/63f354ebf144a01007a979e5.jpg)

上述就是phoenix创建的盐表所分的region

如果在hbase里用4个随机数创建的盐表，则是region是5个

而在phoenix里创建盐表的时候region的数量是根据 `SALT_BUCKETS`来的，就是相当于用几个随机数创建

### dml

数据操作的

upsert：insert + updata

他的使用和insert是一样的和mysql里的insert也类似

它也可以使用hint的写法控制分区

插入数据 `upsert into test.test_salt values(1,'zs',13,'北京')；`

我们多插入几条数据查看盐表的region写数据的情况

```
upsert into test.test_salt values(1,'zs',13,'北京');
upsert into test.test_salt values(2,'zlisi',11,'上海');
upsert into test.test_salt values(3,'wanghwu',18,'河南');
upsert into test.test_salt values(4,'dam',15,'黑龙江');
upsert into test.test_salt values(5,'huamul',14,'子涵');
```

如下图：

![](https://pic.imgdb.cn/item/63f35801f144a01007ad7320.jpg)

上面有12的是因为我之前写入的时候主键忘记改了，导致一直更新一条数据，所以才写12次

通过图片可知，phoenix所造的是读写均衡的盐表

## 索引

索引的好处：

读的速度增加

在phoenix里索引的种类是比较多的[索引的分类](https://phoenix.apache.org/secondary_indexing.html)

当我们不加索引的时候执行查看语句，我们，查看下他的执行计划

`explain select * from test.test_salt;`

```
0: jdbc:phoenix:> explain select * from test.test_salt;
+-------------------------------------------------------------------------+----------------+---------------+-------------+
|                                  PLAN                                   | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT | null           | null          | null        |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
1 row selected (0.015 seconds)

```

无论加不加判断条件他都是全表扫描的

### 添加索引

就可以让它根据索引来选择，不用全表扫描

加索引的时候是和报表有关的

索引一般是加到经常使用的维度字段的

`create index test_salt_index1 on test.test_salt(name,age);`

```
0: jdbc:phoenix:> create index test_salt_index1 ON test.test_salt(name,age);
5 rows affected (6.274 seconds)
0: jdbc:phoenix:> 

```

但是创建索引之后，要是不使用是没有用的

有的sql是走索引的 有的是不走的

执行以下语句

```
0: jdbc:phoenix:> explain select * from test.test_salt;
+-------------------------------------------------------------------------+----------------+---------------+-------------+
|                                  PLAN                                   | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT | null           | null          | null        |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
1 row selected (0.021 seconds)
0: jdbc:phoenix:> explain select * from test.test_salt where name = 'zs';
+-------------------------------------------------------------------------+----------------+---------------+-------------+
|                                  PLAN                                   | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT | null           | null          | null        |
|     SERVER FILTER BY NAME = 'zs'                                        | null           | null          | null        |
+-------------------------------------------------------------------------+----------------+---------------+-------------+
2 rows selected (0.019 seconds)
0: jdbc:phoenix:> 

```

这些都是全表扫描的因为有FULL SCAN

正常如果带*都是不走索引的，我们要选择索引字段，才会走索引

如下：

```
0: jdbc:phoenix:> explain select name from test.test_salt; 
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
|                                      PLAN                                      | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT_INDEX1 | null           | null          | null        |
|     SERVER FILTER BY FIRST KEY ONLY                                            | null           | null          | null        |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
2 rows selected (0.071 seconds)
0: jdbc:phoenix:> explain select age from test.test_salt;
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
|                                      PLAN                                      | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT_INDEX1 | null           | null          | null        |
|     SERVER FILTER BY FIRST KEY ONLY                                            | null           | null          | null        |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
2 rows selected (0.015 seconds)
0: jdbc:phoenix:> explain select age,name from test.test_salt;
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
|                                      PLAN                                      | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN FULL SCAN OVER TEST:TEST_SALT_INDEX1 | null           | null          | null        |
|     SERVER FILTER BY FIRST KEY ONLY                                            | null           | null          | null        |
+--------------------------------------------------------------------------------+----------------+---------------+-------------+
2 rows selected (0.022 seconds)
0: jdbc:phoenix:> explain select age,name from test.test_salt where name = 'zs';
+-----------------------------------------------------------------------------------------------------+----------------+---------------+-------------+
|                                                PLAN                                                 | EST_BYTES_READ | EST_ROWS_READ | EST_INFO_TS |
+-----------------------------------------------------------------------------------------------------+----------------+---------------+-------------+
| CLIENT 3-CHUNK PARALLEL 3-WAY ROUND ROBIN RANGE SCAN OVER TEST:TEST_SALT_INDEX1 [0,'zs'] - [2,'zs'] | null           | null          | null        |
|     SERVER FILTER BY FIRST KEY ONLY                                                                 | null           | null          | null        |
+-----------------------------------------------------------------------------------------------------+----------------+---------------+-------------+
2 rows selected (0.016 seconds)

```

如上述：

我们如果想使用索引，则要在判断条件里加上索引字段，要不然就不会使用索引

phoenix和mysql的索引是不一样的，phoenix的索引是以表存在底层的，mysql是不会存在的

```
0: jdbc:phoenix:> !tables
+-----------+-------------+------------------+--------------+---------+-----------+---------------------------+----------------+-------------+----------------+--------------+--------------+----------------+--+
| TABLE_CAT | TABLE_SCHEM |    TABLE_NAME    |  TABLE_TYPE  | REMARKS | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION | INDEX_STATE | IMMUTABLE_ROWS | SALT_BUCKETS | MULTI_TENANT | VIEW_STATEMENT |  |
+-----------+-------------+------------------+--------------+---------+-----------+---------------------------+----------------+-------------+----------------+--------------+--------------+----------------+--+
|           | TEST        | TEST_SALT_INDEX1 | INDEX        |         |           |                           |                | ACTIVE      | false          | 3            | false        |                |  
```

查看里面的内容如下

```
0: jdbc:phoenix:> select * from test.TEST_SALT_INDEX1;
+---------+-------+-----+
| 0:NAME  | 0:AGE | :ID |
+---------+-------+-----+
| dam     | 15    | 4   |
| huamul  | 14    | 5   |
| zs      | 13    | 1   |
| zlisi   | 11    | 2   |
| wanghwu | 18    | 3   |
+---------+-------+-----+
5 rows selected (0.043 seconds)

```

上述自动把id加上了，是因为id是主键，就是hbase里的rowkey

加索引快的原因 => 是把hbase里的rowkey再次进行索引，原本hbase里的rowkey就是相当于一层索引的

这个也就是phoenix的索引被称为二级索引的原因

触发索引流程：

* 先去索引表，然后找到对应的rowkey
* 然后通过这个rowkey => 进行精准的get操作

但是当数据越来越多的时候，索引表也会越来越多，我们该如何解决？

我现在的想法是给这个索引表再创建索引

问！

# phoenix和hive打通

hbase + hive 打通

phoenix + hive 打通

我们选择下面的那种，因为hbase的数据结构到hive里可能会出问题[官网](https://phoenix.apache.org/hive_storage_handler.html)

他是有要求的

* phoenix 4.8+
* hive 1.2+
