---
title: 同步工具
date: 2-2 8.40
categories: 日志
comments: "true"
tag: 同步工具
---
# api

## jdbctohive

```
package project


import java.util

import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import tool.sqlUtils
import tool.getmysqldf
import tool.savefile
import tool.readfile
import org.apache.flink.api.java.utils.ParameterTool
object jdbctohive{
  def apply(parameterTool: ParameterTool): jdbctohive = new jdbctohive(parameterTool)

  def main(args: Array[String]): Unit = {
    if (args.length==0){
      println(
        """
          |欢迎使用本程序
          |参数详情 mysql hive
          |-------------------------mysql
          |url 例子 ： jdbc:mysql://bigdata2:3306/try
          |user 例子 ： root
          |password 例子 ： liuzihan010616
          |tablename => 支持谓词下压  例子 ： emp 或者 select * from emp 等
          |driver => com.mysql.jdbc.Driver
          |---------------------------hive
          |mode模式 overwrite append 等
          |hive中的table 例子 bigdata.emp
          |可选参数 分区字段 自动开启的是动态分区 例子 deptno
          |分区字段 [字段值] [标志位]：代表是不是只更新这一个分区的数据
          |jdbc:mysql://bigdata2:3306/try root liuzihan010616 "select * from emp " com.mysql.jdbc.Driver append default.tmp deptno,sal,test,re 999,888
          |""".stripMargin)
    }
    val tool = ParameterTool.fromArgs(args)
    jdbctohive(tool).excute(args)
  }
}




class jdbctohive(parameterTool: ParameterTool) {
  System.setProperty("HADOOP_USER_NAME","hadoop")
  val spark = SparkSession.builder().appName("sqoop").master("local[4]").enableHiveSupport().getOrCreate()
  spark.sparkContext.setCheckpointDir("/tmp/checkpoint")

  val getmysqldf = new readfile
  val sqlUtils = new sqlUtils
  val saveFile = new savefile
  private val catalog: Catalog = spark.catalog
  var changecolunm = false
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val url = parameterTool.getRequired("url")
  val user = parameterTool.getRequired("user")
  val password = parameterTool.getRequired("password")
  val table = parameterTool.getRequired("table")
  val driver = parameterTool.getRequired("driver")
  val mode = parameterTool.getRequired("mode")
  val hivetable = parameterTool.getRequired("hivetable")
  val hivepartition = parameterTool.get("hivepartition",null)
  val partitionValues = parameterTool.get("partitionValues")
  val insertpartition = parameterTool.get("insertpartition")


  def excute(args: Array[String]): Unit = {


    // 获取jdbc的df
    val mysqlconnect = getmysqldf.getmysqldataframe(spark, url, user, password, table , driver)
    // 验证指示
    mysqlconnect.show()
    // 生成hive参数数组
//    var hiveconf = new Array[String](args.length-5)
//    hiveconf = util.Arrays.copyOfRange(args, 5, args.length)
    //hiveconf.foreach(println(_))
    jdbctohive(args.length,catalog,mysqlconnect)
    spark.stop()
  }



  def changecolnums(int: Int,resourcesql:DataFrame) ={
    var finallyresult:Dataset[Row] = null // 最终结果集
    var frame:DataFrame = null // 中间变量
    val strings2 = hivepartition.split(",")
    var hiveconclumns = spark.table(hivetable).columns // hive的列数
    //hiveconclumns.foreach(println((_))) // 验证hive的列数
    var mysqlconnect:DataFrame = resourcesql // 设置数据源的resource

    // 判断分区字段在不在jdbc的数据里，如果不在，则在jdbc的数据源中先添加上分区字段
    var strings1:Array[String] =null
    if (int > 8 && partitionValues != null){
      strings1 = partitionValues.split(",")
    }
    var flagtmp:Int = 0;
    for (elem <- strings2){
      if (!mysqlconnect.columns.contains(elem)){
        println(elem)
        println(strings1(flagtmp))
        mysqlconnect = mysqlconnect.withColumn(elem,lit(strings1(flagtmp)))
        flagtmp = flagtmp + 1
        mysqlconnect.show()
      }
    }



    val jdbcconclumns = mysqlconnect.columns // jdbc的列数


    var jdbcoldsource:Dataset[Row] = null // 源数据库的数据 checkpoint是为了破坏数据均衡，以后能编写变读取

    if (int == 10){
      hivepartition.split(",")(0) match {
        case "" => {
          println("-------------------------无操作")
        }
        case _ => {
          hivepartition.split(",").length match {
            case 1 =>
            {
              jdbcoldsource = spark.sql(
                s"""
                   |select * from ${hivetable} where ${hivepartition} != ${partitionValues}
                   |""".stripMargin).checkpoint()
            }
            case _ =>
            {
              var tmpstring:String  = null
              var flag:Int = 0
                 val flagvalue = partitionValues.split(",")
                  for (elem <- hivepartition.split(",")){
                 if (elem == hivepartition.split(",")(hivepartition.split(",").length-1)){
                   tmpstring = tmpstring + elem + "!=" + flagvalue(flag)
                 }else{
                   tmpstring = tmpstring + elem + "!=" + flagvalue(flag) + "and"
                 }
                    flag = flag + 1
              }
              jdbcoldsource = spark.sql(
                s"""
                   |select * from ${hivetable} where ${tmpstring}
                   |""".stripMargin).checkpoint()
            }
          }
        }
      }


    }else{
      jdbcoldsource =  spark.sql(
        s"""
           |select * from ${hivetable}
           |""".stripMargin).checkpoint()
    }

    var existcolunms: Array[String] = null  // 设置hive或者mysql的额外列
    var resultdf: DataFrame = jdbcoldsource // 获取hive的数据原始数据

    // 判断是hive的列多，还是数据源的列数多
    if (hiveconclumns.length >= jdbcconclumns.length){
      // 判断额外列的存在
      existcolunms= hiveconclumns.filter(hivecol => {
        val bool = jdbcconclumns.map(jdbccol => {
          jdbccol == hivecol
        }).contains(true)
        !bool
      })
      // 判断两个列数是不是相等
        if (existcolunms.isEmpty) {
          frame = mysqlconnect.selectExpr(hiveconclumns: _*)
          frame
        }else{
        // 列数不相等的时候让列数少的加列
        resultdf = mysqlconnect
        for (elem <- existcolunms){
          resultdf = resultdf.withColumn(elem, lit(null))
        }
        // 对字段进行排序 ， 让分区数据的分区字段在最后一列
        frame = resultdf.selectExpr(hiveconclumns: _*)
        // 验证数据
        frame.show()
        // 整合历史数据
        finallyresult = jdbcoldsource.union(frame)
        // 验证数据
        finallyresult.show()
        changecolunm = true
        finallyresult
      }
    }else{
      // 数据的列多
      existcolunms= jdbcconclumns.filter(jdbccol => {
        val bool = hiveconclumns.map(hivecol => {
          jdbccol == hivecol
        }).contains(true)
        !bool
      })

      if (existcolunms.isEmpty) {
        frame = mysqlconnect.selectExpr(hiveconclumns: _*)
        frame
      }else{
        for (elem <- existcolunms){
          resultdf = resultdf.withColumn(elem, lit(null))
        }
        frame = resultdf.selectExpr(jdbcconclumns: _*)
        finallyresult = frame.union(mysqlconnect)
        changecolunm = true
        finallyresult
      }
    }
  }






  def jdbctohive(int: Int,catalog: Catalog,mysqlconnect: DataFrame)={
    // 分割字符串获取hive的 表和数据库
    val hivedbandtables = hivetable.split("\\.")
    val hivepart = hivepartition.split(",")
    hivepart.foreach(println(_))
// catalog的方法 获取表存不存在的方法
//    catalog.listTables(strings(0)).show()
//    val empty = catalog.listTables(strings(0)).filter(x => {
//      x.name == strings(1)
//    }).isEmpty
    val empty = catalog.tableExists(hivedbandtables(0),hivedbandtables(1))
//-----------------------------------------------------------------------------
//    sql的方法
//    val empty1 = spark.sql(
//      """
//        |show tables in hivedb
//        |""".stripMargin).filter("tableName = 'hivetablename'").isEmpty
// --------------------------------------------------------------------------


    // 判断列数是不是相等
    var frameresult:DataFrame = mysqlconnect
    // 先判断表存不存在 ，因为判断列数的方法要求表存在
      empty match {
          // 表不存在
      case false => {
        // 判断输入的变量个数执行 判断分区表还是普通表
        if (int > 7) {
          println("-----------------分区表")
          // 判断分区的参数在不在列中 如果不在 ，则加上 ，在的话就自动往下走
          var hivepartval:Array[String] =null
            if (int > 8 && partitionValues != null){
            hivepartval = partitionValues.split(",")
          }
          var flagtmp:Int = 0;
          for (elem <- hivepart){
            if (!mysqlconnect.columns.contains(elem)){
              println(elem)
              println(hivepartval(flagtmp))
              frameresult = frameresult.withColumn(elem,lit(hivepartval(flagtmp)))
              flagtmp = flagtmp + 1
            }
          }
        }else{
          println("-----------普通表")
          frameresult = mysqlconnect
          mysqlconnect.show()
        }
      }

      case true => {
        // 表存在
        // 判断是不是分区表
        frameresult = changecolnums(int, mysqlconnect)
//        if (args.length > 7) {
//          println("-----------------分区表")
//          if (!mysqlconnect.columns.contains(args(7))){
//            frameresult = changecolnums(args, hiveconf, mysqlconnect)
//          }
//        }else{
//          println("-----------普通表")
//          frameresult = mysqlconnect
//        }
        frameresult.show()}
    }









    spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
    spark.conf.set("hive.exec.dynamic.partition","true")
    spark.conf.set("spark.sql.parquet.writeLegacyFormat", "true")
    println(empty)
    saveFile.savetohiveapi(spark,empty,frameresult,hivetable,mode,hivepartition,changecolunm)
}

}

```

## hivetojdbc

```
package project

import java.util

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.catalog.Catalog
import tool.{getmysqldf, savefile, sqlUtils,readfile}
import org.apache.flink.api.java.utils.ParameterTool
object hivetojdbc{
  def apply(parameterTool: ParameterTool): hivetojdbc = new hivetojdbc(parameterTool)

  def main(args: Array[String]): Unit = {
    if (args.length==0){
      println(
        """
          |欢迎使用本程序
          |参数说明
          |总体参数种类 hive mysql
          |---------------------------hive
          |hive中要选择的字段 例子 ： "sal,big  / *  "
          |hive的table的名字 例子 ： bigdata_hive3.emp
          |hive中的 条件可以为空 例子 ： where sal > '300'
          |---------------------------mysql
          |savemode overwrite append 等
          |url 例子 ： jdbc:mysql://bigdata2:3306/try
          |user 例子 ： root
          |password 例子 ： liuzihan010616
          |dbtable 例子 ： emp
          |幂等性的列 ： 例子 ： sal
          |驱动名称 ： 例子 com.mysql.jdbc.Driver
          |""".stripMargin)
    }
    val tool = ParameterTool.fromArgs(args)
    hivetojdbc(tool).excute()
  }
}




class hivetojdbc(parameterTool: ParameterTool) {
  val spark = SparkSession.builder().enableHiveSupport().getOrCreate()
  val getmysqldf = new readfile
  val sqlUtils = new sqlUtils
  val saveFile = new savefile
  private val catalog: Catalog = spark.catalog
  val hiveconclunms = parameterTool.getRequired("hiveconclumns")
  val hivetable = parameterTool.getRequired("hivetable")
  val hiveoption = parameterTool.get("hiveoption",null)
  val url = parameterTool.get("url","jdbc:mysql://bigdata2:3306/bigdata")
  val user = parameterTool.get("user","root")
  val pasword = parameterTool.get("password","liuzihan010616")
  val dbtable = parameterTool.getRequired("dbtable")
  val driver = parameterTool.getRequired("driver")
  val midengconclumns = parameterTool.getRequired("col")
  val mode = parameterTool.getRequired("mode")

  def excute(): Unit = {

    val frame = sqlUtils.checksql(spark, sqlUtils.hivesqlchoose(hiveconclunms,hivetable,hiveoption))
    saveFile.savetojdbc(spark,frame,url,user,pasword,dbtable,driver,midengconclumns,mode)
  }

}

```

# sql方式

## jdbctohive

```
package sparkfirst

import org.apache.spark.sql.SparkSession
import tool.savefile
import tool.sqlUtils
import org.apache.spark.sql.functions._
object test {
  val spark = SparkSession.builder().appName("Sparksql01").master("local[4]").enableHiveSupport().getOrCreate()
  private val savefile = new savefile
  private val utils = new sqlUtils
  def main(args: Array[String]): Unit = {
    val df = spark.read.format("JDBC")
      .option("url","jdbc:mysql://bigdata2:3306/try")
      .option("dbtable", "emp")
      .option("user", "root")
      .option("password", "liuzihan010616")
      .load()
    df.select("sal").tail(1).foreach(println(_))
    println(df.select("sal").tail(1)(0)(0))
    df.show()


    var str:String = null
    val bool = spark.catalog.tableExists("default.tmp")
    if (bool){
      spark.sql(
        s"""
          |drop table default.tmp
          |""".stripMargin)
      str = utils.mkcreatesql(df, "default.tmp", "text", "','","deptno,hiredate")
      utils.checksql(spark,str)
    }else{
      str = utils.mkcreatesql(df, "default.tmp", "text", "','","deptno,hiredate")
      utils.checksql(spark,str)
    }

    spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
    spark.conf.set("hive.exec.dynamic.partition","true")
    val frame = df.withColumn("ee", lit("aaa"))
    utils.insertmake(spark,df,"default.tmp","','","deptno,hiredate")
    utils.changecolunms(spark,frame,"default.tmp")
    utils.insertmake(spark,frame,"default.tmp","','","deptno,hiredate")


  }
}

```

## hivetojdbc

用sqlUtils里自己定义的api来进行

# 思路及实现功能

## source

通过api进行对jdbc数据接收

通过

```
getmysqldf.getmysqldataframe(spark, url, user, password, table , driver)
----------------------------------------------------------------------------------------------
  def getmysqldataframe(sparkSession: SparkSession,string: String*) ={
    val sql = string(3)
    val frame: DataFrame = sparkSession.read.format("jdbc").options(Map("url" -> string(0), "user" -> string(1), "password" -> string(2), "dbtable" -> s"($sql) as tmp","driver"->string(4))).load()
    frame
  }

```

来获取jdbc数据

其他的方法 ： 其中options 可以换成多个option来进行，option里是KV类型的

通过

```
 val frame = sqlUtils.checksql(spark, sqlUtils.hivesqlchoose(hiveconclunms,hivetable,hiveoption))
-----------------------------------------------------------------------------------------------------------
  def checksql(spark:SparkSession, string: String)={
    spark.sql(string)
}
---------------------------------------------------------------------------------------------------------------
 def hivesqlchoose(hiveconclumns:String,hivetable:String,hiveoptions:String)={

    "select" + " " + hiveconclumns + " " + "from" + " " +  hiveconclumns + " " + hiveoptions
  }

```

前提在saprksession处打开enablesupporthive参数

## todo

通过api对数据进行整合以及处理

功能 ：

* jdbctohive
  * 基本功能
    * 同步普通表
    * 同步分区表
      * 单分区
      * 多分区
  * 追加功能api
    * 用户自定义分区字段及其值
    * 用户在jdbc数据中增加列，hive中自动增加列
    * 分区字段更改且不丢失源数据
    * 可以实现表中自带的字段以及用户定义的字段一起分区的操作
    * 实现单独对一个分区的数据追加或者重新写入
    * 设置hive表的存储以及压缩格式
    * 实现对所有分区的追加或者重新写入
    * 通过flink的参数工具部署
  * 追加功能sql
    * 实现用户自定义分区字段以及值
    * 用户在jdbc数据中增加列，hive中自动增加列
    * 可以实现表中自带的字段以及用户定义的字段一起分区的操作
    * 实现单独对一个分区的数据追加或者重新写入
    * 实现对所有分区的追加或者重新写入
    * 设置hive表的存储格式
    * 实现设置存储格式text等
    * 通过flink的参数工具部署
* hivetojdbc
  * 基本功能
    * 同步数据
  * 追加功能
    * 幂等性操作

基本功能没有上面要注意的点，但是多分区的时候我是采用获取字符串然后split之后map加上数据类型然后mkstring制作的

sql：

但是分区一般分为数据里的分区字段，以及用户自定义的分区字段，针对于用户自定义的分区，我直接把他们定义为string，但是对于数据里的分区字段，我选择保留他原始的类型，通过筛选出他包含的分区字段的schema信息，然后通过他的datatype，来进行数据的备份，最后和上述用户自定义的分区字段拼接到一起，就可以了，最后前面加上partitioned by 就好了，注意点是要提取变量，以及判断最后一次的时机，以及如何判断分区列在不在字段里。

api：

对于api则更为简单，直接调用partitionbyapi然后把字符串通过split然后：_*的方式传入，就ok了，但是api的分区字段必须是在df里的，也就是说，我们要提前把分区字段加上，先判断有没有分区字段，然后进而加上分区字段

追加功能：


## sink

通过api对数据进行输出到表

### sink到hive

api

```
    saveFile.savetohiveapi(spark,empty,frameresult,hivetable,mode,hivepartition,changecolunm,fileformated,codec)
------------------------------------------------------------------------------------------------------------------
def savetohiveapi(sparkSession: SparkSession,boolean: Boolean,spark: DataFrame,hivetable:String,mode:String,hivepartition:String,changecolnums:Boolean,fileformat:String,codec:String) = {




    if (!boolean){
      if (hivepartition != null){
        spark.write.partitionBy(hivepartition.split(","):_*).option("fileFormat",fileformat).option("compression",codec).mode(mode).format("hive").saveAsTable(hivetable)
      }else {

        println(hivetable)
        println(hivepartition)
        spark.write.option("fileFormat",fileformat).option("compression",codec).mode(mode).format("hive").saveAsTable(hivetable)
      }

    }else{
      changecolnums match {
        case true =>  {
          if (hivepartition != null){
            if(sparkSession.table(hivetable).columns.length != spark.columns.length){
             sparkSession.sql(
               s"""
                 |drop table ${hivetable}
                 |""".stripMargin)
            }
            spark.write.partitionBy(hivepartition.split(","):_*).option("fileFormat",fileformat).option("compression",codec).mode(mode).format("hive").saveAsTable(hivetable)
          }else {
            spark.write.option("fileFormat",fileformat).option("compression",codec).mode(mode).format("hive").saveAsTable(hivetable)
          }
        }
        case false => spark.write.option("fileFormat",fileformat).option("compression",codec).mode(mode).format("hive").insertInto(hivetable)
      }

      spark.show()
      println(spark.count())

    }
  }

```

sql

```
def insertmake(sparkSession: SparkSession,dataFrame: DataFrame,tablename:String,otheroptions:String*) ={

    var strings:Array[String] = null

      dataFrame.selectExpr(sparkSession.table(tablename).columns:_*).createOrReplaceTempView("tmp")

   // val partitionstring = sparkSession.table(tablename).columns.tail(sparkSession.table(tablename).columns.length - 2)
    otheroptions.length match {
      case 0 => {
        sparkSession.sql(
          s"""
             |insert overwrite ${tablename}
             |select * from tmp
             |""".stripMargin)
      }
      case _ => {

        if (otheroptions.length > 1){
          strings = otheroptions(1).split(",").filter(conclunms => {
            !dataFrame.columns.contains(conclunms)
          })
          val fuzhiarray:Array[String] = util.Arrays.copyOfRange(otheroptions.toArray, 2, otheroptions.length)
          fuzhiarray.foreach(println(_))
          strings.isEmpty match {
            case true => {

          sparkSession.sql(
          s"""
            |insert overwrite ${tablename} partition(${otheroptions(1).split(",").map(conclunms => {s"${conclunms}"}).mkString(",")})
            |select * from tmp
            |""".stripMargin)
          }
            case false => {
              var tmpdf:DataFrame = dataFrame
              for (i <- 0 to strings.length-1){
                tmpdf = tmpdf.withColumn(strings(i),lit(fuzhiarray(i)))
              }
              tmpdf.show()
              tmpdf.printSchema()
              tmpdf = tmpdf.selectExpr(sparkSession.table(tablename).columns: _*)
              tmpdf.show()
              tmpdf.printSchema()
              val str = tmpdf.columns.mkString(",\n")
              tmpdf.createOrReplaceTempView("smp")
              sparkSession.sql(
                s"""
                   |insert overwrite ${tablename} partition(${otheroptions(1).split(",").map(conclunms => {s"${conclunms}"}).mkString(",")})
                   |select ${str} from smp
                   |""".stripMargin)
            }
          }



        }else{
          strings = otheroptions(0).split(",").filter(conclunms => {
            !dataFrame.columns.contains(conclunms)
          })
          val fuzhiarray:Array[String] = util.Arrays.copyOfRange(otheroptions.toArray, 1, otheroptions.length)

          strings.isEmpty match {
            case true => {
              sparkSession.sql(
                s"""
                   |insert overwrite ${tablename} partition(${otheroptions(0).split(",").map(conclunms => {s"${conclunms}"}).mkString(",")})
                   |select * from tmp
                   |""".stripMargin)
            }

            case false => {
              var tmpdf:DataFrame = dataFrame

              for (i <- 0 to strings.length-1){
                tmpdf = tmpdf.withColumn(strings(i),lit(fuzhiarray(i)))
              }
              tmpdf.show()
              tmpdf.printSchema()
              tmpdf = tmpdf.selectExpr(sparkSession.table(tablename).columns: _*)
              val str = tmpdf.columns.mkString(",\n")
              tmpdf.createOrReplaceTempView("smp")
              sparkSession.sql(
                s"""
                   |insert overwrite ${tablename} partition(${otheroptions(0).split(",").map(conclunms => {s"${conclunms}"}).mkString(",")})
                   |select ${str} from smp
                   |""".stripMargin)
            }
        }
      }
    }
  }
  }
```

### sink到jdbc

api

```
def savetojdbc(spark: SparkSession,df: DataFrame, url:String,user:String,password:String,dbtable:String,driver:String,mideng:String,mode:String)={
    val map = Map("url" -> url,
      "user" -> user,
      "password" -> password,
      "dbtable" -> dbtable,
      "driver"-> driver)

//    df.write.mode(string(2)).format(string(1)).options(map).save()
    // -------------------------------------幂等性
    val connection = jdbcconnect.getconncet(driver,url,user,password)
    try{
      val bool = connection.createStatement().executeQuery(s"show tables like '${dbtable}'").next()
      if (!bool){
        throw new NullPointerException(s"写入的结果表${dbtable} 尚未创建！！！")
      }else{
        var flag:Any = null
        val flagbool = mysqldf.getmyqsldffromMap(spark, map).select(mideng).isEmpty
        if (!flagbool){
         flag = mysqldf.getmyqsldffromMap(spark, map).select(mideng).tail(1)(0)(0)
        }
        val tmpresult = df.select(mideng).filter(line => {
          line.getString(0) != flag
        })

        if (df.isEmpty){
          println("数据集为空")
        }else{
          if (tmpresult.isEmpty){
            println("你的数据已经插入过")
            df.show(false)
          }else {
//            df.show()
//            println(df.count())
            //val insertresult = tmpresult.join(df, string(5))
            tmpresult.show()
            println(tmpresult.count())
//            insertresult.show()
//            println(insertresult.count())
            tmpresult.write.mode(mode).format("jdbc").options(map).save()
          }
        }
      }
    }finally {
      connection.close()
    }


  }

```
