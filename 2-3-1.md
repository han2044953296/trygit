---
title: 监控yarn
date: 2-3 8.40
categories: 日志
comments: "true"
tag: 监控yarn
---
# 目标

监控yarn的资源

# 流程

数据采集：采集yarn的指标

数据处理：实时处理：spark-streaming

数据输出：mysql(加索引)，olap（毫秒级别）

数据可视化：superset,dataease

olap:clickhouse,doris,tidb,phenix

oltp:支持事务的

# 链路

yarn -> jar 采集数据 ->    kafka->  sparkstreaming -> ck -> superset/dataease

采集的数据格式：

* 文本数据 ： 网络io占据量少，分隔符设置有要求
* json数据 ： json要占用网络io ，解析方便

# start

## 采集数据

### yarn的api

添加yarn的依赖

```
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>3.3.4</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-yarn-common</artifactId>
      <version>3.3.4</version>
    </dependency>
```

在idea里来进行开发

设置获取yarn数据的接口

```
package sparkfirst

import java.util

import org.apache.hadoop.yarn.api.records.YarnApplicationState
import org.apache.hadoop.yarn.client.api.YarnClient
import org.apache.hadoop.yarn.conf.YarnConfiguration

trait YarnInfo {

  def getYarnInfo={
    val client = YarnClient.createYarnClient()
    /*
    init 方法里面要求的是conf ，是yarn的配置文件
    先在resource里加上yarn-site.xml
    然后new配置文件类
    然后启动客户端
     */
    val configuration = new YarnConfiguration()
    client.init(configuration)
    client.start()

    val states = util.EnumSet.noneOf(classOf[YarnApplicationState])
    states.add(YarnApplicationState.ACCEPTED)
    states.add(YarnApplicationState.RUNNING)
    states.add(YarnApplicationState.NEW)
    states.add(YarnApplicationState.SUBMITTED)
    states.add(YarnApplicationState.KILLED)
    states.add(YarnApplicationState.NEW_SAVING)
    states.add(YarnApplicationState.FAILED)


    val reports = client.getApplications(states)

    val value = reports.iterator()

    val builder = new StringBuilder
    while (value.hasNext){
      val report = value.next()
      val report1 = report.getApplicationResourceUsageReport
      val id = report.getApplicationId
      val host = report.getHost
      val applicationType = report.getApplicationType
      val name = report.getName
      val starttime = report.getStartTime
      val user = report.getUser
      val finishtime = report.getFinishTime
      val mem = report1.getMemorySeconds
      val vcore = report1.getVcoreSeconds
      val size = report1.getUsedResources.getMemorySize
      val cores = report1.getUsedResources.getVirtualCores
      val resources = report1.getUsedResources.getResources
      val state = report.getYarnApplicationState
      val url = report.getTrackingUrl
      val margin =
        s"""
           |report: ${report}
           |report1 : ${report1}
           |id:${id}
           |host:${host}
           |applicationtype : ${applicationType}
           |name : ${name}
           |starttime ${starttime}
           |finishtime : ${finishtime}
           |user:${user}
           |memeveryscends:${mem}
           |vcoreeveryscends:${vcore}
           |size:${size}
           |cores${cores}
           |state:${state}
           |url:${url}
           |resources:${resources.mkString(",")}
           |---
           |""".stripMargin
      builder.appendAll(margin)
    }

    println(builder)

    }
}

```

在主方法里继承接口并且实现方法

```
package sparkfirst
import org.apache.flink.api.java.utils.ParameterTool
object testyarn {

  def apply(parameterTool: ParameterTool): testyarn = new testyarn(parameterTool)
  def main(args: Array[String]): Unit = {
    val tool = ParameterTool.fromArgs(args)
    testyarn(tool).excute()
  }
}
class testyarn(parameterTool: ParameterTool) extends YarnInfo {




  def excute(): Unit ={

    getYarnInfo
  }
}

```

在机器上启动一个sparksql通过yarn模式的部署

获取数据如下

```
report: applicationId { id: 1 cluster_timestamp: 1675390427337 } user: "hadoop" queue: "default" name: "SparkSQL::192.168.41.132" host: "192.168.41.133" rpc_port: -1 yarn_application_state: RUNNING trackingUrl: "http://bigdata4:9999/proxy/application_1675390427337_0001/" diagnostics: "" startTime: 1675390547814 finishTime: 0 final_application_status: APP_UNDEFINED app_resource_Usage { num_used_containers: 3 num_reserved_containers: 0 used_resources { memory: 5120 virtual_cores: 3 resource_value_map { key: "memory-mb" value: 5120 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 3 units: "" type: COUNTABLE } } reserved_resources { memory: 0 virtual_cores: 0 resource_value_map { key: "memory-mb" value: 0 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 0 units: "" type: COUNTABLE } } needed_resources { memory: 5120 virtual_cores: 3 resource_value_map { key: "memory-mb" value: 5120 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 3 units: "" type: COUNTABLE } } memory_seconds: 3804903 vcore_seconds: 2232 queue_usage_percentage: 41.666664 cluster_usage_percentage: 41.666664 preempted_memory_seconds: 0 preempted_vcore_seconds: 0 application_resource_usage_map { key: "memory-mb" value: 3804903 } application_resource_usage_map { key: "vcores" value: 2232 } application_preempted_resource_usage_map { key: "memory-mb" value: 0 } application_preempted_resource_usage_map { key: "vcores" value: 0 } } originalTrackingUrl: "http://bigdata3:4040" currentApplicationAttemptId { application_id { id: 1 cluster_timestamp: 1675390427337 } attemptId: 1 } progress: 0.1 applicationType: "SPARK" log_aggregation_status: LOG_NOT_START unmanaged_application: false priority { priority: 0 } appNodeLabelExpression: "<Not set>" amNodeLabelExpression: "<DEFAULT_PARTITION>" appTimeouts { application_timeout_type: APP_TIMEOUT_LIFETIME application_timeout { application_timeout_type: APP_TIMEOUT_LIFETIME expire_time: "UNLIMITED" remaining_time: -1 } } launchTime: 1675390548575
report1 : num_used_containers: 3 num_reserved_containers: 0 used_resources { memory: 5120 virtual_cores: 3 resource_value_map { key: "memory-mb" value: 5120 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 3 units: "" type: COUNTABLE } } reserved_resources { memory: 0 virtual_cores: 0 resource_value_map { key: "memory-mb" value: 0 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 0 units: "" type: COUNTABLE } } needed_resources { memory: 5120 virtual_cores: 3 resource_value_map { key: "memory-mb" value: 5120 units: "Mi" type: COUNTABLE } resource_value_map { key: "vcores" value: 3 units: "" type: COUNTABLE } } memory_seconds: 3804903 vcore_seconds: 2232 queue_usage_percentage: 41.666664 cluster_usage_percentage: 41.666664 preempted_memory_seconds: 0 preempted_vcore_seconds: 0 application_resource_usage_map { key: "memory-mb" value: 3804903 } application_resource_usage_map { key: "vcores" value: 2232 } application_preempted_resource_usage_map { key: "memory-mb" value: 0 } application_preempted_resource_usage_map { key: "vcores" value: 0 }
id:application_1675390427337_0001
host:192.168.41.133
applicationtype : SPARK
name : SparkSQL::192.168.41.132
starttime 1675390547814
finishtime : 0
user:hadoop
memeveryscends:3804903
vcoreeveryscends:2232
size:5120
cores3
state:RUNNING
url:http://bigdata4:9999/proxy/application_1675390427337_0001/
resources:name: memory-mb, units: Mi, type: COUNTABLE, value: 5120, minimum allocation: 0, maximum allocation: 9223372036854775807, tags: [], attributes {},name: vcores, units: , type: COUNTABLE, value: 3, minimum allocation: 0, maximum allocation: 9223372036854775807, tags: [], attributes {}
---
```

yarn上的数据如下：

![img](https://pic.imgdb.cn/item/63dc72ba07d5ca7206288d89.jpg)经对比数据一致

## 发送数据到kafka

如下：

```
package sparkfirst
import java.util.Properties

import org.apache.kafka.clients.producer.KafkaProducer
import org.apache.kafka.clients.producer.Producer
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.flink.api.java.utils.ParameterTool

import scala.util.Random
object testyarn {
  def apply(parameterTool: ParameterTool): testyarn = new testyarn(parameterTool)
  def main(args: Array[String]): Unit = {
    val tool = ParameterTool.fromArgs(args)
    testyarn(tool).excute()
  }
}

class testyarn(parameterTool: ParameterTool) extends YarnInfo {

  val properties = new Properties
  properties.put("bootstrap.servers", "bigdata3:9092,bigdata4:9092,bigdata5:9092 ")
  properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
  properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")
  properties.put("acks", "all")


  def excute() ={
    val producer: Producer[String, String] = new KafkaProducer[String, String](properties)
  val i = new Random().nextInt(10) % 3
  val strings = getYarnInfo.split("-------------------------------------------------------------")
  for (elem <- strings) {
    println(elem)
    producer.send(new ProducerRecord[String, String]("yarninfo", i, " ",  elem))
  }

    producer.close()
}
}
```

## 消费数据

### 通过sparkstreaming消费

```
package project

import java.lang.reflect.Field
import java.util.Properties

import org.apache.flink.api.java.utils.ParameterTool
import org.apache.poi.ss.formula.functions.T

import scala.reflect.runtime.{universe => ru}
import org.apache.spark.sql
import org.apache.spark.sql.catalyst.plans.logical.MapPartitions
import tool._
object makeYArninfo {

  def apply(parameterTool: ParameterTool): makeYArninfo = new makeYArninfo(parameterTool)

  def main(args: Array[String]): Unit = {
    val tool = ParameterTool.fromArgs(args)
      makeYArninfo(tool).excute()
  }
}

class makeYArninfo(parameterTool: ParameterTool) extends Serializable {
  import org.apache.spark.streaming.kafka010._
  import org.apache.spark.sql.SparkSession
  import tool._
  import org.apache.kafka.clients.consumer.ConsumerRecord
  import org.apache.kafka.common.serialization.StringDeserializer
  import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
  import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
  import org.apache.spark.TaskContext


  val kafkaip = parameterTool.get("kafkaip","bigdata3:9092,bigdata4:9092,bigdata5:9092")
  val groupid = parameterTool.get("groupid","test-3")
  val offsetreset = parameterTool.get("offsetset" , "earliest")
  val topicid = parameterTool.get("topic","yarninfo")
  val mideng = parameterTool.get("mideng","timestamp")
  val url = parameterTool.get("url","jdbc:clickhouse://ip:8123/bigdata")
  val root = parameterTool.get("root","default")
  val password = parameterTool.get("password","123456")
  val driver = parameterTool.get("driver","com.clickhouse.jdbc.ClickHouseDriver")
  val dbtable = parameterTool.get("dbtable","yarninfo_zihang")
  val mode = parameterTool.get("mode","append")



  val kafkaParams = Map[String,Object](
    "bootstrap.servers" -> kafkaip, // kafka地址
  "key.deserializer" -> classOf[StringDeserializer], // 反序列化
  "value.deserializer" -> classOf[StringDeserializer], // 反序列化
  "group.id" -> groupid, // 指定消费者组
  "auto.offset.reset" -> offsetreset, // 从什么地方开始消费
  "enable.auto.commit" -> (false: java.lang.Boolean) // offset的提交 是不是自动提交
  )
  private val streamingcontext = new streamingcontext

  private val savefile = new savefile

  def excute()={

    val streaming = streamingcontext.getstreamingnocheckpoint()
    val topic = Array(topicid)
    val stream = KafkaUtils.createDirectStream(
      streaming,
      PreferConsistent,
      Subscribe[String, String](topic, kafkaParams)
    )
    // 获取offset信息
    stream.foreachRDD { rdd =>
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      println(rdd.partitions.size)
      rdd.foreachPartition { iter =>
        val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
        println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
      }
      val spark = SparkSession.builder().config(rdd.sparkContext.getConf).getOrCreate()
      import spark.implicits._
      println("----------------------------------------------")
      val wordsDataFrametmp =  rdd.map(_.value()).filter(_.nonEmpty).map(line => {
        var str:String = line
        if (line.startsWith("\r\n\r\n")){
          if (line.startsWith("\r\n\r\n")){
            str = line.replace("\r\n\r\n", "\r\n")
          }
        }
        str.split("\r\n")
      }).filter(_.nonEmpty)

      var wordsDataFrame:sql.DataFrame = null

//      def getTypeTag[T: ru.TypeTag](obj: T) = ru.typeTag[T]
//
//      val tpe = getTypeTag(wordsDataFrametmp).tpe
//
//      tpe.dealias.getClass.getFields.foreach(println(_))
//      println("---------------------------------")
//      tpe.getClass.getDeclaredFields.foreach(println(_))



//      println("\"wordsDataFrametmp的数据\" ")
//      //wordsDataFrametmp.collect().foreach(_.foreach(println(_)))
//      println("wordsDataFrametmp")
//      wordsDataFrametmp.toDF("total").show(false)
//      println("rdd.map(_.value())")
//      rdd.map(_.value()).toDF("total").show(false)
//      println("rdd.map(_.value()).map(_.split(\"\\r\\n\"))")
//      rdd.map(_.value()).map(line => {
//        var str:String = line
//          if (line.startsWith("\r\n\r\n")){
//          str = line.replace("\r\n\r\n", "\r\n")
//        }
//       str.split("\r\n")
//      }).toDF("total").show(false)




      // ------------------------------------------------------------------------------------------------------
      if(!((wordsDataFrametmp.collect().length == 1)&&(wordsDataFrametmp.collect().length == 0))){
        wordsDataFrame= wordsDataFrametmp.map(strings=>{
          val id = strings(1).split(":")(1)
          val host = strings(2).split(":")(1)
          val applicationtype = strings(3).split(":")(1)
          val name = strings(4).split("&&")(1)
          val startime = strings(5).split(":")(1)
          val endtime = strings(6).split(":")(1)
          val user = strings(7).split(":")(1)
          val memeveryscends = strings(8).split(":")(1)
          val vcoreeveryscends = strings(9).split(":")(1)
          val size = strings(10).split(":")(1).toLong
          val cores = strings(11).split(":")(1).toLong
          val state = strings(12).split(":")(1)
          val url = strings(13).split("&&")(1)
          val queue = strings(14).split(":")(1)
          val timestamp = strings(15).split("&&")(1)
          (id,host,applicationtype,name,startime,endtime,user,memeveryscends,vcoreeveryscends,size,cores,state,url,queue,timestamp)
        })
          .toDF("id","host",
            "applicationtype","name",
            "startime","endtime",
            "user","memeveryscends",
            "vcoreeveryscends","size",
            "cores","state","url","queue","timestamp")
        if (!wordsDataFrame.isEmpty){
          wordsDataFrame.show()
          savefile.savetojdbc(spark, wordsDataFrame, url , root , password,dbtable,driver,mideng,mode)
        }
      }



       //存储offset和提交offset
      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
    }
    streaming.start()
    streaming.awaitTermination()
  }


}

```

## 部署

在机器上部署添加依赖的时候可以使用--jars

如下：

```
spark-submit \
--master yarn \
--deploy-mode client \
--name 录入yarninfo \
--executor-memory 1g \
--num-executors 1 \
--executor-cores 1 \
--jars /home/hadoop/software/jar/kafka/spark-streaming-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/spark-token-provider-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/kafka-clients-2.2.1.jar,/home/hadoop/software/jar/connect/clickhouse-jdbc-0.3.2.jar,/home/hadoop/software/jar/connect/clickhouse-http-client-0.3.2.jar,/home/hadoop/software/jar/connect/clickhouse-client-0.3.2.jar,/home/hadoop/software/jar/flink/flink-clients_2.12-1.13.6.jar,/home/hadoop/software/jar/flink/flink-core-1.13.6.jar,/home/hadoop/software/jar/flink/flink-scala_2.12-1.13.6.jar,/home/hadoop/software/jar/flink/flink-java-1.13.6.jar \
--class project.makeYArninfo \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
--kafkaip namenode:9092,resourcemanager:9092,workers:9092 
-------------------------------------------------------------------
spark-submit \
--master yarn \
--name 采集yarn \
--deploy-mode client \
--executor-memory 1g \
--num-executors 1 \
--executor-cores 1 \
--jars /home/hadoop/software/jar/kafka/spark-streaming-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/spark-token-provider-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/kafka-clients-2.2.1.jar,/home/hadoop/software/jar/flink/flink-clients_2.12-1.13.6.jar,/home/hadoop/software/jar/flink/flink-core-1.13.6.jar,/home/hadoop/software/jar/flink/flink-scala_2.12-1.13.6.jar,/home/hadoop/software/jar/flink/flink-java-1.13.6.jar \
--class sparkfirst.testyarn \
--queue zihan \
/home/hadoop/project/jar/bigdatajava-1.0-SNAPSHOT.jar \
--kafkaip namenode:9092,resourcemanager:9092,workers:9092


```

或者使用maven的方式

通过参数

```
--repositories https://oss.sonatype.org/content/groups/public/ \
--packages org.apache.spark:spark-streaming-kafka_2.10:1.6.0,org.elasticsearch:elasticsearch-spark_2.10:2.2.0 \
```

就可以控制，packages里加的是依赖，上面则是maven仓库的地址

只有第一次使用的时候会下载，之后就不会了

或者直接打胖包通过插件

```
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>3.0.0</version>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
```

## xxl进行调度，报警等

然后我们把启动脚本封装到一个sh中，通过xxl进行调度

脚本如下：

```
pid=$(jps |  grep SparkSubmit | awk '{print $1}')
if [ ! -n "$pid" ];then
yarninfo.sh
ssh bigdata3 "/home/hadoop/shell/ding.sh 梅花十三 采集yarn日志 请登录查看 192.168.41.133 15046528047"
else
echo "信息正常"
fi
```

上述只是个简单的脚本，如果要真正的实时监控请自行编写
