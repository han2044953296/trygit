```
title: 实习学习进度
date: 6/25/2023 9.06 PM 
categories: 实习
comments: "true"
tag: 实习
```

# 实习学习进度

在项目中进行实习的时间已经有7天左右了，整理下实习的学习进度。

首先是技术性的，技术上学习了maven的深度使用，通过探查底层maven的方法进行学习。学习使用离线的安装工具，查找scala底层架构等等

其次就是业务性的了，业务性的是在项目里观察项目架构得出来得。

首先业务中得大数据是包括一整套处理得。一个业务项目组中有多个人，其中包括大数据技术人员，还有算法技术人员等等。

简单谈谈对于数据得处理情况。

底层数据->etl清洗数据->提炼指标->提炼特征->提炼相关专家规则->把符合规则得数据进行计算->机器学习训练模型->预测数据

我所接触的项目大概就是这样的，其中建模所使用的算法是xgbox和决策树。

# 新思想，优化spark的执行速度

对于数据量很大的数据，可以分成多个分区，对于每个分区通过多线程进行处理。可以提高spark的速度执行效率。

对于两个表join的时候可以通过对每个分区进行广播变量的方法进行。

# 学习xgboost

xgboost就是训练多棵树模型，就是在训练了一棵树的基础上再往下训练树，让多个树模型的值相加达到越来越拟合实际情况的树。以便可以预测。而且xgboost可以通过多种代码实现，其中包括python，scala，c等等。

其中xgboost包含了两个比较重要的点：正则项，以及损失函数，作为xgboost的目标函数，这也是他和GBDT较大的区别。

正则项的目的是限制树的节点，防止过拟合，减轻树的复杂度等等。

损失函数的意义是代表模型拟合数据的程度，通常用他的一阶导数指出xgboost的梯度下降的方向。但是xgboost还计算了其二阶导数，更精准。

scala代码如下

```
import ml.dmlc.xgboost4j.scala.DMatrix
import ml.dmlc.xgboost4j.scala.XGBoost

object XGBoostScalaExample {
  def main(args: Array[String]) {
    // 读取 xgboost/demo/data 目录中可用的训练数据
    val trainData =
      new DMatrix("/path/to/agaricus.txt.train")
    // 定义参数
    val paramMap = List(
      "eta" -> 0.1,
      "max_depth" -> 2,
      "objective" -> "binary:logistic").toMap
    // 迭代次数
    val round = 2
    // train the model
    val model = XGBoost.train(trainData, paramMap, round)
    // 预测
    val predTrain = model.predict(trainData)
    // 保存模型至文件
    model.saveModel("/local/path/to/model")
  }
}

```

其中paramMap里的参数是有意义的，如下：

```
General Parameters
booster [default=gbtree]
有两中模型可以选择gbtree和gblinear。gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算。缺省值为gbtree。

silent [default=0]
取0时表示打印出运行时信息，取1时表示以缄默方式运行，不打印运行时信息。缺省值为0。

nthread [default to maximum number of threads available if not set]
XGBoost运行时的线程数。缺省值是当前系统可以获得的最大线程数

num_pbuffer [set automatically by xgboost, no need to be set by user]
size of prediction buffer, normally set to number of training instances. The buffers are used to save the prediction results of last boosting step.

num_feature [set automatically by xgboost, no need to be set by user]
boosting过程中用到的特征维数，设置为特征个数。XGBoost会自动设置，不需要手工设置。

Booster Parameters
eta [default=0.3]
为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为 0.3
取值范围为：[0,1]

gamma [default=0]

minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.
range: [0,∞]
max_depth [default=6]

数的最大深度。缺省值为6
取值范围为：[1,∞]
min_child_weight [default=1]

孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该成熟越大算法越conservative
取值范围为: [0,∞]
max_delta_step [default=0]

Maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update
取值范围为：[0,∞]
subsample [default=1]

用于训练模型的子样本占整个样本集合的比例。如果设置为0.5则意味着XGBoost将随机的冲整个样本集合中随机的抽取出50%的子样本建立树模型，这能够防止过拟合。
取值范围为：(0,1]
colsample_bytree [default=1]

在建立树时对特征采样的比例。缺省值为1

取值范围：(0,1]

Task Parameters
objective [ default=reg:linear ]
定义学习任务及相应的学习目标，可选的目标函数如下：

“reg:linear” –线性回归。
“reg:logistic” –逻辑回归。
“binary:logistic”–二分类的逻辑回归问题，输出为概率。
“binary:logitraw”–二分类的逻辑回归问题，输出的结果为wTx。
“count:poisson”–计数问题的poisson回归，输出结果为poisson分布。在poisson回归中，max_delta_step的缺省值为0.7。(used to safeguard optimization)
“multi:softmax” –让XGBoost采用softmax目标函数处理多分类问题，同时需要设置参数num_class（类别个数）
“multi:softprob” –和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。
“rank:pairwise”–set XGBoost to do ranking task by minimizing the pairwise loss
base_score [ default=0.5 ]

the initial prediction score of all instances, global bias

eval_metric [ default according to objective ]
校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标（rmse for regression, and error for classification, mean average precision for ranking）
用户可以添加多种评价指标，对于Python用户要以list传递参数对给程序，而不是map参数list参数不会覆盖’eval_metric’

The choices are listed below:

“rmse”: root mean square error
“logloss”: negative log-likelihood
“error”: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.
“merror”: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).
“mlogloss”: Multiclass logloss
“auc”: Area under the curve for ranking evaluation.
“ndcg”:Normalized Discounted Cumulative Gain
“map”:Mean average precision
“ndcg@n”,”map@n”: n can be assigned as an integer to cut off the top positions in the lists for evaluation.
“ndcg-”,”map-”,”ndcg@n-”,”map@n-”: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. training repeatively
“gamma-deviance”: [residual deviance for gamma regression]
seed[ default=0 ]

random number seed.
随机数的种子。缺省值为0
dtrain：训练的数据

num_boost_round：这是指提升迭代的次数，也就是生成多少基模型

evals：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals = [(dtrain,‘train’),(dval,‘val’)]或者是evals = [(dtrain,‘train’)]，对于第一种情况，它使得我们可以在训练过程中观察验证集的效果

obj：自定义目的函数

feval：自定义评估函数

maximize：是否对评估函数进行最大化

early_stopping_rounds：早期停止次数 ，假设为100，验证集的误差迭代到一定程度在100次内不能再继续降低，就停止迭代。这要求evals 里至少有 一个元素，如果有多个，按最后一个去执行。返回的是最后的迭代次数（不是最好的）。如果early_stopping_rounds存在，则模型会生成三个属性，bst.best_score，bst.best_iteration和bst.best_ntree_limit

evals_result：字典，存储在watchlist中的元素的评估结果。

verbose_eval ：(可以输入布尔型或数值型)，也要求evals里至少有 一个元素。如果为True,则对evals中元素的评估结果会输出在结果中；如果输入数字，假设为5，则每隔5个迭代输出一次。

learning_rates：每一次提升的学习率的列表，

xgb_model：在训练之前用于加载的xgb model。
```

讲到这里就要讲讲关于objective的选择了

```
1. 线性回归Linear regression

线性回归是最典型的回归类型，大约250年前就已出现，也被称为普通最小二乘法(OLS)和线性最小二乘法回归。可以使用它对小数据集进行计算，甚至可以手动计算。目前线性回归常用于插值，但不适合实际预测和主动分析。

另外，现代数据常常结构混乱，线性回归容易“滞后”：线性回归过于精确。如果模型对一组数据计算精确，对另一组数据却极不精确，而线性回归本应描述一般模式，过于精确会使其在几乎所有情况下变得不稳定。

2. 岭回归Ridge regression

岭回归是线性回归的重要改进，增加了误差容忍度，对回归系数进行了限制，从而得到更加真实的结果，并且结果更容易解释。该方法用于解决自变量之间相互关联(多重共线性)时的数据冗余问题。

3.套索回归Lasso-regression

套索回归与岭回归类似，但回归系数可为0（模型中排除了一些符号）

4. 偏最小二乘法回归Partial least squares（PLS）

与自变量数目相比，观察结果很少时，或者自变量高度相关时，PLS会很有用。PLS可将自变量减少，并使其不相关，类似于主成分分析。然后，对这些自变量而非原始数据进行线性回归。

PLS强调发展预测模型，不用于筛选变量。与OLS不同，PLS可以包含多个连续因变量。PLS利用相关结构识别较小的效应，并对因变量中的多元模式进行建模。

5. 逻辑回归Logistic regression

逻辑回归广泛应用于临床试验、量化，或者欺诈分析——当测试药物或信用卡交易的信息可以二进制形式(是/否)获得时。线性回归固有的缺点它也有，如低误差容忍度、依赖数据集，但总的来说，逻辑回归更好，并且可以简化为线性回归类型来简化计算。有些版本如泊松回归得到了改进，以便有时需要得到非二进制答案，例如分类、年龄组、甚至回归树。

6. 生态回归 Ecological Regression

生态回归用于将数据划分为相当大的层或组的情况(回归分别应用于每个层或组)，例如，在政治学中生态回归用于根据汇总数据评估选民的群体行为。

然而，应该警惕“大数据的诅咒”：如果对数百万次回归进行统计，其中一些模型可能完全不准确，成功的模型将被高度(且人为)一致的嘈杂模型“击溃”。因此，这种类型的回归不适合预测极端事件(地震)和研究因果关系(全球变暖)。

7.贝叶斯线性回归Bayesian linear regression

贝叶斯线性回归与岭回归类似，但它的前提是所有可能的误差都服从正态分布。因此，假设对数据结构有基本了解，就可能获得更精确的模型(特别是与线性回归相比)。

然而，在实际操作中，若处理大数据，对数据的初始了解并不能保证准确性，所以这种假设是基于共轭值的，即本质上是人为的，这是这种回归类型的一个显著缺陷。

8. 分位数回归Quantile regression

分位数回归用于极端事件，包括故意在结果中引入偏差，从而提高模型的准确性。

9. 最小绝对偏差Least absolute deviations（LAD）

最小绝对偏差也称为最小绝对误差（LAE）、最小绝对值（LAV）、最小绝对残差（LAR）、绝对偏差之和或L1范数条件，是最小的模量方法。它用于从包含随机误差的测量值中评估未知值，以及估算给定函数的表示法（近似值）。最小绝对偏差看起来像线性回归，但使用的是绝对值而不是平方。因此，模型的准确性有所提高，且没有使计算复杂化。

10. 刀切法重采样Jackknife resampling（大折刀法）

刀切法重采样是一种用于聚类和数据细化的新型回归方法。这种方法不具有典型回归类型的缺点，能为回归问题提供近似但非常准确、抗误差的解决方案，自变量相关或不“服从”正态分布时都可使用。

这种类型的回归很适合黑盒类型预测算法，它非常接近线性回归，没有精度损失，即使传统回归假设（变量不相关、数据正态分布、条件方差恒定）由于数据性质不被接受，它依旧可以使用。

· 如果模型需要连续的因变量：

线性回归是最常见和最直接的使用类型。如果有一个连续的因变量，可能要首先考虑线性回归模型。然而，要注意线性回归的几个缺点，如对异常值和多重共线性很敏感。在这种情况下，最好使用更高级的线性回归变体，如岭回归、套索回归和偏最小二乘法回归(PLS)。

· 如果模型需要分类因变量：

应使用逻辑回归。这种模型最适合二元因变量。在进行更复杂的分类建模之前，最好先使用这种模型。分类变量的有些值可以根据特征放入可计数的不同组中。逻辑回归对因变量进行变换，然后使用最大似然估计法而非最小二乘法来估计参数。

· 如果模型需要计数因变量：

应使用泊松回归。计数数据往往遵循泊松分布，因此泊松回归很适合。使用泊松变量可以计算和评估发生率。

```

# 黑白样本&稀疏矩阵

黑样本是指由风险的样本，白样本则是表示安全的样本。一般来说训练模型的时候如果样本数量偏向一方的特别多会造成误差。一般的比例是3：1，所以为了解决要通过稀疏矩阵来进行处理样本偏差过多的就是样本中0或者空值站的比较多的。用稀疏矩阵训练模型会让训练的时间减少很多。

当数据样本特别不均衡的时候如何解决？

当数据样本特别不均衡的时候，我们要采取一些方法让样本均衡。

如：减少大样本的数量或者增加小样本的数量。

采取的方法有如下几种：

## 扩充小样本数据集。

对数据进行重采样：

大样本进行欠采样。

小样本进行过采样。

但是上述的做法会让训练出来的模型预测结果不太准确。不过胜在容易实现。

## 人造数据

对于小样本进行人造数据。把小样本的每种取值封装到一个list中，然后进行造数据。但是这样会有可能改变原数据集的数据曲线，很不推荐。可能会产生现实中不会存在的数据。

但是有一种造数据的方法。叫SMOTE方法。他是一种过采样的方法。他是构建新的小样数据而不是制作已有的小样数据的副本。就是相当于选择一条数据。进行每次从和他相似的数据中选择一个属性给这个数据替换上。就可以制造很多数据了。

## 改变分类算法

使用代价函数时，可以增加小类样本的权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。

可以把小类样本作为异常点(outliers)，把问题转化为异常点检测问题(anomaly detection)。此时分类器需要学习到大类的决策分界面，即分类器是一个单个类分类器（One Class Classifier）

由Robert E. Schapire提出的”The strength of weak learnability”方法，该方法是一个boosting算法，它递归地训练三个弱学习器，然后将这三个弱学习器结合起形成一个强的学习器。算法流程如下：
•首先使用原始数据集训练第一个学习器L1。
•然后使用50%在L1学习正确和50%学习错误的那些样本训练得到学习器L2，即从L1中学习错误的样本集与学习正确的样本集中，循环采样一边一个。
•接着，使用L1与L2不一致的那些样本去训练得到学习器L3。
•最后，使用投票方式作为最后输出。
那么如何使用该算法来解决数据不均衡问题呢？ 假设是一个二分类问题，大部分的样本都是true类。
•让L1输出始终为true。
•使用50%在L1分类正确的与50%分类错误的样本训练得到L2，即从L1中学习错误的样本集与学习正确的样本集中，循环采样一边一个。因此，L2的训练样本是平衡的。
•接着使用L1与L2分类不一致的那些样本训练得到L3，即在L2中分类为false的那些样本。
•最后，结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则true。

以下方法同样会破坏某些类的样本的分布：
•设超大类中样本的个数是极小类中样本个数的L倍，那么在随机梯度下降（SGD，stochastic gradient descent）算法中，每次遇到一个极小类中样本进行训练时，训练L次。
•将大类中样本划分到L个聚类中，然后训练L个分类器，每个分类器使用大类中的一个簇与所有的小类样本进行训练得到。最后对这L个分类器采取少数服从多数对未知类别数据进行分类，如果是连续值（预测），那么采用平均值。
•设小类中有N个样本。将大类聚类成N个簇，然后使用每个簇的中心组成大类中的N个样本，加上小类中所有的样本进行训练。
如果不想破坏样本分布，可以使用全部的训练集采用多种分类方法分别建立分类器而得到多个分类器，投票产生预测结果。

## 通过用其他评价指标
