```
title: pyspark
date: 13/6/2023 11:16:34 AM 
categories: 日志
comments: "true"
tag: pyspark开始
```

本节是通过python链接spark。工作单位很多人都在用pyspark。于是就学习下pyspark，并记录笔记

他的底层原理和scala的spark是一样的，而且spark是scala编写的，其实用scala能更好的控制spark。

通过pycharm创建项目的时候引入python的虚拟环境出现问题报错内容为：*`Pipenv executable is not found`*

原因是因为python的虚拟环境没有安装，确认当前机器的py环境的位置后，`win + R => cmd 输入 pip install pipenv` 就可以了

初始项目构建如下：

```python
from pyspark import SparkContext
sc = SparkContext("local", "count app")
words = sc.parallelize(
    ["scala",
     "java",
     "hadoop",
     "spark",
     "akka",
     "spark vs hadoop",
     "pyspark",
     "pyspark and spark"
     ])
counts = words.count()
print("Number of elements in RDD -> %i" % counts)
```
