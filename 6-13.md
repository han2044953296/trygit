```
title: spark-sql
date: 13/6/2023 9:11:14 AM 
categories: 日志
comments: "true"
tag: spark-sql优化
```

数据量过大时，spark-sql也会跑不动。针对sql进行优化会好很多。

sql优化分为以下几个方案。

# 任务拆解（人为分解sql）

    通过人为分解sql，可以增强sql查询的速度。人为构建子查询，减少子查询的数据量，在left join的时候因为sql查询的语句执行顺序的不同。所以可以通过构建子查询的方法进行减少left join 的数据量。sql查询的执行顺序如下：

```
(8)SELECT (9)DISTINCT <select_list>
(1)FROM <left_table>
(3)<join_type>JOIN <right_table>
(2)ON<join_condition>
(4)WHERE<where_condition>
(5)GROUP BY<group_by_list>
(6)WITH {CUBE|ROLLUP}
(7)HAVING<having_condition>
(10)ORDER BY<order_by_list>
(11)LIMIT<limit_number>
```

# json数据

    对于json数据的sql提速可以使用schema进行拆解数据实现。如下

```
    with tmp as(
        select  
            explode( from_json(get_json_object(eparam, '$._d'), 'array< struct<`_origin`: string, `_re`: int, `_se`: string, `_ue`: int  > >')) element
        from table_name
        where dt='20210403' and etype='10257001' and ts%5=0
    )
    select 
    count(1)
    from tmp;
```

json的模式匹配可以选择的种类有rlike，like，instr，get_json_object等，其中的速度按照测试结果来看是like最快，其次是rlike，再其次是instr，最后是get_json_object。如下：

```
1) rlike 方式 6.1min
    select
        dt, '10410007' etype, count(distinct gazj)
    from table_name
    where dt like '202011%'
    and etype='10410007'
    and eparam rlike '"_pkg":"net.bat.store"'
    group by dt;

   2) like 方式 6min
    select
        dt, '10410007' etype, count(distinct gazj)
    from table_name
    where dt like '202011%'
    and etype='10410007'
    and eparam like '%"_pkg":"net.bat.store"%'
    group by dt;

   3) instr方式 8min
    select
       dt, '10410007' etype, count(distinct gazj)
    from table_name
    where dt like '202011%'
    and etype='10410007'
    and instr(eparam, '"_pkg":"net.bat.store"')!=0
    group by dt;

   4) 解析json方式 15min
    select
        dt, '10410007' etype, count(distinct gazj)
    from table_name
    where dt like '202011%'
    and etype='10410007'
    and get_json_objet(eparam,'$._pkg')='net.bat.store'
    group by dt;
```

    解析json字段的提速方法。解析json字段一般是通过get_json_object/json_touple,但是经过测试json_touple的速度要快于get_object_json.但是json_touple的使用也有限制，json_touple只能解析二级json，多级嵌套json则要用get_json_object进行解析。

# hints写法

    spark有关于join的hints写法可以让数据量比较大的时候执行速度变快。总体分为几种：mapjoin类，merge类，shuffle_hash类，SHUFFLE_REPLICATE_NL类

下面分别介绍：

## mapjoin类

mapjoin是指在map端开启join，在不经过shuffle的时候进行join，可以有效的避免数据倾斜，他的本质是把小表当作广播变量广播出去，让大表的每一个分区都接收到数据，然后进行join。api的使用如下：

```
import session.implicits._
// table1: a列为key, b,c组合列为value, 此为小表, 需要调整为(key, value)形式
val table1 = sc.textFile(table1Path).map(
            line => {
                val lines = line.split("\t")
                val a = lines(0)
                val b = lines(1)
                val c = lines(2)
                val others = Array(b, c)
                (a, others.mkString(","))
            }
        ).cache()

// table2: a列为key, d列为value, 此为大表,
val table2 = sc.textFile(table2Path).map(
            line => {
                val lines = line.split("\t")
                val a = lines(0)
                val d = lines(1)
                (a, d)
            }
        ).cache()

val factor = (a: String) => { Array(a.split(",")(0), a.split(",")(1)) }
val StringToArrayUDF = udf(factor)

// 广播小表, sc为SparkContext实例, session.sparkContext
val table1Broad = sc.broadcast(table1.collectAsMap())

// toDF() 函数为隐式转换, 需要添加 import session.implicits._
val result = table2.mapPartitions(x => {
    // 获取广播变量
    val table1BroadValue: scala.collection.Map[String, String] = table1Broad.value
    for ((key, value) <- x if table1BroadValue.contains(key))
    	yield (key, table1BroadValue.get(key), value)
    // 列名为 (_1, _2, _3)
    // 列对应于 (a, (b, c), d)
    }).toDF().withColumn("cols", StringToArrayUDF($"_2"))
		.select(
            $"_1".as("a"),
            $"cols".getItem(0).as("b"),
            $"cols".getItem(1).as("c"),
            $"_3".as("d"),
        )
})
// 对result进行其他操作
result.show(20, truncate = false)

```

    提高shuffle的效率的方法。通过下面几个参数调优。

```
spark.shuffle.file.buffer
spark.reducer.maxSizeInFlight
spark.shuffle.compress
----------------------------------------
第一个配置是Map端输出为中间结果的缓冲区大小，默认为32KB。

第二个配置是Map端输出为中间结果的文件大小，默认为48 MB，该文件还会与其他文件进行合并。

第三个配置是Map端输出是否开启压缩，默认开启。
```

缓冲区当然越大，写入性能越高，所以有条件可以增大缓冲区大小，提升Shuffle Write的性能。

Spark Shuffle会将中间结果写到spark.local.dir配置的目录下，可以将该目录配置多路磁盘目录，以提升写入性能。

另外一个办法是尽量减少shuffle或者降低shuffle数据量。

hints写法如下：

```
SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ BROADCASTJOIN (t1) */ * FROM t1 left JOIN t2 ON t1.key = t2.key;
SELECT /*+ MAPJOIN(t2) */ * FROM t1 right JOIN t2 ON t1.key = t2.key;
```

# shuffle_hash类

其实也叫reduce join `Reduce Join` 包含Map、Shuffle、Reduce阶段，而 `join` 会在 `reduce` 阶段完成，故称为 `Reduce Join`。很可能会造成数据倾斜。所以一般不用。

hints写法如下：

```
SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
```

# merge类

merge类的join其实主要是short merge join 因为上述的mapjoin和reducejoin都是要把一侧数据完全加载之后才开始进行join的。但是他不用，他是排序的方法把两边的数据先排序，然后直接join。

hints写法如下：

```
SELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ MERGEJOIN(t2) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
```

# 同时指定多个join

```
-- When different join strategy hints are specified on both sides of a join, Spark
-- prioritizes the BROADCAST hint over the MERGE hint over the SHUFFLE_HASH hint
-- over the SHUFFLE_REPLICATE_NL hint.
-- Spark will issue Warning in the following example
-- org.apache.spark.sql.catalyst.analysis.HintErrorLogger: Hint (strategy=merge)
-- is overridden by another hint and will not take effect.
SELECT /*+ BROADCAST(t1), MERGE(t1, t2) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
```

# 笛卡尔积join

一般不用就是SHUFFLE_REPLICATE_NL类

hints写法如下：

```
SELECT /*+ SHUFFLE_REPLICATE_NL(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
```
