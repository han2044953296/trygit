---
title: 11-19作业
date: 11/19/2022 11:13:45 AM  
categories: 作业
comments: "true"
tag: hive
---
# 通过mapreduce的api统计emp表里的每个工作的人数

- 数据源

```

	7369,SMITH,CLERK,7902,1980-12-17 14:00:00,800.00,,20
	7499,ALLEN,SALESMAN,7698,1981-02-20 14:00:00,1600.00,300.00,30
	7521,WARD,SALESMAN,7698,1981-02-22 14:00:00,1250.00,500.00,30
	7566,JONES,MANAGER,7839,1981-04-02 14:00:00,2975.00,,20
	7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,1250.00,1400.00,30
	7698,BLAKE,MANAGER,7839,1981-05-01 13:00:00,2850.00,,30
	7782,CLARK,MANAGER,7839,1981-06-09 13:00:00,2450.00,,10
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10
	7844,TURNER,SALESMAN,7698,1981-09-08 13:00:00,1500.00,0.00,30
	7876,ADAMS,CLERK,7788,1983-01-12 14:00:00,1100.00,,20
	7900,lebulang,CLERK,7698,1981-12-03 14:00:00,950.00,,30
	7902,FORD,ANALYST,7566,1981-12-03 14:00:00,3000.00,,20
	7934,MILLER,CLERK,7782,1982-01-23 14:00:00,1300.00,,10
	7839,KING,PRESIDENT,,1981-11-17 14:00:00,5000.00,,10
	7654,MARTIN,SALESMAN,7698,1981-09-28 13:00:00,3200.00,1400.00,30
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20
	7788,SCOTT,ANALYST,7566,1982-12-09 14:00:00,3000.00,,20


```

- 其中第二列就是工作
- 代码如下 ：

```

	package org.example;


	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.fs.Path;
	import org.apache.hadoop.io.IntWritable;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapreduce.Job;
	import org.apache.hadoop.mapreduce.Mapper;
	import org.apache.hadoop.mapreduce.Reducer;
	import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
	import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

	import java.io.IOException;
	import java.util.StringTokenizer;

	/**
	 * @author sxwang
	 * 11 18 14:00
	 */
	public class test {

	    /**
	     * driver
	     * @param args
	     */
	    public static void main(String[] args) throws Exception {

	        String input="D://emp.txt";
	        String output="out";
	        Configuration conf = new Configuration();
	        //0.todo... 删除目标路径
	        FileUtils.deletePath(conf,output);

	        //1.设置 作业名称
	        Job job = Job.getInstance(conf, "WCAPP");
	        //2.设置map reduce 执行代码的主类
	        job.setJarByClass(test.class);
	        job.setMapperClass(MyMapper.class);
	        job.setReducerClass(MyReducer.class);
	        //3.指定 oupput kv类型
	        job.setOutputKeyClass(Text.class);
	        job.setOutputValueClass(IntWritable.class);
	        //4. 设置数据源路径 数据输出路径
	        FileInputFormat.addInputPath(job, new Path(input));
	        FileOutputFormat.setOutputPath(job, new Path(output));
	        //5. 提交mr yarn
	        System.exit(job.waitForCompletion(true) ? 0 : 1);

	    }

	    /**
	     * mapper
	     */
	    public static class MyMapper
	            extends Mapper<Object, Text, Text, IntWritable> {

	        @Override
	        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
	            /**
	             * 1.按照分隔符 进行拆分 每个单词 ，每个单词赋值为1
	             * (word ,1)
	             */

	            String[] words = value.toString().split("\n");

	            for (String word : words) {
	                String[] split = word.split(",");
	                context.write(new Text(split[2]) ,new IntWritable(1));
	            }

	        }
	    }

	    /**
	     * reducer
	     */
	    public static class MyReducer
	            extends Reducer<Text,IntWritable,Text,IntWritable> {

	        /**
	         *  (word ,1)
	         *
	         *  (word,<1,1,1,1>)
	         *
	         *  1.聚合value
	         *
	         *  2.写出去
	         *  (word ,3)
	         */
	        @Override
	        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
	            int sum=0;
	            for (IntWritable value : values) {
	                sum +=Integer.parseInt(value.toString());
	            }
	            context.write(key,new IntWritable(sum));
	        }
	    }
	}

```

- 基本概念 ： 把数据先通过map进行etl，然后通过redurce进行数据的整合之类的
- 最后输出
