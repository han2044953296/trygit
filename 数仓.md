---
title: 数仓
date: 2-13
categories: 日志
comments: "true"
tag: 数仓
---
# 简介

正常的数据：

* 日志
* 业务
* 其他

数仓可以做的功能：

* 数据报表：显示数据
* 用户画像：就是按照消费能力分组
* 做推荐系统
* 机器/深度学习 ： 预测走向
* 风控：风险控制 -> 例子：薅羊毛太多进黑名单

电商行业为列：

* 展示商品都是业务数据->存放在业务数据库 mysql/es
* 用户与页面交互的时候发送的数据，据说作用在html上的操作数据，甚至是鼠标停留时间
* 其他数据包括-> 线下的一些行为，比如签单等，或者excel，hdfs，redis，mongo等

数据仓库：

* 数据采集
* 数仓建设 0-1
* 数据可视化

技术选型：

* 数据采集：flume日志，业务sqoop，datax，自己开发
* 数据存储：hdfs，hbase，mysql，clickhouse，drios
* 数据计算：hive，spark，flink，hbase-> phonix
* 数据可视化：superset/dataeases echarts quickbi datav sugar
* 任务调度：xxl dolphin airflow azkzban oozie 自己开发
* 报警系统：手写钉钉报警 微信 电话 邮箱 zabbix 普罗米修斯
* 元数据管理：任务元数据 管理 -> 可做可不做 atlas datahub 自己开发 -> 看airflow里或者原生的antlr4/calicete
* 临时查询工具：ad-hoc 即席查询 -> presto ,sparksql ,kylin ,impala,druid ,phonix

impala + kudu => 数仓

hive =>  数仓

hbase => 数仓

数据湖 => 数仓

流程 ：

app 产生数据 ：

* 业务数据 ：sqoop/datax/自己开发的 -> hive/hdfs -> 数据分层 -> myqsl/Ck -> 可视化
* 日志数据 ：用户触发埋点则产生日志数据 -> 存到日志落盘机 -> flume -> hdfs（离线）/kafka-flume-hdfs(半实时) -> 数据分层-> mysql/Ck -> 可视化

离线数仓：我们要做的

* 数据采集
* 数仓搭建
* 数据质量监控

离线数仓：

* 数据分层
  * ods operation data store => 原数据层
  * dw 层
    * dwd data warehouse detail： 数据明细层 etl
    * dwm data warehouse middle：去重以及确定指标需求
    * dws  data warehouse serive:   构建宽表的
  * ods/rpt层：application data store -> 数据应用层
    * 出结果的基于dws层
      * 各个维度的结果表
      * 维度聚合形成的结果表
  * dim 维度层
    * 业务字段不够，要补充的表的位置

为什么要进行数据分层：

* 复杂问题简单化
* 增加数据复用性
* 减少代码运行时间
* 隔离原始数据 ： 个别公司访问数据有权限
* 校验数据

数据仓库的分层不是越多越好，合理设计最重要

# 离线数仓

关于离线数仓：一般是t+1的业务周期性就是下一天处理上一天的业务

## 技术选型

sparksql，shell，xxl，spark，flume等

通过shell脚本来执行sql语句

* 优点：好维护，好建设
* 缺点：从0到1比较繁琐

## ods层

首先ods层属于原始数据层，在这一层我们的任务就是对数据进行过滤，把他们放到不同的表里，当然你也可以简单的做一些处理

### 日志数据

关于日志数据一般都是通过埋点等方法进行采集的数据通过flume采集，我们下面要把他们分成活动日志，展现日志等

对于flume我进行了一个封装的脚本，可以更方便的使用flume插件，语言是scala的

```java
package progectfinally
import java.io.FileWriter

import org.apache.flink.api.java.utils.ParameterTool
import org.apache.spark.sql.SparkSession

object flume {

  def apply(parameterTool: ParameterTool): flume = new flume(parameterTool)

  def main(args: Array[String]): Unit = {
    val tool = ParameterTool.fromArgs(args)
    flume(tool).makeflumeconf()
  }
}

class flume(parameterTool: ParameterTool) {


  // total
  val Source = parameterTool.getRequired("Source")
  val Sink = parameterTool.getRequired("Sink")
  val Channel = parameterTool.getRequired("Channel")
  val filename = parameterTool.getRequired("filename")
  // taildir
  val taildir_position = parameterTool.get("position", "/home/hadoop/data/flumepostion/taildir_position.json")
  val filegroups = parameterTool.get("filegroups", "f1")
  val maxBatchCount = parameterTool.get("maxBatchCount", "")
  val fileHeadertaildir = parameterTool.get("fileHeadertaildir", "true")
  val filepathtaildir = parameterTool.get("filepathtaildir","")
  // avroSource
  val bind = parameterTool.get("bind", "")
  val port = parameterTool.get("port", "")
  // spooldirSource
  val spoolDir = parameterTool.get("spoolDir", "")
  val fileHeader = parameterTool.get("fileHeader", "false")
  // execSource
  val command = parameterTool.get("command", "")
  // kafka
  val bootstrap = parameterTool.get("bootstrap", "")
  val topics = parameterTool.get("topics", "")
  val groupid = parameterTool.get("groupid", "")
  val batchSize = parameterTool.get("batchSize", "")
  val batchDurationMillis = parameterTool.get("batchDurationMillis", "")

  // mem
  val capacity = parameterTool.get("capacity", "10000")
  val transactionCapacity = parameterTool.get("transactionCapacity", "10000")

  // file
  val checkpointDir = parameterTool.get("checkpointDir", "")
  val dataDirs = parameterTool.get("dataDirs", "")

  // hdfsSink
  val hdfspath = parameterTool.get("hdfspath", "/flume")
  val filePrefix = parameterTool.get("filePrefix", "")
  val fileSuffix = parameterTool.get("fileSuffix", "")
  val writeFormat = parameterTool.get("writeFormat", "Text")
  val rollSize = parameterTool.get("rollSize", "134217728")
  val rollInterval = parameterTool.get("rollInterval", "21600")
  val rollCount = parameterTool.get("rollCount", "1000")
  val round = parameterTool.get("round", "true")
  val roundUnit = parameterTool.get("roundUnit", "minute")
  val roundValue = parameterTool.get("roundValue", "21")
  val useLocalTimeStamp = parameterTool.get("useLocalTimeStamp", "true")
  val codeC = parameterTool.get(" codeC", "")
  val fileType = parameterTool.get("fileTypefileType", "")
  val batchSizehdfs = parameterTool.get("batchSizehdfs", "1200")

  // avroSink
  val hostname = parameterTool.get("hostname", "")
  val portSink = parameterTool.get("portSink", "")
  // hiveSink
  val serializer = parameterTool.get("serializer", "DELIMITED")
  val metastore = parameterTool.get("metastore", "thrift://bigdata3:9083")
  val database = parameterTool.get("database", "")
  val table = parameterTool.get("table", "")
  val delimiter = parameterTool.get("delimiter", "")
  val serdeSeparator = parameterTool.get("serdeSeparator", "")
  val fieldnames = parameterTool.get("fieldnames", "")
  // KafkaSink
  val topicSink = parameterTool.get("topicSink", "")
  val brokerList = parameterTool.get("brokerList", "bigdata3:9092,bigdata4:9092,bigdata5:9092")
  val requiredAcks = parameterTool.get("requiredAcks", "-1")
  val batchSizeSink = parameterTool.get("batchSizeSink", "10000")
  // 阻断器
  val interceptors = parameterTool.get("interceptors", "")
  val interceptorskey = parameterTool.get("key", "")
  val interceptorsvalue = parameterTool.get("value", "")
  // 决策
  val maxpenalty = parameterTool.get("maxpenalty","")
  val priority = parameterTool.get("priority", "")
  val processortype = parameterTool.get("processortype", "")
  val selectortype = parameterTool.get("selectortype","")
  // 第三方jar包
  val classpath = parameterTool.get("classpath","")



  // 定准备工作
  val preSource = "a1.sources = "
  val preSink = "a1.sinks = "
  val preChannel = "a1.channels = "



  def makeflumeconf() = {
    // 获取source，Sink，Channel列表
    val SourceList = Source.split(",")
    val SinkList = Sink.split(",")
    val ChannelList = Channel.split(",")

    /*
    tmpconf : 存储排列好的字符串
    tmpSource : 存储Source
    tmpChannel ： 存储目前Channel的字符串
    tmpSink ： 同理
     */
    var tmpconf = ""
    var tmpSource = ""
    var tmpChannel = ""
    var tmpSink = ""


    /*
    拼接总体信息
     */
    for (i <- 1 to SourceList.length) {
      tmpSource = tmpSource + "r" + i + " "
    }

    tmpconf = tmpconf + preSource + tmpSource + "\n"

    for (i <- 1 to ChannelList.length) {
      tmpChannel = tmpChannel + "c" + i + " "
    }
    tmpconf = tmpconf + preChannel + tmpChannel + "\n"

    for (i <- 1 to SinkList.length) {
      tmpSink = tmpSink + "k" + i + " "
    }

    // 传入tmpconf
    tmpconf = tmpconf + preSink + tmpSink + "\n"


    // 清空tmpSource内容
    tmpSource = ""

    // 定义各种标签
    var avroflag = 0
    var netcatflag = 0
    var spooldirflag = 0
    var execflag = 0
    var TAILDIRflag = 0
    var kafkaflag = 0
    //    println(SourceList.filter(_.contains("avro")).length)
    // source循环判断
    for (i <- 1 to SourceList.length) {
      SourceList(i - 1) match {
        case "avro" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = ${SourceList(i - 1)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.bind = ${bind.split(",")(avroflag)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.port = ${port.split(",")(avroflag)}" + "\n"
          avroflag = avroflag + 1
        }
        case "netcat" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = ${SourceList(i - 1)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.bind = ${bind.split(",")(netcatflag)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.port = ${port.split(",")(netcatflag)}" + "\n"
          netcatflag = netcatflag + 1
        }
        case "spooldir" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = ${SourceList(i - 1)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.spoolDir = ${spoolDir.split(",")(spooldirflag)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.fileHeader = ${fileHeader}" + "\n"
          spooldirflag = spooldirflag + 1
        }
        case "exec" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = ${SourceList(i - 1)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.command = ${command.split(",")(execflag)} " + "\n"
          execflag = execflag + 1
        }
        case "taildir" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = ${SourceList(i - 1)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.positionFile = ${taildir_position.split(",")(TAILDIRflag)} " + "\n"

          if (SourceList.filter(_.contains("taildir")).size == 1){
            for (j <- 0 to filegroups.split(",").length - 1){
              tmpSource = tmpSource + s"a1.sources.r${i}.filegroups = ${filegroups.split(",")(j)}" + "\n"
              tmpSource = tmpSource + s"a1.sources.r${i}.filegroups.${filegroups.split(",")(j)} = ${filepathtaildir.split(",")(j)}" + "\n"
            }
          }else {
            tmpSource = tmpSource + s"a1.sources.r${i}.filegroups = ${filegroups.split(",")(TAILDIRflag)}" + "\n"
            tmpSource = tmpSource + s"a1.sources.r${i}.filegroups.${filegroups.split(",")(TAILDIRflag)} = ${filepathtaildir.split(",")(TAILDIRflag)}" + "\n"
          }


          TAILDIRflag = TAILDIRflag + 1
        }
        case "kafka" => {
          tmpSource = tmpSource + s"a1.sources.r${i}.type = org.apache.flume.source.kafka.KafkaSource" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.kafka.bootstrap.servers = ${bootstrap.split("--")(kafkaflag)} " + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.kafka.topics = ${topics.split(",")(kafkaflag)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.kafka.consumer.group.id = ${groupid.split(",")(kafkaflag)}" + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.batchSize = ${batchSize.split(",")(kafkaflag)} " + "\n"
          tmpSource = tmpSource + s"a1.sources.r${i}.batchDurationMillis = ${batchDurationMillis.split(",")(kafkaflag)}" + "\n"
          kafkaflag = kafkaflag + 1
        }
        case _ => println("无匹配的Source")
      }
    }

    tmpconf = tmpconf + tmpSource
    //同上
    tmpChannel = ""
    var memoryflag = 0
    var flieflag = 0
    for (i <- 1 to ChannelList.length) {
      tmpChannel = tmpChannel + s"a1.channels.c${i}.type = ${ChannelList(i - 1)}" + "\n"
      ChannelList(i - 1) match {
        case "memory" => {
          tmpChannel = tmpChannel + s"a1.channels.c${i}.capacity = ${capacity.split(",")(memoryflag)}" + "\n"
          tmpChannel = tmpChannel + s"a1.channels.c${i}.transactionCapacity = ${transactionCapacity.split(",")(memoryflag)}" + "\n"
          memoryflag = memoryflag + 1
        }
        case "file" => {
          tmpChannel = tmpChannel + s"a1.channels.c${i}.checkpointDir = ${checkpointDir.split(",")(flieflag)}" + "\n"
          tmpChannel = tmpChannel + s"a1.channels.c${i}.dataDirs = ${dataDirs.split(",")(flieflag)}" + "\n"
          flieflag = flieflag + 1
        }
        case _ => println("无匹配的channel")
      }
    }
    tmpconf = tmpconf + tmpChannel

    // 同上
    tmpSink = ""
    var hdfsflagSink = 0
    var avroflagSink = 0
    var hiveflagSink = 0
    var kafkaflagSink = 0
    for (i <- 1 to SinkList.length) {

      SinkList(i - 1) match {
        case "logger" => {
          tmpSink = tmpSink + s"a1.sinks.k${i}.type = ${SinkList(i - 1)}" + "\n"
        }
        case "hdfs" => {
          tmpSink = tmpSink + s"a1.sinks.k${i}.type = ${SinkList(i - 1)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.path = hdfs://bigdata3:9000${hdfspath.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.filePrefix = ${filePrefix.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.fileSuffix = ${fileSuffix.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.writeFormat = ${writeFormat.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.rollSize = ${rollSize.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.rollInterval =${rollInterval.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.rollCount =${rollCount.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.round =${round.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.roundUnit =${roundUnit.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.batchSize =${batchSizehdfs.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.roundValue =${roundValue.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.useLocalTimeStamp =${useLocalTimeStamp.split(",")(hdfsflagSink)}" + "\n"
          if (!(codeC.eq("")))
            tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.codeC =${codeC.split(",")(hdfsflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hdfs.fileType =${fileType.split(",")(hdfsflagSink)}" + "\n"
          hdfsflagSink = hdfsflagSink + 1
        }
        case "avro" => {
          tmpSink = tmpSink + s"a1.sinks.k${i}.type = ${SinkList(i - 1)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hostname=${hostname.split(",")(avroflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.port=${portSink.split(",")(avroflagSink)}" + "\n"
          avroflagSink = avroflagSink + 1
        }
        case "hive" => {
          tmpSink = tmpSink + s"a1.sinks.k${i}.type = ${SinkList(i - 1)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.serializer = ${serializer.split(",")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hive.metastore = ${metastore.split(",")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hive.database =${database.split(",")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.hive.table = ${table.split(",")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.serializer.delimiter = ${delimiter.split("--")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.serializer.serdeSeparator = ${serdeSeparator.split(",")(hiveflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.serializer.fieldnames =  ${fieldnames.split("--")(hiveflagSink)}" + "\n"
          hiveflagSink = hiveflagSink + 1
        }
        case "kafka" => {
          tmpSink = tmpSink + s"a1.sinks.k${i}.type =  org.apache.flume.sink.kafka.KafkaSink" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.kafka.topic = ${topicSink.split(",")(kafkaflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.kafka.bootstrap.servers = ${brokerList.split("--")(kafkaflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.kafka.producer.acks = ${requiredAcks.split(",")(kafkaflagSink)}" + "\n"
          tmpSink = tmpSink + s"a1.sinks.k${i}.flumeBatchSize = ${batchSizeSink.split(",")(kafkaflagSink)}" + "\n"
          kafkaflagSink = kafkaflagSink + 1
        }
        case _ => println("Sink出现问题")
      }
    }
    tmpconf = tmpconf + tmpSink

    var interflag = 0
    var tmpoption = ""
    if (!(interceptors.eq(""))) {
      for (i <- 1 to interceptors.split(",").length) {
        tmpoption = tmpoption + s"a1.sources.r${i}.interceptors = example${i}" + "\n"
        tmpoption = tmpoption + s"a1.sources.r${i}.interceptors.example${i}.type = ${interceptors.split(",")(interflag)}" + "\n"
        if (!(interceptorskey.eq("")))
          tmpoption = tmpoption + s"a1.sources.r${i}.interceptors.example${i}.key = ${interceptorskey.split(",")(interflag)}" + "\n"
        if (!(interceptorsvalue.eq("")))
          tmpoption = tmpoption + s"a1.sources.r${i}.interceptors.example${i}.value = ${interceptorsvalue.split(",")(interflag)}" + "\n"
        interflag = interflag + 1
      }
    }


    var jureceflag = 0
    if (!(maxpenalty.equals(""))) {
      tmpoption = tmpoption + "a1.sinkgroups = g1" + "\n"
      tmpoption = tmpoption + s"a1.sinkgroups.g1.processor.maxpenalty = ${maxpenalty}" + "\n"
      tmpoption = tmpoption + s"a1.sinkgroups.g1.processor.type = ${processortype}" + "\n"
      var tmpgroup = ""

      for (i <- 1 to SinkList.length) {
        tmpoption = tmpoption + s"a1.sinkgroups.g1.processor.priority.k${i} = ${priority.split(",")(jureceflag)}" + "\n"
        tmpgroup = tmpgroup + s"k${i}" + " "
        jureceflag = jureceflag + 1
      }
      tmpoption = tmpoption + "a1.sinkgroups.g1.sinks = " + tmpgroup + "\n"
    }


    if(!(selectortype.eq(""))){
      for (i <- 1 to  selectortype.split(",").length)
        tmpoption = tmpoption + s"a1.sources.r${i}.selector.type = ${selectortype.split(",")(i-1)}" + "\n"
    }


    tmpconf = tmpconf + tmpoption


    var tmpconnect = ""
    if ((SourceList.length > 1) && (ChannelList.length > 1)) {
      for (i <- 1 to SinkList.length) {
        tmpconnect = tmpconnect + s"a1.sources.r${i}.channels = c${i}" + "\n"
      }
    } else if ((SourceList.length == 1) && (ChannelList.length > 1)) {
      tmpconnect = "a1.sources.r1.channels = "
      var tmpString = ""
      for (i <- 1 to ChannelList.length) {
        tmpString = tmpString + s"c${i}" + " "
      }
      tmpconnect = tmpconnect + tmpString + "\n"
    } else if ((SourceList.length > 1) && (ChannelList.length == 1)) {
      for (i <- 1 to (SourceList.length)) {
        tmpconnect = tmpconnect + s"a1.sources.r${i}.channels = c1" + "\n"
      }
    } else {
      tmpconnect = tmpconnect + "a1.sources.r1.channels = c1" + "\n"
    }


    if ((ChannelList.length == 1) && (SinkList.length > 1)) {
      for (i <- 1 to SinkList.length)
        tmpconnect = tmpconnect + s"a1.sinks.k${i}.channel = c1" + "\n"
    } else if ((ChannelList.length > 1) && (SinkList.length == 1)) {
      for (i <- 1 to ChannelList.length)
        tmpconnect = tmpconnect + s"a1.sinks.k1.channel = c${i}" + "\n"
    } else if ((ChannelList.length == 1) && (SinkList.length == 1)) {
      tmpconnect = tmpconnect + "a1.sinks.k1.channel = c1" + "\n"
    } else {
      for (i <- 1 to ChannelList.length)
        tmpconnect = tmpconnect + s"a1.sinks.k${i}.channel = c${i}" + "\n"
    }

    tmpconf = tmpconf + tmpconnect


    val writer = new FileWriter(s"${filename}")
    writer.write(tmpconf)
    writer.close()
    import sys.process._
    val diaodu = s"/home/hadoop/app/flume/bin/flume-ng agent --conf /home/hadoop/app/flume/conf --conf-file ./${filename} -Dflume.root.logger=info,console --classpath ${classpath} --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=5555" !!
    val shanchu = s"rm -rf ./${filename}" !!

    println(tmpconf)
  }


}

```

上面是我给flume包起来的一个外壳，接下来我们要写个脚本让我们更方便调用这个外壳

```powershell
case $1 in 
"-help")
echo "--Source"
echo "--Sink"
echo "--Channel"
echo "--filename"
echo "Source相关参数"
echo "-------------------------------------------------------taildir"
echo "--position ： taildir的position文件位置 默认 /home/hadoop/data/flumepostion/taildir_position.json"
echo "--filegroups : 文件组 默认： f1"
echo "--maxBatchCount ： 最大传输条数 "
echo "--fileHeadertaildir ： 文件头 默认 ： true"
echo "--filepathtaildir ： 文件路径"
echo "-------------------------------------------------------avroSource"
echo "--bind 主机host"
echo "--port 端口"
echo "-------------------------------------------------------spooldirSource"
echo "--spoolDir 文件夹"
echo "--fileHeader 文件头"
echo "-------------------------------------------------------exec"
echo "--command 执行的命令"
echo "--------------------------------------------------------kafka"
echo "--bootstrap kafka的对外端口"
echo "--topics topic的名称"
echo "--groupid 组id"
echo "--batchSize 传输大小"
echo "--batchDurationMillis"
echo "---------------------------------------------------------memory"
echo "--capacity 容量 默认 10000"
echo "--transactionCapacity 支持的事务的容量 默认10000"
echo "----------------------------------------------------------file"
echo "--checkpointDir 文件检查点的文件夹"
echo "--dataDirs 数据文件夹"
echo "----------------------------------------------------------hdfsSink"
echo "--hdfspath"
echo "--filePrefix 前缀"
echo "--fileSuffix 后缀"
echo "--writeFormat 写入格式"
echo "--rollSize 滚动大小 默认 134217728"
echo "--rollInterval 滚动间隔 默认 21600"
echo "--rollCount 滚动数量 默认 1000"
echo "--round 是不是滚动 默认 true"
echo "--roundUnit 默认 minute"
echo "--roundValue 默认 21"
echo "--useLocalTimeStamp 使用本地时间戳 默认 true"
echo "--codeC 压缩格式 默认 bzip2"
echo "--fileTypefileType 文件格式 默认 CompressedStream"
echo "--batchSizehdfs 传输大小 默认 1200"
echo "------------------------------------------------------------avroSink"
echo "--hostname ： 主机名称"
echo "--portSink ： Sink端口"
echo "-------------------------------------------------------------hiveSink"
echo "--serializer 序列化 默认 DELIMITED"
echo "--metastore 源数据库 默认 thrift://bigdata3:9083"
echo "--database 数据库"
echo "--table \数据库表"
echo "--delimiter 字段之间的分割符 用--分开"
echo "--serdeSeparator 表之间的分隔符 "
echo "--fieldnames 字段映射 用 -- 分开 "
echo "--------------------------------------------------------------KafkaSink"
echo "--topicSink topic的名称"
echo "--brokerList kafka地址 默认 bigdata3:9092,bigdata4:9092,bigdata5:9092 用 -- 分开"
echo "--requiredAcks 交付语义 默认-1/all"
echo "--batchSizeSink 大小 ：10000"
echo "------------------------------------------------------------interceptors"
echo "--interceptors 阻断器形式 "
echo "--key key值"
echo "--value value值"
echo "----------------------------------------------------------决策"
echo "--maxpenalty  "
echo "--priority"
echo "--processortype"
echo "--selectortype"
echo "----------------------------------------------------------额外参数"
echo "--filename 文件的名字"
echo "--classpath 额外的jar包"
echo "注意 ： 如果参数中包含$ 则要用单引号把$包起来才可正常执行"  
;;
*)
spark-submit --master local[4] --name flumemaking --executor-memory 1g --num-executors 1 --executor-cores 1 --jars /home/hadoop/software/jar/kafka/spark-streaming-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/spark-token-provider-kafka-0-10_2.12-3.2.1.jar,/home/hadoop/software/jar/kafka/kafka-clients-2.2.1.jar,/home/hadoop//home/hadoop/software/jar/flink/flink-clients_2.12-1.13.6.jar,/home/hadoop//home/hadoop/software/jar/flink/flink-core-1.13.6.jar,/home/hadoop//home/hadoop/software/jar/flink/flink-scala_2.12-1.13.6.jar,/home/hadoop//home/hadoop/software/jar/flink/flink-java-1.13.6.jar --class  progectPlugin.flume /home/hadoop/project/javapro/flumePlugin.jar  $@
;;
esac
```

这样我们就简单的封装完自己的flume插件了，然后我们执行以下语句开始采集flume.sh \

```powershell
flume.sh \
--Source taildir \
--Channel memory \
--capacity 10000 \
--transactionCapacity 10000 \
--Sink hdfs \
--hdfspath /user/hive/warehouse/warehouse.db/log_info/dt=%Y-%m-%d \
--filePrefix te \
--fileSuffix .lz \
--writeFormat Text \
--useLocalTimeStamp false \
--codeC none \
--rollCount 10000000 \
--rollInterval 216000 \
--fileTypefileType DataStream \
--filename test \
--interceptors flume.flumeinter'$'Builder \
--filegroups f1 \
--filepathtaildir /home/hadoop/project/mnilog/rzhi/log/app.log \
--classpath /home/hadoop/project/jar/flumeinter.jar
```

我们把日志数据采集到hdfs上以及kafka里

这样的目的是方便离线与实时不用重复采集

采集完成之后我们就要开始ods层的建设了

关于ods层，我们自然还是要建表啥的，刚刚采集的数据只是所有数据混合在一切的

接下来，我们要进行创建

* hive里的表
* kafka里的topic

关于hive里的表

```sql
<!--日志表-->
create table log_info (
    info String comment "日志数据"
)
PARTITIONED BY (dt String comment "数据时间")
row format delimited fields terminated by ''
STORED as TEXTFILE; 

```

对于日志文件的ods层很简单的就完成了，因为ods也叫数据原始层，所以我们不用过多的处理

### 业务数据

关于业务数据，在离线数仓中是存储到mysql中的，我通过自己定义的类sqoop工具进行同步，工具如下

```java
package project


import java.util

import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import tool.sqlUtils
import tool.getmysqldf
import tool.savefile
import tool.readfile
import org.apache.flink.api.java.utils.ParameterTool
object jdbctohive{
  def apply(parameterTool: ParameterTool): jdbctohive = new jdbctohive(parameterTool)

  def main(args: Array[String]): Unit = {
    if (args.length==0){
      println(
        """
          |欢迎使用本程序
          |参数详情 mysql hive
          |-------------------------mysql
          |url 例子 ： jdbc:mysql://bigdata2:3306/try
          |user 例子 ： root
          |password 例子 ： liuzihan010616
          |tablename => 支持谓词下压  例子 ： emp 或者 select * from emp 等 order_info
          |driver => com.mysql.jdbc.Driver
          |---------------------------hive
          |mode模式 overwrite append 等
          |hive中的table 例子 bigdata.emp
          |可选参数 分区字段 自动开启的是动态分区 例子 deptno
          |分区字段 [字段值] [标志位]：代表是不是只更新这一个分区的数据
          |jdbc:mysql://bigdata2:3306/try root liuzihan010616 "select * from emp " com.mysql.jdbc.Driver append default.tmp deptno,sal,test,re 999,888
          |""".stripMargin)
    }
    val tool = ParameterTool.fromArgs(args)
    jdbctohive(tool).excute(args)
  }
}




class jdbctohive(parameterTool: ParameterTool) {
  System.setProperty("HADOOP_USER_NAME","hadoop")
  val spark = SparkSession.builder().appName("sqoop").master("local[4]").enableHiveSupport().getOrCreate()
//  spark.sparkContext.setCheckpointDir("/tmp/checkpoint")

  val getmysqldf = new readfile
  val sqlUtils = new sqlUtils
  val saveFile = new savefile
  private val catalog: Catalog = spark.catalog
  var changecolunm = false
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val url = parameterTool.getRequired("url")
  val user = parameterTool.getRequired("user")
  val password = parameterTool.getRequired("password")
  val table = parameterTool.getRequired("table")
  val driver = parameterTool.getRequired("driver")
  val mode = parameterTool.getRequired("mode")
  val hivetable = parameterTool.getRequired("hivetable")
  val hivepartition = parameterTool.get("hivepartition",null)
  val partitionValues = parameterTool.get("partitionValues",null)
  val insertpartition = parameterTool.get("insertpartition")
  val fileformated = parameterTool.get("fileformated" , "textfile")
  val codec = parameterTool.get("Codec" , "none")

  def excute(args: Array[String]): Unit = {


    // 获取jdbc的df
    val mysqlconnect = getmysqldf.getmysqldataframe(spark, url, user, password, table , driver)
    // 验证指示
    mysqlconnect.show()
    // 生成hive参数数组
//    var hiveconf = new Array[String](args.length-5)
//    hiveconf = util.Arrays.copyOfRange(args, 5, args.length)
    //hiveconf.foreach(println(_))
    jdbctohive(args.length,catalog,mysqlconnect)
    spark.stop()
  }



  def changecolnums(int: Int,resourcesql:DataFrame) ={
    var finallyresult:Dataset[Row] = null // 最终结果集
    var frame:DataFrame = null // 中间变量

    var strings2:Array[String] = null
    if (!(hivepartition.eq(null))){
      strings2 = hivepartition.split(",")
    }

    var hiveconclumns = spark.table(hivetable).columns // hive的列数
    //hiveconclumns.foreach(println((_))) // 验证hive的列数
    var mysqlconnect:DataFrame = resourcesql // 设置数据源的resource

    // 判断分区字段在不在jdbc的数据里，如果不在，则在jdbc的数据源中先添加上分区字段
    var strings1:Array[String] =null
    if ((int > 16) && !(partitionValues.eq(null))){
      strings1 = partitionValues.split(",")
    }


    var flagtmp:Int = 0;

    if (!(strings2.eq(null))){
      for (elem <- strings2){
        if (!mysqlconnect.columns.contains(elem)){
          mysqlconnect.show()
          println(elem)
          //println(strings1(flagtmp))
          mysqlconnect = mysqlconnect.withColumn(elem,lit(strings1(flagtmp)))
          flagtmp = flagtmp + 1
          mysqlconnect.show()
        }
      }
    }




    val jdbcconclumns = mysqlconnect.columns // jdbc的列数


    var jdbcoldsource:Dataset[Row] = null // 源数据库的数据 checkpoint是为了破坏数据均衡，以后能编写变读取

    if (int == 20){
      hivepartition.split(",")(0) match {
        case "" => {
          println("-------------------------无操作")
        }
        case _ => {
          hivepartition.split(",").length match {
            case 1 =>
            {
              jdbcoldsource = spark.sql(
                s"""
                   |select * from ${hivetable} where ${hivepartition} != ${partitionValues}
                   |""".stripMargin).checkpoint()
            }
            case _ =>
            {
              var tmpstring:String  = null
              var flag:Int = 0
                 val flagvalue = partitionValues.split(",")
                  for (elem <- hivepartition.split(",")){
                 if (elem == hivepartition.split(",")(hivepartition.split(",").length-1)){
                   tmpstring = tmpstring + elem + "!=" + flagvalue(flag)
                 }else{
                   tmpstring = tmpstring + elem + "!=" + flagvalue(flag) + "and"
                 }
                    flag = flag + 1
              }
//              jdbcoldsource = spark.sql(
//                s"""
//                   |select * from ${hivetable} where ${tmpstring}
//                   |""".stripMargin).checkpoint()
              jdbcoldsource = spark.sql(
                s"""
                   |select * from ${hivetable} where ${tmpstring}
                   |""".stripMargin).persist()
            }
          }
        }
      }


    }else{
//      jdbcoldsource =  spark.sql(
//        s"""
//           |select * from ${hivetable}
//           |""".stripMargin).checkpoint()
      jdbcoldsource =  spark.sql(
        s"""
           |select * from ${hivetable}
           |""".stripMargin).persist()
    }

    var existcolunms: Array[String] = null  // 设置hive或者mysql的额外列
    var resultdf: DataFrame = jdbcoldsource // 获取hive的数据原始数据

    // 判断是hive的列多，还是数据源的列数多
    if (hiveconclumns.length >= jdbcconclumns.length){
      // 判断额外列的存在
      existcolunms= hiveconclumns.filter(hivecol => {
        val bool = jdbcconclumns.map(jdbccol => {
          jdbccol == hivecol
        }).contains(true)
        !bool
      })
      // 判断两个列数是不是相等
        if (existcolunms.isEmpty) {
          frame = mysqlconnect.selectExpr(hiveconclumns: _*)
          frame
        }else{
        // 列数不相等的时候让列数少的加列
        resultdf = mysqlconnect
        for (elem <- existcolunms){
          resultdf = resultdf.withColumn(elem, lit(null))
        }
        // 对字段进行排序 ， 让分区数据的分区字段在最后一列
        frame = resultdf.selectExpr(hiveconclumns: _*)
        // 验证数据
        frame.show()
        // 整合历史数据
        finallyresult = jdbcoldsource.union(frame)
        // 验证数据
        finallyresult.show()
        changecolunm = true
        finallyresult
      }
    }else{
      // 数据的列多
      existcolunms= jdbcconclumns.filter(jdbccol => {
        val bool = hiveconclumns.map(hivecol => {
          jdbccol == hivecol
        }).contains(true)
        !bool
      })

      if (existcolunms.isEmpty) {
        frame = mysqlconnect.selectExpr(hiveconclumns: _*)
        frame
      }else{
        for (elem <- existcolunms){
          resultdf = resultdf.withColumn(elem, lit(null))
        }
        frame = resultdf.selectExpr(jdbcconclumns: _*)
        finallyresult = frame.union(mysqlconnect)
        changecolunm = true
        finallyresult
      }
    }
  }






  def jdbctohive(int: Int,catalog: Catalog,mysqlconnect: DataFrame)={
    // 分割字符串获取hive的 表和数据库
    val hivedbandtables = hivetable.split("\\.")
    var hivepart:Array[String] = null
    if (!hivepartition.eq(null)){
      hivepart = hivepartition.split(",")
      hivepart.foreach(println(_))
    }

// catalog的方法 获取表存不存在的方法
//    catalog.listTables(strings(0)).show()
//    val empty = catalog.listTables(strings(0)).filter(x => {
//      x.name == strings(1)
//    }).isEmpty
    val empty = catalog.tableExists(hivedbandtables(0),hivedbandtables(1))
//-----------------------------------------------------------------------------
//    sql的方法
//    val empty1 = spark.sql(
//      """
//        |show tables in hivedb
//        |""".stripMargin).filter("tableName = 'hivetablename'").isEmpty
// --------------------------------------------------------------------------


    // 判断列数是不是相等
    var frameresult:DataFrame = mysqlconnect
    // 先判断表存不存在 ，因为判断列数的方法要求表存在
      empty match {
          // 表不存在
      case false => {
        // 判断输入的变量个数执行 判断分区表还是普通表
        if (int > 14) {
          println("-----------------分区表")
          // 判断分区的参数在不在列中 如果不在 ，则加上 ，在的话就自动往下走
          var hivepartval:Array[String] =null
            if (int > 16 && partitionValues != null){
            hivepartval = partitionValues.split(",")
          }
          var flagtmp:Int = 0;
          for (elem <- hivepart){
            if (!mysqlconnect.columns.contains(elem)){
              println(elem)
              println(hivepartval(flagtmp))
              frameresult = frameresult.withColumn(elem,lit(hivepartval(flagtmp)))
              flagtmp = flagtmp + 1
            }
          }
        }else{
          println("-----------普通表")
          frameresult = mysqlconnect
          mysqlconnect.show()
        }
      }

      case true => {
        // 表存在
        // 判断是不是分区表
        frameresult = changecolnums(int, mysqlconnect)
//        if (args.length > 7) {
//          println("-----------------分区表")
//          if (!mysqlconnect.columns.contains(args(7))){
//            frameresult = changecolnums(args, hiveconf, mysqlconnect)
//          }
//        }else{
//          println("-----------普通表")
//          frameresult = mysqlconnect
//        }
        frameresult.show()}
    }









    spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
    spark.conf.set("hive.exec.dynamic.partition","true")
//   spark.conf.set("spark.sql.parquet.writeLegacyFormat", "true")
    println(empty)
    saveFile.savetohiveapi(spark,empty,frameresult,hivetable,mode,hivepartition,changecolunm,fileformated,codec)
}

}

```

其中[tool](https://zihang.fun/2001/01/16/tool/)是我自己定义的工具包

其实现在又出现了一个版本的类sqoop工具，比上述这个更好，他会根据你所输入的参数自动来识别你的意图，但是我最开始开发的的确是上述的这个版本，那个是我后来又整合上述进行开发的

然后我们通过上述的类sqoop工具从业务数据库中同步数据到hive中

其中业务表较多，我就步一一赘述了如下

```powershell
-----------------------------------------------------------------------
ods_order_detail
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_order_detail
--table \
"
select
bigdata.order_detail.id,
order_id,
user_id,
sku_id,
sku_name,
order_price,
cast(sku_num AS signed) as sku_num,
bigdata.order_detail.create_time,
source_type,
source_id,
'${v_date}' as dt
from bigdata.order_detail left join bigdata.order_info
on bigdata.order_detail.order_id = bigdata.order_info.id
where date_format(bigdata.order_detail.create_time,'%Y-%m-%d') ='${v_date}'
"
--hivepartition dt
-------------------------------------------------------------------------------------------------
sku_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_sku_info
--table \
"
select
id,
spu_id,
price,
sku_name,
sku_desc,
weight,
tm_id,
category3_id,
create_time,
'${v_date}' as dt
from bigdata.sku_info
where
date_format(create_time,'%Y-%m-%d') <'${v_date}'
"
--hivepartition dt
------------------------------------------------------------------------------------------------------
coupon_use
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_coupon_use
--table \
"
select
id,
coupon_id,
user_id,
order_id,
coupon_status,
get_time,
using_time,
used_time,
'${v_date}' as dt
from bigdata.coupon_use
where
date_format(get_time,'%Y-%m-%d') <'${v_date}'
"
--hivepartition dt

------------------------------------------------------------------------------------------------------
coupon_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_coupon_info
--table \
"
select
id,
coupon_name,
coupon_type,
condition_amount,
condition_num,
activity_id,
benefit_amount,
benefit_discount,
create_time,
range_type,
spu_id,
tm_id,
category3_id,
limit_num,
operate_time,
expire_time,
'${v_date}' as dt
from bigdata.coupon_info
where
date_format(create_time,'%Y-%m-%d') <'${v_date}'
"
--hivepartition dt

----------------------------------------------------------------------------------------------------------
activity_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_activity_info
--table \
"
select
id,
activity_name,
activity_type,
start_time,
end_time,
create_time
from bigdata.activity_info
"

------------------------------------------------------------------------------------------------------------------
activity_order
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_activity_order
--table \
"
select
id,
activity_id,
order_id,
create_time,
'${v_date}' as dt
from bigdata.activity_order
where
date_format(create_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
------------------------------------------------------------------------------------------------------------------
activity_rule
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_activity_rule
--table \
"
select
id,
activity_id,
condition_amount,
condition_num,
benefit_amount,
benefit_discount,
benefit_level
from bigdata.activity_rule
"
------------------------------------------------------------------------------------------------------------------
base_dic
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_base_dic
--table \
"
select
dic_code,
dic_name,
parent_code,
create_time,
operate_time
from bigdata.base_dic
"
-------------------------------------------------------------------------------------------------------------------
user_info

--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_user_info
--table \
"
select
id,
name,
birthday,
gender,
email,
user_level,
create_time,
operate_time,
'20230215' as dt
from bigdata.user_info
where
date_format(operate_time,'%Y-%m-%d') <='20230215'
"
--hivepartition dt
------------------------------------------------------------------------------------------------------------
payment_info

--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_payment_info
--table \
"
select
id,
out_trade_no,
order_id,
user_id,
alipay_trade_no,
total_amount,
subject,
payment_type,
payment_time,
'${v_date}' as dt
from bigdata.payment_info
where
date_format(payment_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
---------------------------------------------------------------------------------------------------------------
base_province
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_base_province
--table \
"
select
id,
name,
region_id,
area_code,
iso_code
from bigdata.base_province
"
--------------------------------------------------------------------------------------------------------------------
base_region
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_base_region
--table 
"
select
id,
region_name
from bigdata.base_region
"
------------------------------------------------------------------------------------------------------------------------
base_trademark
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_base_trademark
--table "
select
tm_id,
tm_name
from bigdata.base_trademark
"
-------------------------------------------------------------------------------------------------------------------
order_status_log;
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_order_status_log
--table \
"
select
id,
order_id,
order_status,
operate_time,
'${v_date}' as dt
from bigdata.order_status_log
where
date_format(operate_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
------------------------------------------------------------------------------------------------------------------------
spu_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_spu_info
--table \
"
select
id,
spu_name,
category3_id,
tm_id
from bigdata.spu_info
"
--------------------------------------------------------------------------------------
comment_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_comment_info
--table \
"
select
id,
user_id,
sku_id,
spu_id,
order_id,
appraise,
create_time,
'${v_date}' as dt
from bigdata.comment_info
where
date_format(operate_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
------------------------------------------------------------------------------------------------
order_refund_info

comment_info
--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_order_refund_info
--table \
"
select
id,
user_id,
order_id,
sku_id,
refund_type,
refund_num,
refund_amount,
refund_reason_type,
create_time,
'${v_date}' as dt
from bigdata.order_refund_info
where
date_format(create_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
------------------------------------------------------------------------------------------------
cart_info

--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_cart_info
--table \
"
select
id,
user_id,
sku_id,
cart_price,
sku_num,
sku_name,
create_time,
operate_time,
is_ordered,
order_time,
source_type,
source_id,
'${v_date}' as dt
from bigdata.cart_info
where
date_format(create_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
-------------------------------------------------------------------------
favor_info

--url jdbc:mysql://43.140.193.43:3306/bigdata 
--user root 
--password Password123@mysql
--driver com.mysql.jdbc.Driver 
--mode overwrite 
--hivetable warehouse.ods_favor_info
--table \
"
select
id,
user_id,
sku_id,
spu_id,
is_cancel,
create_time,
cancel_time,
'${v_date}' as dt
from bigdata.favor_info
where
date_format(create_time,'%Y-%m-%d') <='${v_date}'
"
--hivepartition dt
----------------------------------------------------------------------------
order_info

--url jdbc:mysql://43.140.193.43:3306/bigdata  \
--user root  \
--password Password123@mysql \
--driver com.mysql.jdbc.Driver  \
--mode overwrite \
--hivetable warehouse.ods_order_info \
--table \
"
select
id,
final_total_amount,
order_status,
user_id,
out_trade_no,
create_time,
operate_time,
province_id,
benefit_reduce_amount,
original_total_amount,
feight_fee,
'${v_date}' as dt
from bigdata.order_info
where
date_format(create_time,'%Y-%m-%d') <='${v_date}'
" \
--hivepartition dt
```

其中v_date是我们的数据时间，每天通过xxl进行定时调度，然后在shell脚本上v_date的赋值情况如下

```powershell
if [ -n "$1" ];then
v_date=$1
else
v_date=`date '+%Y-%m-%d'`
fi
echo $v_date
```

## dwd层

对于dw层同样是分为日志数据和业务数据的

### 日志数据

首先对日志数据进行简单的筛选解析，因为我们的日志数据采用的是json形式的，所以要先把数据解析出来

```sql
-------------------------------------------------------------------------------------------------------------------
dwd


create table warehouse.dwd_start_log(
`area_code` string comment '地区编码',
`brand` string comment '手机品牌',
`channel` string comment '渠道',
`model` string comment '手机型号',
`mid_id` string comment '设备id',
`os` string comment '操作系统',
`user_id` string comment '会员id',
`version_code` string comment 'app版本号',
`entry` string comment ' icon手机图标 notice 通知 install 安装后启动',
`loading_time` bigint comment '启动加载时间',
`open_ad_id` string comment '广告页id ',
`open_ad_ms` bigint comment '广告总共播放时间',
`open_ad_skip_ms` bigint comment '用户跳过广告时点',
`ts` bigint comment '时间'
) comment '启动日志表'
partitioned by (dt string)
stored as textfile;

{"common":{"ar":"230000","ba":"iPhone","ch":"Appstore","md":"iPhone Xs Max","mid":"mid_7","os":"iOS 13.2.9","uid":"278","vc":"v2.1.134"},"err":{"error_code":2584,"msg":" Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atgugu.gmall2020.mock.log.bean.AppError.main(AppError.java:xxxxxx)"},"start":{"entry":"icon","loading_time":16186,"open_ad_id":17,"open_ad_ms":1116,"open_ad_skip_ms":0},"ts":1676599070000}	2023-02-17

insert overwrite  table dwd_start_log partition (dt)
select 
/*COALESCE(1)*/
get_json_object(info,'$.common.ar') as area_code,
get_json_object(info,'$.common.ba') as brand,
get_json_object(info,'$.common.ch') as channel,
get_json_object(info,'$.common.md') as model,
get_json_object(info,'$.common.mid') as mid_id,
get_json_object(info,'$.common.os') as os,
get_json_object(info,'$.common.uid') as user_id,
get_json_object(info,'$.common.vc') as version_code,
get_json_object(info,'$.start.entry') as entry,
cast(get_json_object(info,'$.start.loading_time')as bigint) as loadingtime, 
get_json_object(info,'$.start.open_ad_id')  as open_ad_id,
cast(get_json_object(info,'$.start.open_ad_ms') as bigint) as open_ad_ms,
cast(get_json_object(info,'$.start.open_ad_skip_ms') as bigint) as open_ad_skip_ms,
cast(get_json_object(info,'$.start.ts') as bigint) as ts,
dt
from log_info
where 
dt='2023-02-17' and get_json_object(info,'$.start') is not null;



---------------------------------------------------------------------------------------------------------------------
create table warehouse.dwd_page_log(
`area_code` string comment '地区编码',
`brand` string comment '手机品牌',
`channel` string comment '渠道',
`model` string comment '手机型号',
`mid_id` string comment '设备id',
`os` string comment '操作系统',
`user_id` string comment '会员id',
`version_code` string comment 'app版本号',
`during_time` bigint comment '持续时间毫秒',
`page_item` string comment '目标id ',
`page_item_type` string comment '目标类型',
`last_page_id` string comment '上页类型',
`page_id` string comment '页面id ',
`source_type` string comment '来源类型',
`ts` bigint
) comment '页面日志表'
partitioned by (dt string)
stored as textfile;

{"common":{"ar":"230000","ba":"iPhone","ch":"Appstore","md":"iPhone Xs Max","mid":"mid_7","os":"iOS 13.2.9","uid":"278","vc":"v2.1.134"},"page":{"during_time":9828,"item":"2,7","item_type":"sku_ids","last_page_id":"comment","page_id":"trade"},"ts":1676599137183}	2023-02-17

{"common":{"ar":"310000","ba":"iPhone","ch":"Appstore","md":"iPhone X","mid":"mid_41","os":"iOS 13.3.1","uid":"267","vc":"v2.1.134"},"displays":[{"display_type":"activity","item":"1","item_type":"activity_id","order":1},{"display_type":"activity","item":"2","item_type":"activity_id","order":2},{"display_type":"recommend","item":"10","item_type":"sku_id","order":3},{"display_type":"query","item":"8","item_type":"sku_id","order":4},{"display_type":"promotion","item":"2","item_type":"sku_id","order":5},{"display_type":"query","item":"1","item_type":"sku_id","order":6},{"display_type":"promotion","item":"8","item_type":"sku_id","order":7}],"page":{"during_time":5576,"last_page_id":"home","page_id":"category"},"ts":1676599094404}	2023-02-17

insert overwrite table dwd_page_log partition(dt)
select
/*COALESCE(1)*/
get_json_object(info,'$.common.ar') as area_code,
get_json_object(info,'$.common.ba') as brand,
get_json_object(info,'$.common.ch') as channel,
get_json_object(info,'$.common.md') as model,
get_json_object(info,'$.common.mid') as mid_id,
get_json_object(info,'$.common.os') as os,
get_json_object(info,'$.common.uid') as user_id,
get_json_object(info,'$.common.vc') as version_code,
cast(get_json_object(info,'$.page.during_time') as bigint) as during_time,
nvl(get_json_object(info,'$.page.item') , 'unknown') as page_item,
nvl(get_json_object(info,'$.page.item_type'), 'unknown') as page_item_type,
nvl(get_json_object(info,'$.page.last_page_id'),'unknown') as last_page_id,
nvl(get_json_object(info,'$.page.page_id'),'unknown') as page_id,
nvl(get_json_object(info,'$.page.source_type'),'unknown') as source_type,
cast(get_json_object(info,'$.ts') as bigint) as ts,
dt
from log_info
where dt='2023-02-17' and get_json_object(info,'$.page') is not null ;

--------------------------------------------------------------------------------------------------------
create table warehouse.dwd_action_log(
`area_code` string comment '地区编码',
`brand` string comment '手机品牌',
`channel` string comment '渠道',
`model` string comment '手机型号',
`mid_id` string comment '设备id',
`os` string comment '操作系统',
`user_id` string comment '会员id',
`version_code` string comment 'app版本号',
`during_time` bigint comment '持续时间毫秒',
`page_item` string comment '目标id ',
`page_item_type` string comment '目标类型',
`last_page_id` string comment '上页类型',
`page_id` string comment '页面id ',
`source_type` string comment '来源类型',
`action_id` string comment '动作id',
`item` string comment '目标id ',
`item_type` string comment '目标类型',
`ts` bigint comment '时间'
) comment '动作日志表'
partitioned by (dt string)
stored as textfile;

{"actions":[{"action_id":"favor_add","item":"6","item_type":"sku_id","ts":1676599118576},{"action_id":"cart_add","item":"6","item_type":"sku_id","ts":1676599123612}],"common":{"ar":"440000","ba":"Xiaomi","ch":"xiaomi","md":"Xiaomi 10 Pro ","mid":"mid_15","os":"Android 11.0","uid":"417","vc":"v2.1.134"},"displays":[{"display_type":"query","item":"7","item_type":"sku_id","order":1},{"display_type":"query","item":"1","item_type":"sku_id","order":2},{"display_type":"query","item":"3","item_type":"sku_id","order":3},{"display_type":"query","item":"10","item_type":"sku_id","order":4},{"display_type":"query","item":"4","item_type":"sku_id","order":5},{"display_type":"query","item":"10","item_type":"sku_id","order":6},{"display_type":"query","item":"1","item_type":"sku_id","order":7},{"display_type":"query","item":"7","item_type":"sku_id","order":8},{"display_type":"query","item":"1","item_type":"sku_id","order":9}],"page":{"during_time":15110,"item":"6","item_type":"sku_id","last_page_id":"login","page_id":"good_detail","source_type":"query"},"ts":1676599113540}	2023-02-17

insert overwrite table dwd_action_log partition(dt)
select
/*COALESCE(1)*/
area_code,
brand,
channel,
model,
mid_id,
os,
user_id,
version_code,
during_time,
page_item,
page_item_type,
last_page_id,
page_id,
source_type,
nvl(get_json_object(actions_total,'$.action_id'),'unknown') as action_id,
nvl(get_json_object(actions_total,'$.item'),'unknown') as actionsitem,
nvl(get_json_object(actions_total,'$.item_type'),'nuknown') as actionsitem_type,
ts,
dt
from
(
select 
get_json_object(info,'$.common.ar') as area_code,
get_json_object(info,'$.common.ba') as brand,
get_json_object(info,'$.common.ch') as channel,
get_json_object(info,'$.common.md') as model,
get_json_object(info,'$.common.mid') as mid_id,
get_json_object(info,'$.common.os') as os,
get_json_object(info,'$.common.uid') as user_id,
get_json_object(info,'$.common.vc') as version_code,
cast(get_json_object(info,'$.page.during_time') as bigint) as during_time,
nvl(get_json_object(info,'$.page.item') , 'unknown') as page_item,
nvl(get_json_object(info,'$.page.item_type'), 'unknown') as page_item_type,
nvl(get_json_object(info,'$.page.last_page_id'),'unknown') as last_page_id,
nvl(get_json_object(info,'$.page.page_id'),'unknown') as page_id,
nvl(get_json_object(info,'$.page.source_type'),'unknown') as source_type,
dt,
cast(get_json_object(info,'$.ts') as bigint) as ts,
split(regexp_replace(regexp_replace(get_json_object(info,'$.actions'),'\\[|\\]',""),'},','}&'),'&') as action
from log_info
where dt = '2023-02-17' and get_json_object(info,'$.actions') is not null
) as a
lateral view explode(action) log_info as actions_total


---------------------------------------------------------------------------------------
create table warehouse.dwd_display_log(
`area_code` string comment '地区编码',
`brand` string comment '手机品牌',
`channel` string comment '渠道',
`model` string comment '手机型号',
`mid_id` string comment '设备id',
`os` string comment '操作系统',
`user_id` string comment '会员id',
`version_code` string comment 'app版本号',
`during_time` bigint comment 'app版本号',
`page_item` string comment '目标id ',
`page_item_type` string comment '目标类型',
`last_page_id` string comment '上页类型',
`page_id` string comment '页面id ',
`source_type` string comment '来源类型',
`ts` bigint comment 'app版本号',
`display_type` string comment '曝光类型',
`item` string comment '曝光对象id ',
`item_type` string comment 'app版本号',
`order` bigint comment '出现顺序'
) comment '曝光日志表'
partitioned by (dt string)
stored as textfile;

{"common":{"ar":"440000","ba":"Xiaomi","ch":"xiaomi","md":"Xiaomi 10 Pro ","mid":"mid_15","os":"Android 11.0","uid":"417","vc":"v2.1.134"},"displays":[{"display_type":"activity","item":"1","item_type":"activity_id","order":1},{"display_type":"activity","item":"1","item_type":"activity_id","order":2},{"display_type":"promotion","item":"4","item_type":"sku_id","order":3},{"display_type":"promotion","item":"1","item_type":"sku_id","order":4},{"display_type":"recommend","item":"5","item_type":"sku_id","order":5},{"display_type":"query","item":"8","item_type":"sku_id","order":6},{"display_type":"promotion","item":"1","item_type":"sku_id","order":7},{"display_type":"query","item":"7","item_type":"sku_id","order":8},{"display_type":"query","item":"1","item_type":"sku_id","order":9}],"page":{"during_time":4324,"page_id":"home"},"ts":1676599075078}	2023-02-17


insert overwrite table dwd_display_log partition(dt)
select
/*COALESCE(1)*/
area_code,
brand,
channel,
model,
mid_id,
os,
user_id,
version_code,
during_time,
page_item,
page_item_type,
last_page_id,
page_id,
source_type,
ts,
nvl(get_json_object(display_total,'$.display_type'),'unknown') as display_type,
nvl(get_json_object(display_total,'$.item'),'unknown') as item,
nvl(get_json_object(display_total,'$.item_type'),'unknown') as item_type,
nvl(get_json_object(display_total,'$.order'),'unknown') as display_order,
dt
from
(
select 
get_json_object(info,'$.common.ar') as area_code,
get_json_object(info,'$.common.ba') as brand,
get_json_object(info,'$.common.ch') as channel,
get_json_object(info,'$.common.md') as model,
get_json_object(info,'$.common.mid') as mid_id,
get_json_object(info,'$.common.os') as os,
get_json_object(info,'$.common.uid') as user_id,
get_json_object(info,'$.common.vc') as version_code,
cast(get_json_object(info,'$.page.during_time') as bigint) as during_time,
nvl(get_json_object(info,'$.page.item') , 'unknown') as page_item,
nvl(get_json_object(info,'$.page.item_type'), 'unknown') as page_item_type,
nvl(get_json_object(info,'$.page.last_page_id'),'unknown') as last_page_id,
nvl(get_json_object(info,'$.page.page_id'),'unknown') as page_id,
nvl(get_json_object(info,'$.page.source_type'),'unknown') as source_type,
dt,
cast(get_json_object(info,'$.ts') as bigint) as ts,
split(regexp_replace(regexp_replace(get_json_object(info,'$.displays'),'\\[|\\]',""),'},','}&'),'&') as display
from log_info
where dt = '2023-02-17' and get_json_object(info,'$.displays') is not null
) as b 
lateral view explode(display) log_info as display_total

----------------------------------------------------------------------------------------------------------------------------
create table warehouse.dwd_error_log(
`area_code` string comment '地区编码',
`brand` string comment '手机品牌',
`channel` string comment '渠道',
`model` string comment '手机型号',
`mid_id` string comment '设备id',
`os` string comment '操作系统',
`user_id` string comment '会员id',
`version_code` string comment 'app版本号',
`page_item` string comment '目标id ',
`page_item_type` string comment '目标类型',
`last_page_id` string comment '上页类型',
`page_id` string comment '页面id ',
`source_type` string comment '来源类型',
`entry` string comment ' icon手机图标 notice 通知 install 安装后启动',
`loading_time` string comment '启动加载时间',
`open_ad_id` string comment '广告页id ',
`open_ad_ms` string comment '广告总共播放时间',
`open_ad_skip_ms` string comment '用户跳过广告时点',
`actions` string comment '动作',
`displays` string comment '曝光',
`ts` string comment '时间',
`error_code` string comment '错误码',
`msg` string comment '错误信息'
) comment '错误日志表'
partitioned by (dt string)
stored as textfile

{"common":{"ar":"230000","ba":"iPhone","ch":"Appstore","md":"iPhone Xs Max","mid":"mid_7","os":"iOS 13.2.9","uid":"278","vc":"v2.1.134"},"err":{"error_code":2584,"msg":" Exception in thread \\  java.net.SocketTimeoutException\\n \\tat com.atgugu.gmall2020.mock.log.bean.AppError.main(AppError.java:xxxxxx)"},"start":{"entry":"icon","loading_time":16186,"open_ad_id":17,"open_ad_ms":1116,"open_ad_skip_ms":0},"ts":1676599070000}	2023-02-17

insert overwrite table dwd_error_log partition(dt)
select 
/*COALESCE(1)*/
get_json_object(info,'$.common.ar') as area_code,
get_json_object(info,'$.common.ba') as brand,
get_json_object(info,'$.common.ch') as channel,
get_json_object(info,'$.common.md') as model,
get_json_object(info,'$.common.mid') as mid_id,
get_json_object(info,'$.common.os') as os,
get_json_object(info,'$.common.uid') as user_id,
get_json_object(info,'$.common.vc') as version_code,
nvl(get_json_object(info,'$.page.item') , 'unknown') as page_item,
nvl(get_json_object(info,'$.page.item_type'), 'unknown') as page_item_type,
nvl(get_json_object(info,'$.page.last_page_id'),'unknown') as last_page_id,
nvl(get_json_object(info,'$.page.page_id'),'unknown') as page_id,
nvl(get_json_object(info,'$.page.source_type'),'unknown') as source_type,
get_json_object(info,'$.start.entry') as entry,
cast(get_json_object(info,'$.start.loading_time')as bigint) as loadingtime, 
get_json_object(info,'$.start.open_ad_id')  as open_ad_id,
cast(get_json_object(info,'$.start.open_ad_ms') as bigint) as open_ad_ms,
cast(get_json_object(info,'$.start.open_ad_skip_ms') as bigint) as open_ad_skip_ms,
nvl(get_json_object(info,'$.actions'),'unknown') as actions,
nvl(get_json_object(info,'$.displays'),'unknown') as displays,
cast(get_json_object(info,'$.ts') as bigint) as ts,
get_json_object(info,'$.err.error_code') as error_code,
get_json_object(info,'$.err.msg') as msg,
dt
from log_info
where 
dt='2023-02-17' and get_json_object(info,'$.err') is not null
```

然后日志数据的dw层基本就是完事了

### 业务数据

关于业务数据我们要开始搭建维度表

对于dw层的主要工作内容：

* 维度建模
  * 星星：ok dwm比较多
  * 雪花：java
    * 三泛式：
      * 一泛式：全部数据在一起的表
      * 二泛式：分为订单主表和订单明细表
      * 三泛式：然后把订单明细表拆成商品，和订单明细表
  * 星座：ok dws比较多

星星模型：由一级明细分表join到一起

雪花模型：有二级明细分表

星座模型：就是多个星星模型联系到一起join到一起

如下

```sql
create table warehouse.dwd_dim_sku_info (
`id` string comment '商品id',
`spu_id` string comment 'spuid',
`price` decimal(16,2) comment '商品价格',
`sku_name` string comment '商品名称',
`sku_desc` string comment '商品描述',
`weight` decimal(16,2) comment '重量',
`tm_id` string comment '品牌id',
`tm_name` string comment '品牌名称',
`category3_id` string comment '三级分类id',
`category2_id` string comment '二级分类id',
`category1_id` string comment '一级分类id',
`category3_name` string comment '三级分类名称',
`category2_name` string comment '二级分类名称',
`category1_name` string comment '一级分类名称',
`spu_name` string comment 'spu名称',
`create_time` string comment '创建时间'
) comment '商品维度表'
partitioned by (`dt` string)
stored as textfile;




insert overwrite table dwd_dim_sku_info partition(dt)
select
/*COALESCE(1)*/
ods_sku_info.id,
spu_id,
price,
sku_name,
sku_desc,
weight,
ods_base_trademark.tm_id,
ods_base_trademark.tm_name,
a.category3_id,
a.category2_id,
a.category1_id,
a.category3_name,
a.category2_name,
a.category1_name,
ods_spu_info.spu_name,
create_time,
dt
from
ods_sku_info
left join ods_base_trademark
on ods_sku_info.tm_id = ods_base_trademark.tm_id
left join 
(
   select
   ods_base_category3.id as category3_id,
   ods_base_category3.name as category3_name,
   ods_base_category2.id as category2_id,
   ods_base_category2.name as category2_name,
   ods_base_category1.id as category1_id,
   ods_base_category1.name as category1_name
   from ods_base_category3
   left join ods_base_category2 
   on ods_base_category3.category2_id = ods_base_category2.id
   left join ods_base_category1
   on ods_base_category2.category1_id = ods_base_category1.id

) as a 
on  a.category3_id=ods_sku_info.category3_id
left join ods_spu_info
on ods_sku_info.spu_id = ods_spu_info.id
where dt = '2023-02-17';



-----------------------------------------------------------------------
create table warehouse.dwd_dim_coupon_info(
`id` string comment '购物券编号',
`coupon_name` string comment '购物券名称',
`coupon_type` string comment '购物券类型 1 现金券 2 折扣券 3 满减券 4 满件打折券',
`condition_amount` decimal(16,2) comment '满额数',
`condition_num` bigint comment '满件数',
`activity_id` string comment '活动编号',
`benefit_amount` decimal(16,2) comment '减金额',
`benefit_discount` decimal(16,2) comment '折扣',
`create_time` string comment '创建时间',
`range_type` string comment '范围类型 1、商品 2、品类 3、品牌',
`spu_id` string comment '商品id',
`tm_id` string comment '品牌id',
`category3_id` string comment '品类id',
`limit_num` bigint comment '最多领用次数',
`operate_time` string comment '修改时间',
`expire_time` string comment '过期时间'
) comment '优惠券维度表'
partitioned by (`dt` string)
stored as textfile;

insert overwrite table dwd_dim_coupon_info partition(dt)
select
/*COALESCE(1)*/
id,
coupon_name,
coupon_type,
condition_amount,
condition_num,
activity_id,
benefit_amount,
benefit_discount,
create_time,
range_type,
nvl(spu_id,'unknown'),
nvl(tm_id,'unknown'),
category3_id,
limit_num,
nvl(operate_time,'unknown'),
nvl(expire_time,'unknown'),
dt
from ods_coupon_info
where dt = '2023-02-20';

--------------------------------------------------------------------------------------
create table warehouse.dwd_dim_activity_info(
`id` string comment '编号',
`activity_name` string comment '活动名称',
`activity_type` string comment '活动类型',
`start_time` string comment '开始时间',
`end_time` string comment '结束时间',
`create_time` string comment '创建时间'
) comment '活动信息表'
partitioned by (`dt` string)
stored as textfile;


insert overwrite table dwd_dim_activity_info partition(dt='2023-02-20')
select
id,
activity_name,
activity_type,
start_time,
end_time,
create_time
from ods_activity_info;
---------------------------------------------------------------------------------------------

create table warehouse.dwd_dim_base_province (
`id` string comment 'id',
`province_name` string comment '省市名称',
`area_code` string comment '地区编码',
`iso_code` string comment 'iso编码',
`region_id` string comment '地区id',
`region_name` string comment '地区名称'
) comment '地区维度表'
stored as textfile;

insert overwrite table  dwd_dim_base_province 
select
/*COALESCE(1)*/
ods_base_province.id,
ods_base_province.name,
area_code,
iso_code,
ods_base_province.region_id,
ods_base_region.region_name
from ods_base_province
left join ods_base_region
on ods_base_province.region_id=ods_base_region.id


------------------------------------------------------------------------------------------------------

create table warehouse.dwd_fact_payment_info (
`id` string comment 'id',
`out_trade_no` string comment '对外业务编号',
`order_id` string comment '订单编号',
`user_id` string comment '用户编号',
`alipay_trade_no` string comment '支付宝交易流水编号',
`payment_amount` decimal(16,2) comment '支付金额',
`subject` string comment '交易内容',
`payment_type` string comment '支付类型',
`payment_time` string comment '支付时间',
`province_id` string comment '省份id'
) comment '支付事实表表'
partitioned by (`dt` string)
stored as textfile;

insert overwrite table  dwd_fact_payment_info partition(dt)
select 
/*COALESCE*/
ods_payment_info.id,
ods_payment_info.out_trade_no,
ods_payment_info.order_id,
ods_payment_info.user_id,
ods_payment_info.alipay_trade_no,
ods_payment_info.total_amount,
ods_payment_info.subject,
ods_payment_info.payment_type,
ods_payment_info.payment_time,
province_id,
ods_payment_info.dt
from ods_payment_info
left join  ods_order_info
on ods_payment_info.order_id=ods_order_info.id
and ods_payment_info.out_trade_no = ods_order_info.out_trade_no
where ods_payment_info.dt = '2023-02-20';

------------------------------------------------------------------------------------------

create table warehouse.dwd_fact_order_refund_info(
`id` string comment '编号',
`user_id` string comment '用户id',
`order_id` string comment '订单id',
`sku_id` string comment '商品id',
`refund_type` string comment '退款类型',
`refund_num` bigint comment '退款件数',
`refund_amount` decimal(16,2) comment '退款金额',
`refund_reason_type` string comment '退款原因类型',
`create_time` string comment '退款时间'
) comment '退款事实表'
partitioned by (`dt` string)
stored as textfile;


insert overwrite table dwd_fact_order_refund_info partition(dt)
select 
/*COALESCE(1)*/
id,
user_id,
order_id,
sku_id,
refund_type,
refund_num,
refund_amount,
refund_reason_type,
create_time,
dt
from order_refund_info
where dt='2023-02-20';

---------------------------------------------------------------------------------
create table warehouse.dwd_fact_comment_info(
`id` string comment '编号',
`user_id` string comment '用户id',
`sku_id` string comment '商品sku',
`spu_id` string comment '商品spu',
`order_id` string comment '订单id',
`appraise` string comment '评价',
`create_time` string comment '评价时间'
) comment '评价事实表'
partitioned by (`dt` string)
stored as textfile;

insert overwrite table dwd_fact_comment_info
partition(dt)
select
id,
user_id,
sku_id,
spu_id,
order_id,
appraise,
create_time,
dt
from ods_comment_info
where dt='2023-02-20';

--------------------------------------------------------------------------------------
create table warehouse.dwd_fact_cart_info(
`id` string comment '编号',
`user_id` string comment '用户id',
`sku_id` string comment 'skuid',
`cart_price` string comment '放入购物车时价格',
`sku_num` string comment '数量',
`sku_name` string comment 'sku名称 (冗余)',
`create_time` string comment '创建时间',
`operate_time` string comment '修改时间',
`is_ordered` string comment '是否已经下单。1为已下单;0为未下单',
`order_time` string comment '下单时间',
`source_type` string comment '来源类型',
`srouce_id` string comment '来源编号'
) comment '加购事实表'
partitioned by (`dt` string)
stored as textfile;

insert overwrite table dwd_fact_cart_info
partition(dt)
select
id,
user_id,
sku_id,
cart_price,
sku_num,
sku_name,
create_time,
operate_time,
is_ordered,
order_time,
source_type,
source_id,
dt
from ods_cart_info
where dt='2023-02-20';

-------------------------------------------------------------------------------------
create table warehouse.dwd_fact_favor_info(
`id` string comment '编号',
`user_id` string comment '用户id',
`sku_id` string comment 'skuid',
`spu_id` string comment 'spuid',
`is_cancel` string comment '是否取消',
`create_time` string comment '收藏时间',
`cancel_time` string comment '取消时间'
) comment '收藏事实表'
partitioned by (`dt` string)
stored as textfile;

insert overwrite table dwd_fact_favor_info
partition(dt)
select
id,
user_id,
sku_id,
spu_id,
is_cancel,
create_time,
cancel_time,
dt
from ods_favor_info
where dt='2023-02-20';

-------------------------------------------------------------------------------------
create table warehouse.dwd_dim_user_info_his(
`id` string COMMENT '用户id',
`name` string COMMENT '姓名',
`birthday` string COMMENT '生日',
`gender` string COMMENT '性别',
`email` string COMMENT '邮箱',
`user_level` string COMMENT '用户等级',
`create_time` string COMMENT '创建时间',
`operate_time` string COMMENT '操作时间',
`start_date` string COMMENT '有效开始日期',
`end_date` string COMMENT '有效结束日期'
) COMMENT '用户拉链表'
stored as textfile;

--初始化拉链表
insert overwrite table dwd_dim_user_info_his
select
id,
name,
birthday,
gender,
email,
user_level,
create_time,
operate_time,
'2023-02-14',
'9999-99-99'
from ods_user_info
where dt='2023-02-14';

---
-- 合并拉链表 思想：新增+merge
insert overwrite table dwd_dim_user_info_his
select
/*COALESCE(1)*/
* 
from
(
select
id,
name,
birthday,
gender,
email,
user_level,
create_time,
operate_time,
'2023-02-21' start_date,
'9999-99-99' end_date
from ods_user_info where dt='2023-02-21'

union all

select
uh.id,
uh.name,
uh.birthday,
uh.gender,
uh.email,
uh.user_level,
uh.create_time,
uh.operate_time,
uh.start_date,
case
when uh.end_date = '9999-99-99'
and ui.id is not null then uh.start_date
else uh.end_date end as end_date
from dwd_dim_user_info_his uh left join
(
select
*
from ods_user_info
where dt='2023-02-21'
) ui on uh.id=ui.id
)his ;

-------------------------------------------------------------------
create table warehouse.dwd_fact_coupon_use(
`id` string comment '编号',
`coupon_id` string comment '优惠券id',
`user_id` string comment 'userid',
`order_id` string comment '订单id',
`coupon_status` string comment '优惠券状态',
`get_time` string comment '领取时间',
`using_time` string comment '使用时间(下单)',
`used_time` string comment '使用时间(支付)'
) comment '优惠券领用事实表'
partitioned by (`dt` string)
stored as textfile;


set
hive.exec.dynamic.partition.mode = nonstrict;

insert
overwrite table dwd_fact_coupon_use partition(dt)
select
id,
coupon_id,
user_id,
order_id,
coupon_status,
get_time,
using_time,
used_time,
date_format(get_time, 'yyyyMMdd') as dt
from
(
select
a.*
from
(
select
*
from
dwd_fact_coupon_use
where
date_format(get_time, 'yyyy-MM-dd') < '2023-02-14'
) as a
left join (
select
*
from
ods_coupon_use
where
dt = '2023-02-14'
) as b on a.id = b.id
where
b.id is null
union all
select
*
from
ods_coupon_use
where
dt = '2023-02-14'
) a;
---------------------------------------------------------
create table warehouse.dwd_fact_order_detail (
`id` string comment '订单编号',
`order_id` string comment '订单号',
`user_id` string comment '用户id',
`sku_id` string comment 'sku商品id',
`sku_name` string comment '商品名称',
`order_price` decimal(16,2) comment '商品价格',
`sku_num` bigint comment '商品数量',
`create_time` string comment '创建时间',
`province_id` string comment '省份id',
`source_type` string comment '来源类型',
`source_id` string comment '来源编号',
`benefit_reduce_amount` decimal(16,2) COMMENT '优惠金额',
`original_total_amount` decimal(16,2) COMMENT '原价金额',
`feight_fee` decimal(16,2) COMMENT '运费',
`final_total_amount` decimal(16,2) COMMENT '总金额'
) comment '订单明细事实表表'
partitioned by (`dt` string)
stored as textfile;

主要选取：
ods_order_detail【订单详情表里面字段】+
ods_order_info【订单表】：
final_total_amount 【订单金额】
province_id【省份id】
benefit_reduce_amount【优惠金额】
original_total_amount【原价金额】
feight_fee【运费】
注意：
dwd层主要是 拼接维度、退化维度、脏数据处理
不要做任何指标计算s


insert
overwrite table dwd_fact_order_detail partition(dt)
select
/*COALESCE(1)*/
a.id as id,
order_id,
user_id,
sku_id,
sku_name,
order_price,
sku_num,
create_time,
province_id,
source_type,
source_id,
benefit_reduce_amount,
original_total_amount,
feight_fee,
final_total_amount,
dt
from
(
select
*
from
ods_order_detail
where
dt = '2023-02-14'
) a
left join (
select
id,
final_total_amount,
province_id,
benefit_reduce_amount,
original_total_amount,
feight_fee
from
ods_order_info
where
dt = '2023-02-14'
) b on a.order_id = b.id;

-----------------------------------------------------------------------------

create table warehouse.dwd_fact_order_info (
`id` string comment '订单编号',
`order_status` string comment '订单状态',
`user_id` string comment '用户id',
`out_trade_no` string comment '支付流水号',
`create_time` string comment '创建时间(未支付状态)',
order_status_time string comment '状态变更时间',
`province_id` string comment '省份id',
`original_total_amount` decimal(16, 2) comment '原价金额',
`benefit_reduce_amount` decimal(16, 2) comment '优惠金额',
`feight_fee` decimal(16, 2) comment '运费',
`final_total_amount` decimal(16, 2) comment '订单金额'
) comment '订单事实表' partitioned by (`dt` string) stored as textfile;


insert
overwrite table dwd_fact_order_info partition(dt)
select
a.id as id,
b.order_status,
user_id,
out_trade_no,
create_time,
b.operate_time as order_status_time,
province_id,
original_total_amount,
benefit_reduce_amount,
feight_fee,
final_total_amount,
date_format(create_time, 'yyyy-MM-dd') as dt
from
(
select
*
from
ods_order_info
where
dt <= '2023-02-14'
) a
left join (
select
*
from
ods_order_status_log
where
dt = '2023-02-14'
) b on a.id = b.order_id;

```

## dwm层

dhkahsd
